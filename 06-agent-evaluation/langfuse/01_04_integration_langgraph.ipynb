{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPJsVa1O4TuL"
   },
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"LangGraph 开源可观测性实践\" description: \"介绍如何在 LangGraph（Python）应用中使用 Langfuse 构建开源可观测性与追踪能力。\" category: \"Integrations\" -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlCI9KeX4Zn4"
   },
   "source": [
    "## 什么是 LangGraph？\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) 是由 LangChain 团队开源的框架，用于基于大语言模型（LLM）构建复杂、有状态的多智能体应用。LangGraph 内置了持久化能力，可保存与恢复状态，从而支持错误恢复与包含“人类参与”（Human-in-the-loop, HITL）的工作流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o8L1qPcaZeC"
   },
   "source": [
    "## 本实践手册的目标\n",
    "\n",
    "本手册演示如何借助 [Langfuse](https://langfuse.com/docs)，通过其与 [LangChain 的集成](https://langfuse.com/integrations/frameworks/langchain)，对你的 LangGraph 应用进行调试、分析与迭代优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPTaMtxH4eHV"
   },
   "source": [
    "**完成本手册后，你将能够：**\n",
    "\n",
    "- 自动通过 Langfuse 集成对 LangGraph 应用进行追踪（tracing）\n",
    "- 监控复杂的多智能体（multi-agent）方案\n",
    "- 添加评分（例如用户反馈）\n",
    "- 使用 Langfuse 管理 LangGraph 中使用的提示词（prompt）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sSIS88y9Ewm"
   },
   "source": [
    "## 初始化 Langfuse\n",
    "\n",
    "在 Langfuse 控制台项目设置页获取你的 [API 密钥](https://langfuse.com/faq/all/where-are-langfuse-api-keys)，并将其加入到运行环境变量中以初始化 Langfuse 客户端。\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"⚠️\" -->\n",
    "_**注意：** 本笔记使用 Langfuse Python SDK v3。若你仍在使用 v2，请参阅我们的[旧版 LangGraph 集成指南](https://github.com/langfuse/langfuse-docs/blob/662509b3296daddcddb292f14b10a62e7c39407d/cookbook/integration_langgraph.ipynb)。_\n",
    "<!-- CALLOUT_END -->\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"ℹ️\" -->\n",
    "_**注意：** 需要至少 Python 3.11（参见 [GitHub Issue](https://github.com/langfuse/langfuse/issues/1926)）。_\n",
    "<!-- CALLOUT_END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C85BK1vJ5yD3",
    "outputId": "73f44b09-ae33-4bd0-8e92-9c1bfc8a1e7c"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse langchain langgraph langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1yglQ464VD-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 🔑 从项目设置页面获取您的 API 密钥：https://cloud.langfuse.com\n",
    "# 📋 配置步骤：\n",
    "# 1. 登录 Langfuse 控制台\n",
    "# 2. 选择或创建项目\n",
    "# 3. 进入项目设置页面\n",
    "# 4. 复制公钥和私钥\n",
    "\n",
    "# Langfuse 公钥（用于客户端身份验证）\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "\n",
    "# Langfuse 私钥（用于服务端身份验证，请妥善保管）\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "\n",
    "# Langfuse 服务器地址（选择离您最近的区域以获得更好性能）\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # 🇪🇺 欧洲区域\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"  # 🇺🇸 美国区域\n",
    "\n",
    "# 🤖 OpenAI API 密钥\n",
    "# 从 https://platform.openai.com/api-keys 获取\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "\n",
    "# 🚨 安全提醒：\n",
    "# - 不要将真实密钥提交到公共代码仓库\n",
    "# - 生产环境建议使用环境变量文件（.env）或密钥管理服务\n",
    "# - 定期轮换 API 密钥以提高安全性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在环境变量设置完成后，我们即可初始化 Langfuse 客户端。`get_client()` 会使用环境变量中提供的凭据来初始化 Langfuse 客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "# 🚀 初始化 Langfuse 客户端\n",
    "# get_client() 会自动读取环境变量中的配置信息\n",
    "langfuse = get_client()\n",
    " \n",
    "# 🔍 验证客户端连接状态\n",
    "# 这个步骤非常重要，确保后续的追踪功能能够正常工作\n",
    "if langfuse.auth_check():\n",
    "    print(\"✅ Langfuse 客户端已通过身份验证，准备就绪！\")\n",
    "    print(\"🔧 现在可以开始使用追踪功能了\")\n",
    "else:\n",
    "    print(\"❌ 身份验证失败！\")\n",
    "    print(\"🔍 请检查以下配置项：\")\n",
    "    print(\"   - LANGFUSE_PUBLIC_KEY 是否正确\")\n",
    "    print(\"   - LANGFUSE_SECRET_KEY 是否正确\") \n",
    "    print(\"   - LANGFUSE_HOST 是否可访问\")\n",
    "    print(\"   - 网络连接是否正常\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqYMmi6n9Nh1"
   },
   "source": [
    "## 示例 1：使用 LangGraph 构建简单聊天应用\n",
    "\n",
    "**本节将完成：**\n",
    "\n",
    "- 在 LangGraph 中构建一个可回答常见问题的客服聊天机器人\n",
    "- 使用 Langfuse 对机器人的输入与输出进行追踪（tracing）\n",
    "\n",
    "我们先从一个基础机器人入手，随后在下一节扩展为更高级的多智能体（multi-agent）设置，并在过程中介绍关键的 LangGraph 概念。\n",
    "\n",
    "### 创建智能体（Agent）\n",
    "\n",
    "首先创建一个 `StateGraph`。`StateGraph` 定义了聊天机器人的状态机结构。我们会添加节点来表示 LLM 以及机器人可调用的函数，并通过边（edge）定义机器人在这些函数之间的状态流转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGIxgPww6VX6"
   },
   "outputs": [],
   "source": [
    "# 🔧 导入 LangGraph 构建智能体所需的核心模块\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI  # OpenAI 聊天模型\n",
    "from langchain_core.messages import HumanMessage  # 人类消息类型\n",
    "from typing_extensions import TypedDict  # 类型化字典\n",
    "from langgraph.graph import StateGraph  # LangGraph 状态图\n",
    "from langgraph.graph.message import add_messages  # 消息添加函数\n",
    "\n",
    "# 📋 定义智能体的状态结构\n",
    "# State 是一个类型化字典，定义了智能体在执行过程中需要维护的状态信息\n",
    "class State(TypedDict):\n",
    "    # 💬 消息列表：存储对话历史\n",
    "    # Annotated[list, add_messages] 的含义：\n",
    "    # - list: 消息的数据类型是列表\n",
    "    # - add_messages: 指定状态更新策略，新消息会追加到列表末尾而不是覆盖整个列表\n",
    "    # 这种设计确保了对话历史的完整保存\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 创建状态图构建器\n",
    "# StateGraph 是 LangGraph 的核心组件，用于定义智能体的工作流程\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 初始化语言模型\n",
    "# 选择 GPT-4o 模型，temperature=0.2 确保输出相对稳定但仍有一定创造性\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 🔄 定义聊天机器人节点函数\n",
    "# 这是 LangGraph 节点函数的基本模式：接收当前状态，返回更新后的状态\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    聊天机器人节点的核心逻辑\n",
    "    \n",
    "    参数:\n",
    "        state (State): 当前的智能体状态，包含消息历史\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含新生成消息的状态更新\n",
    "    \n",
    "    工作流程:\n",
    "    1. 获取当前的消息历史\n",
    "    2. 将消息历史发送给语言模型\n",
    "    3. 接收模型生成的回复\n",
    "    4. 将回复包装成状态更新返回\n",
    "    \"\"\"\n",
    "    # 调用语言模型处理当前对话历史，生成回复\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    # 返回状态更新：将模型的回复添加到消息列表中\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 🔗 向图中添加\"chatbot\"节点\n",
    "# 节点代表工作单元，通常是普通的 Python 函数\n",
    "# 每个节点负责特定的处理逻辑，如调用 LLM、处理工具、数据转换等\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 🚀 设置图的入口点\n",
    "# 告诉图每次运行时从哪个节点开始执行\n",
    "# 在这个简单示例中，我们直接从 chatbot 节点开始\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# 🏁 设置图的结束点\n",
    "# 指示图\"当这个节点运行完成后，可以退出执行\"\n",
    "# 对于简单的单轮对话，chatbot 节点执行完就可以结束\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# ⚙️ 编译图形为可执行对象\n",
    "# compile() 方法将图构建器转换为 CompiledGraph\n",
    "# CompiledGraph 是可以实际运行的图形对象，支持 invoke、stream 等方法\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 💡 理解 LangGraph 的核心概念：\n",
    "# 🏗️ StateGraph: 定义智能体的状态和工作流程\n",
    "# 🔄 Node: 执行具体任务的函数，如调用 LLM、使用工具等\n",
    "# 🔗 Edge: 连接节点，定义执行顺序和条件跳转\n",
    "# 📊 State: 智能体运行过程中维护的数据结构\n",
    "# ⚙️ CompiledGraph: 编译后的可执行图形对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW2SJcRgh7Xo"
   },
   "source": [
    "### 在调用时添加 Langfuse 回调\n",
    "\n",
    "现在，为了追踪应用执行过程，我们将添加 [面向 LangChain 的 Langfuse 回调处理器](https://langfuse.com/integrations/frameworks/langchain)：`config={\"callbacks\": [langfuse_handler]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PxEc455-KYM",
    "outputId": "0d1a6a04-a024-47b8-d320-72cd25b7aefd"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 🛎️ 初始化 Langfuse 回调处理器\n",
    "# 该处理器会自动捕获 LangChain/LangGraph 的执行细节，用于：\n",
    "# - 🕒 记录每个节点的耗时与延迟\n",
    "# - 📝 保存输入、输出及中间状态\n",
    "# - 💰 统计 token 消耗和 API 调用成本\n",
    "# - 🐞 收集异常信息，便于排错\n",
    "# - 📈 在 Langfuse 中生成可视化调用链\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# 🚀 运行智能体并启用 Langfuse 追踪\n",
    "print(\"🤖 智能体开始运行，正在处理问题……\")\n",
    "print(\"❓ 用户提问：什么是 Langfuse？\")\n",
    "print(\"\n",
    "📋 执行过程：\")\n",
    "\n",
    "# 使用 stream 方法可以实时查看智能体的执行步骤\n",
    "for step_result in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"什么是 Langfuse？请详细介绍其主要功能和典型应用场景。\")]},\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    "):\n",
    "    print(f\"📤 节点执行结果：{step_result}\")\n",
    "\n",
    "print(\"\n",
    "✅ 智能体执行完成！\")\n",
    "print(\"🔍 请前往 Langfuse 控制台查看完整的追踪记录。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fdf3ZRnWGZ0N"
   },
   "source": [
    "### 在 Langfuse 中查看追踪结果\n",
    "\n",
    "示例追踪：`https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17Aq7u6_LBR6"
   },
   "source": [
    "![在 Langfuse 中查看聊天应用的追踪](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_chatapp_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3yyVtGKhMPU"
   },
   "source": [
    "### 可视化聊天应用\n",
    "\n",
    "你可以使用 `get_graph` 方法配合相应的 “draw” 方法对图进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "MKkM6mw47kIy",
    "outputId": "9cf8a453-05e0-4193-fc77-81cb176d9ef4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY0HW5xISntw"
   },
   "source": [
    "```mermaid\n",
    "graph TD;\n",
    "\t__start__([__start__]):::first\n",
    "\tchatbot(chatbot)\n",
    "\t__end__([__end__]):::last\n",
    "\t__start__ --> chatbot;\n",
    "\tchatbot --> __end__;\n",
    "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
    "\tclassDef first fill-opacity:0\n",
    "\tclassDef last fill:#bfb6fc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在 LangGraph Server 中使用 Langfuse\n",
    "\n",
    "#### 🖥️ LangGraph Server 简介\n",
    "\n",
    "[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/) 是 LangGraph 提供的服务器部署方案，用于将本地构建的图工作流发布为可扩展的在线服务，具备以下能力：\n",
    "\n",
    "- 🌐 **HTTP API 接口**：将 LangGraph 智能体封装为 REST API，便于与业务系统集成\n",
    "- 🚀 **生产级运行**：支持高并发、负载均衡与容器化交付\n",
    "- 🔧 **运维友好**：自动处理请求路由、状态恢复与错误重试\n",
    "- 📊 **监控集成**：兼容主流监控与追踪体系，便于观测运行状况\n",
    "- 🔒 **安全管控**：内置身份认证与授权机制，满足企业安全需求\n",
    "\n",
    "#### 💡 为什么要在 Server 环境接入 Langfuse？\n",
    "\n",
    "- 🏭 **生产可观测性**：实时查看线上请求的调用链与状态\n",
    "- 🐛 **远程调试**：无需复现场景即可还原问题细节\n",
    "- 📈 **性能洞察**：量化每个节点的耗时与成本\n",
    "- 💰 **费用治理**：准确统计第三方 API 的调用量与费用\n",
    "- 👥 **团队协作**：共享追踪记录，支持跨职能协同排查\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔧 配置方法说明\n",
    "\n",
    "使用 LangGraph Server 时，智能体图的调用由服务器自动处理，用户无法在每次请求时手动指定回调处理器。\n",
    "\n",
    "**关键差异：**\n",
    "- 🏠 **本地开发**：可以在每次调用时添加 `config={\"callbacks\": [langfuse_handler]}`\n",
    "- 🖥️ **服务器部署**：需要在图编译时预先配置回调处理器\n",
    "\n",
    "**解决方案：**\n",
    "在声明和编译图时就添加 Langfuse 回调，这样服务器上的所有请求都会自动启用追踪功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 导入服务器部署所需的模块\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 📋 定义与前文一致的智能体状态结构\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 构建图形结构\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 初始化语言模型\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 🔄 定义聊天机器人节点\n",
    "def chatbot(state: State):\n",
    "    \"\"\"处理用户消息并生成回复。\"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 🔗 组装图形结构\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# 🔄 初始化 Langfuse 回调处理器（服务器模式）\n",
    "# 在服务器环境中，此处理器会自动追踪所有请求的执行情况\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# ⚙️ 编译图形并预配置回调处理器\n",
    "# 🎯 核心方法：with_config()\n",
    "# - compile()：编译图形，生成可执行的 CompiledGraph\n",
    "# - with_config()：为编译后的图形设置默认配置（如回调处理器）\n",
    "#\n",
    "# 💡 工作流程：\n",
    "# 1. 编译图形得到 CompiledGraph 对象\n",
    "# 2. 调用 with_config() 注入 Langfuse 回调\n",
    "# 3. 返回一个已内置追踪能力的新图对象\n",
    "#\n",
    "# 🚀 优势：\n",
    "# - 无需在每次请求时手动添加回调配置\n",
    "# - 所有 API 请求都会自动写入 Langfuse 追踪\n",
    "# - 简化生产环境的部署与运维\n",
    "graph = graph_builder.compile().with_config({\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "# 💡 部署提示：\n",
    "# 在 LangGraph Server 中直接引用此 graph，即可立即获得完整的追踪数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2W94eY19TR1"
   },
   "source": [
    "## 示例 2：基于 LangGraph 的多智能体应用\n",
    "\n",
    "**本节将完成：**\n",
    "\n",
    "- 构建 2 个执行智能体：一个研究智能体使用 LangChain 的 WikipediaAPIWrapper 搜索维基百科，另一个使用自定义工具获取当前时间\n",
    "- 构建一个智能体监督者（supervisor），用于将用户问题分配给上述智能体\n",
    "- 添加 Langfuse 回调以追踪监督者与执行智能体的步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WfnrswDdjYTV",
    "outputId": "0d938cb1-9fd2-4ed3-cfdd-c84a9ad3ed82"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse langgraph langchain langchain_openai langchain_experimental pandas wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tciUQ62IEVec"
   },
   "source": [
    "### 创建工具\n",
    "\n",
    "在本示例中，我们将构建一个用于维基百科检索的智能体，以及一个用于告知当前时间的智能体。先定义它们将使用的工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cet0loyp9p-T"
   },
   "outputs": [],
   "source": [
    "# 🔧 导入多智能体系统所需的工具和模块\n",
    "from typing import Annotated\n",
    "from langchain_community.tools import WikipediaQueryRun  # 维基百科查询工具\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # 维基百科 API 封装\n",
    "from datetime import datetime  # 时间处理模块\n",
    "from langchain.tools import Tool  # 通用工具定义类\n",
    "\n",
    "# 🔍 定义维基百科搜索工具\n",
    "# 功能：根据查询词在维基百科中搜索相关信息\n",
    "# 适用场景：回答百科知识、历史事件、人物传记等问题\n",
    "wikipedia_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=2,  # 返回最相关的2个搜索结果\n",
    "        doc_content_chars_max=1000  # 限制每个结果的字符数，避免信息过载\n",
    "    )\n",
    ")\n",
    "\n",
    "# ⏰ 定义当前时间查询工具  \n",
    "# 功能：返回当前的日期和时间信息\n",
    "# 适用场景：回答\"现在几点\"、\"今天是什么日期\"等时间相关问题\n",
    "datetime_tool = Tool(\n",
    "    name=\"Datetime\",  # 工具名称，智能体会通过这个名称调用工具\n",
    "    func=lambda x: datetime.now().isoformat(),  # 工具函数：返回 ISO 格式的当前时间\n",
    "    description=\"返回当前的日期和时间信息（ISO 格式）\",  # 工具描述，帮助智能体理解何时使用此工具\n",
    ")\n",
    "\n",
    "# 💡 工具设计原则：\n",
    "# 1. 🎯 单一职责：每个工具只负责一个特定功能\n",
    "# 2. 📝 清晰描述：description 要准确描述工具的功能和使用场景\n",
    "# 3. 🔒 错误处理：生产环境中应该添加异常处理逻辑\n",
    "# 4. ⚡ 性能考虑：限制返回数据的大小，避免影响整体性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uhDy_mEqr6"
   },
   "source": [
    "### 🛠️ 辅助工具函数\n",
    "\n",
    "#### 📝 功能说明\n",
    "下面定义的辅助函数用于简化添加新的智能体工作节点。这些函数封装了创建智能体和节点的通用逻辑，提高代码的可重用性和可维护性。\n",
    "\n",
    "#### 🎯 设计目标\n",
    "- **减少重复代码**：避免为每个智能体重复编写相同的初始化逻辑\n",
    "- **标准化接口**：确保所有智能体节点具有一致的输入输出格式\n",
    "- **简化扩展**：新增智能体时只需关注业务逻辑，无需处理框架细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75atiExdqd4P"
   },
   "outputs": [],
   "source": [
    "# 🔧 导入智能体构建所需的核心组件\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent  # 智能体执行器和创建函数\n",
    "from langchain_core.messages import BaseMessage, HumanMessage  # 消息基类和人类消息\n",
    "from langchain_openai import ChatOpenAI  # OpenAI 聊天模型\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, system_prompt: str, tools: list):\n",
    "    \"\"\"\n",
    "    🏭 智能体工厂函数：创建具有特定能力的工作智能体\n",
    "    \n",
    "    参数:\n",
    "        llm (ChatOpenAI): 语言模型实例\n",
    "        system_prompt (str): 系统提示词，定义智能体的角色和行为规范\n",
    "        tools (list): 智能体可使用的工具列表\n",
    "    \n",
    "    返回:\n",
    "        AgentExecutor: 配置完成的智能体执行器\n",
    "    \n",
    "    🔄 工作流程:\n",
    "    1. 构建提示模板（包含系统角色、对话历史、工具使用记录）\n",
    "    2. 创建支持工具调用的 OpenAI 智能体\n",
    "    3. 包装为执行器，处理工具调用和状态管理\n",
    "    \"\"\"\n",
    "    # 📋 构建智能体的提示模板\n",
    "    # 包含三个关键部分：系统角色、对话历史、工具使用记录\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        # 🎭 系统消息：定义智能体的角色、能力和行为规范\n",
    "        (\"system\", system_prompt),\n",
    "        # 💬 消息历史：保存与用户和其他智能体的对话记录\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        # 🔧 工具记录：记录智能体使用工具的过程和结果\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    \n",
    "    # 🤖 创建支持 OpenAI 工具调用的智能体\n",
    "    # 这个智能体能够理解何时需要使用工具，以及如何解释工具的返回结果\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    \n",
    "    # 🎮 创建智能体执行器\n",
    "    # 执行器负责：工具调用、错误处理、状态管理、结果整合\n",
    "    executor = AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        verbose=True,  # 显示详细的执行过程，便于调试\n",
    "        handle_parsing_errors=True  # 自动处理解析错误，提高稳定性\n",
    "    )\n",
    "    \n",
    "    return executor\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    \"\"\"\n",
    "    🔄 智能体节点适配器：将智能体包装为 LangGraph 节点\n",
    "    \n",
    "    参数:\n",
    "        state: 当前的图状态，包含消息历史和其他上下文信息\n",
    "        agent: 智能体执行器实例\n",
    "        name: 智能体的名称，用于在多智能体系统中标识消息来源\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含智能体响应的状态更新\n",
    "    \n",
    "    🔄 适配过程:\n",
    "    1. 调用智能体处理当前状态\n",
    "    2. 提取智能体的输出结果\n",
    "    3. 包装为带有发送者身份的消息\n",
    "    4. 返回状态更新\n",
    "    \"\"\"\n",
    "    # 📤 调用智能体处理当前状态\n",
    "    # agent.invoke() 会处理对话历史，决定是否使用工具，并生成最终回复\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # 🏷️ 将智能体的输出包装为带有身份标识的消息\n",
    "    # name 参数让系统知道这条消息来自哪个智能体\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(\n",
    "            content=result[\"output\"],  # 智能体生成的文本内容\n",
    "            name=name  # 消息发送者的身份标识\n",
    "        )]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74bZqwU6FCOa"
   },
   "source": [
    "### 🎯 创建智能体监督者\n",
    "\n",
    "#### 📋 监督者的核心职责\n",
    "智能体监督者是多智能体系统的\"大脑\"，负责：\n",
    "\n",
    "- 🧠 **任务理解**：分析用户请求，理解任务的性质和需求\n",
    "- 🎯 **智能体选择**：根据任务特点选择最适合的工作智能体\n",
    "- 🔄 **流程控制**：决定何时切换智能体，何时结束处理流程\n",
    "- 📊 **结果整合**：汇总各个智能体的工作成果\n",
    "\n",
    "#### 🔧 技术实现方式\n",
    "监督者使用 **函数调用（Function Calling）** 技术来实现决策：\n",
    "\n",
    "- 📞 **函数调用**：通过结构化的函数调用来表达决策结果\n",
    "- 🎛️ **选择机制**：在可用的工作节点中选择下一个执行者\n",
    "- 🏁 **终止条件**：判断何时任务已完成，可以结束处理流程\n",
    "\n",
    "#### 💡 设计优势\n",
    "- **精确控制**：避免随机或不确定的路由决策\n",
    "- **可解释性**：每个决策都有明确的逻辑依据\n",
    "- **可扩展性**：容易添加新的工作智能体和决策规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu8MzgihrHdF"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Researcher\", \"CurrentTime\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# 🧭 监督者节点由 LLM 扮演，负责选择下一位执行的智能体并判断流程是否结束\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "# 🔁 使用 OpenAI Function Calling，可让结构化输出和解析更加稳定\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 📜 构建监督者提示模板\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# 🔗 构建监督者智能体的执行链\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ognuMaIeFVh7"
   },
   "source": [
    "### 构建图形结构\n",
    "\n",
    "现在可以开始搭建整张图。下面使用刚刚定义的函数指定状态和各个工作节点，并连接图中的所有边。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_LwtCmw_rHVz"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Sequence, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# 🗂️ 智能体状态会作为每个节点的输入数据\n",
    "class AgentState(TypedDict):\n",
    "    # Annotated 告诉图：新的消息会追加到现有消息列表中\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # next 字段指示下一步要跳转到哪个节点\n",
    "    next: str\n",
    "\n",
    "# 🧑‍💻 使用辅助函数创建研究智能体\n",
    "research_agent = create_agent(llm, \"You are a web researcher.\", [wikipedia_tool])\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# 🕰️ 创建报时智能体\n",
    "currenttime_agent = create_agent(llm, \"You are a time keeping assistant who tells the current time.\", [datetime_tool])\n",
    "currenttime_node = functools.partial(agent_node, agent=currenttime_agent, name=\"CurrentTime\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 📦 注册节点：节点代表具体的工作单元，通常是 Python 函数\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"CurrentTime\", currenttime_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)\n",
    "\n",
    "# 🔂 强制所有工作节点在完成后回到监督者\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "# 🔀 条件边根据当前状态决定后续路由\n",
    "# 该函数读取状态并返回要执行的下一个节点名称\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# 🚪 设置入口节点，确定图在运行时从哪里开始\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "# ⚙️ 编译图形，得到可调用的 CompiledGraph\n",
    "graph_2 = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3xfJLJyFwBG"
   },
   "source": [
    "### 在调用中挂载 Langfuse 回调\n",
    "\n",
    "在执行 `graph_2.stream` 时增加 [Langfuse 回调处理器](https://langfuse.com/integrations/frameworks/langchain)：`config={\"callbacks\": [langfuse_handler]}`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsX1gw9kryGP",
    "outputId": "65d94f3c-17e7-4ad8-88b5-f837676d206b"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 📡 初始化 Langfuse 回调处理器，用于记录 LangChain 的执行轨迹\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# 🔗 将回调处理器挂载到图的 stream 调用中；可选的 run_name 会作为追踪名称展示\n",
    "for s in graph_2.stream({\"messages\": [HumanMessage(content=\"光合作用是如何进行的？\")]},\n",
    "                       config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqJnMtP5HDql",
    "outputId": "69c3d5d6-d44c-4784-a484-c66e4748b522"
   },
   "outputs": [],
   "source": [
    "# 🔗 同样地，为其他查询挂载 Langfuse 回调\n",
    "for s in graph_2.stream({\"messages\": [HumanMessage(content=\"现在的准确时间是多少？\")]},\n",
    "                       config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4XjtNenH9GF"
   },
   "source": [
    "### 在 Langfuse 中查看多智能体追踪\n",
    "\n",
    "以下链接展示了本节多智能体示例在 Langfuse 中生成的追踪记录：\n",
    "\n",
    "1. [光合作用是如何进行的？](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7d5f970573b8214d1ca891251e42282c)\n",
    "2. [现在几点？](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/3a69fe4998df50d42054f8944bd6a8d9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-5EEBZAIbwc"
   },
   "source": [
    "![在 Langfuse 中查看多智能体追踪](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_multiagent_traces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCEzabn_jhbf"
   },
   "source": [
    "### 可视化该智能体\n",
    "\n",
    "你可以使用 `get_graph` 方法配合相应的 “draw” 方法进行图形可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "notlPjnl-HXV",
    "outputId": "17d6c6db-92af-4a6e-b1af-61b68e9cc87a"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph_2.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mESkG2IJS8OY"
   },
   "source": [
    "```mermaid\n",
    "graph TD;\n",
    "\t__start__([__start__]):::first\n",
    "\tResearcher(Researcher)\n",
    "\tCurrentTime(CurrentTime)\n",
    "\tsupervisor(supervisor)\n",
    "\t__end__([__end__]):::last\n",
    "\tCurrentTime --> supervisor;\n",
    "\tResearcher --> supervisor;\n",
    "\t__start__ --> supervisor;\n",
    "\tsupervisor -.-> Researcher;\n",
    "\tsupervisor -.-> CurrentTime;\n",
    "\tsupervisor -. &nbspFINISH&nbsp .-> __end__;\n",
    "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
    "\tclassDef first fill-opacity:0\n",
    "\tclassDef last fill:#bfb6fc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多个 LangGraph 智能体的协同\n",
    "\n",
    "在某些架构中，一个 LangGraph 智能体会调用一个或多个其他 LangGraph 智能体。若想让整套执行链在 Langfuse 中聚合为同一条追踪，可显式传入自定义的 `trace_id`。\n",
    "\n",
    "首先生成一个共享的 `trace_id`，供主智能体与子智能体共用，以便在 Langfuse 中合并为同一条记录。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client, Langfuse\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# 🔐 从外部系统生成一个确定性的 trace_id，便于跨服务聚合\n",
    "predefined_trace_id = Langfuse.create_trace_id()\n",
    "\n",
    "# 📡 初始化 Langfuse 回调处理器，用于采集 LangChain 的执行数据\n",
    "langfuse_handler = CallbackHandler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，构建子智能体的逻辑。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "sub_agent = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随后，将该子智能体封装成工具，供主流程调用并复用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def langgraph_research(question):\n",
    "  \"\"\"Conducts research for various topics.\"\"\"\n",
    "\n",
    "  with langfuse.start_as_current_span(\n",
    "      name=\"🤖-sub-research-agent\",\n",
    "      trace_context={\"trace_id\": predefined_trace_id}\n",
    "  ) as span:\n",
    "      span.update_trace(input=question)\n",
    "\n",
    "      response = sub_agent.invoke({\"messages\": [HumanMessage(content = question)]},\n",
    "                        config={\"callbacks\": [langfuse_handler]})\n",
    "    \n",
    "      span.update_trace(output= response[\"messages\"][1].content)\n",
    "\n",
    "  return response[\"messages\"][1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，创建第二个 LangGraph 智能体，通过前面新增的 `langgraph_research` 工具完成协作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "main_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[langgraph_research]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"什么是 Langfuse？\"\n",
    "\n",
    "# 🧭 使用预生成的 trace_id（通过 trace_context 注入）\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"🤖-main-agent\",\n",
    "    trace_context={\"trace_id\": predefined_trace_id}\n",
    ") as span:\n",
    "    span.update_trace(input=user_question)\n",
    "\n",
    "    # 此处的 LangChain 执行都会归属于同一条追踪\n",
    "    response = main_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_question}]},\n",
    "                            config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "    span.update_trace(output=response[\"messages\"][1].content)\n",
    "\n",
    "print(f\"Trace ID: {predefined_trace_id}\")  # 可在后续评分或排查时使用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在 Langfuse 中查看多智能体追踪\n",
    "\n",
    "![多智能体追踪示例](https://langfuse.com/images/cookbook/integration-langgraph/a2a_langgraph.png)\n",
    "\n",
    "示例追踪链接：https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uybP4h8wGvWw"
   },
   "source": [
    "## 为追踪添加评分\n",
    "\n",
    "[评分（Score）](https://langfuse.com/docs/scores/overview) 用于评价单个观测（observation）或整条追踪（trace），可帮助你在运行时执行自定义质量检查，或配合人工审核流程。\n",
    "\n",
    "下面的示例演示如何：\n",
    "\n",
    "- 为某个 span 记录一个数值型评分（如 `relevance`）\n",
    "- 为整条追踪记录一个分类型评分（如 `feedback`）\n",
    "\n",
    "这有助于系统化地评估与改进应用质量。\n",
    "\n",
    "**→ 想深入了解？请参阅 [Langfuse 自定义评分指南](https://langfuse.com/docs/scores/custom)。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgAqYnQuGwCL",
    "outputId": "11e14766-b25b-44b4-c3d4-d980f3d111cc"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# 方案一：使用上下文管理器返回的 span 对象进行评分\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"langgraph-request\") as span:\n",
    "    # ... 此处执行 LangGraph 逻辑 ...\n",
    "\n",
    "    # 直接通过 span.score_trace 记录评分\n",
    "    span.score_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\",\n",
    "        comment=\"This was correct, thank you\"\n",
    "    )\n",
    "\n",
    "# 方案二：在仍位于上下文时调用 score_current_trace()\n",
    "with langfuse.start_as_current_span(name=\"langgraph-request\") as span:\n",
    "    # ... LangGraph execution ...\n",
    "\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\"\n",
    "    )\n",
    "\n",
    "# 方案三：若已离开上下文，可使用 trace_id 直接创建评分\n",
    "langfuse.create_score(\n",
    "    trace_id=predefined_trace_id,\n",
    "    name=\"user-feedback\",\n",
    "    value=1,\n",
    "    data_type=\"NUMERIC\",\n",
    "    comment=\"This was correct, thank you\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq_DeCcXSxwq"
   },
   "source": [
    "### 在 Langfuse 中查看带评分的追踪\n",
    "\n",
    "示例追踪：https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e60a078b828d4fdc7ea22c73193b0fe4\n",
    "\n",
    "![包含评分的追踪展示](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_score.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cIQVrYZJVMO"
   },
   "source": [
    "## 使用 Langfuse 管理提示词\n",
    "\n",
    "通过 [Langfuse Prompt Management](https://langfuse.com/docs/prompts/example-langchain) 可以对提示词进行统一的版本管理。在本示例中，我们演示如何通过 SDK 新增一条提示词；在实际生产环境中，更推荐直接在 Langfuse UI 中创建和发布。\n",
    "\n",
    "Langfuse 的提示词管理相当于一个“提示词内容管理系统（Prompt CMS）”。你可以在 UI 中编辑、预览与发布；也可在代码中通过 SDK 读取或更新。\n",
    "\n",
    "配置提示词时通常需要指定：\n",
    "\n",
    "* `name`：在 Langfuse 中唯一标识该提示词的名称\n",
    "* `prompt`：包含模板占位符（如 `{{input variables}}`）的文本内容\n",
    "* `labels`：可设置为 `production`，让该版本立即成为默认选项\n",
    "\n",
    "在本例中，我们创建一个系统提示词，让助手将所有用户输入翻译成西班牙语。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0J8-nbhUUz6",
    "outputId": "ee71e43d-9f77-451d-b71c-f77cd297b065"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    "\n",
    "langfuse.create_prompt(\n",
    "    name=\"translator_system-prompt\",\n",
    "    prompt=\"You are a translator that translates every input text into Spanish.\",\n",
    "    labels=[\"production\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dullp4XDXhzg"
   },
   "source": [
    "![在 Langfuse UI 中查看提示词](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_prompt_example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNboOjf2YQpD"
   },
   "source": [
    "使用工具方法 `.get_langchain_prompt()` 可以将 Langfuse 中的提示词转换为 LangChain 可直接使用的字符串。\n",
    "\n",
    "**背景说明：** Langfuse 在提示模板中使用双花括号（`{{input variable}}`）声明变量；而 LangChain 的 `PromptTemplate` 使用单花括号（`{input variable}`）。`.get_langchain_prompt()` 会自动完成格式转换。当前示例的提示词没有占位变量，但仍可以统一通过该方法处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z49I82blYeXy",
    "outputId": "6cf7cd23-6dde-4e7b-ae50-e369db37c2d0"
   },
   "outputs": [],
   "source": [
    "# 读取生产环境中最新的提示词版本，并转换为 LangChain 可直接使用的字符串\n",
    "langfuse_system_prompt = langfuse.get_prompt(\"translator_system-prompt\")\n",
    "langchain_system_prompt = langfuse_system_prompt.get_langchain_prompt()\n",
    "\n",
    "print(langchain_system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3zBULfCt0Wq"
   },
   "source": [
    "现在可以使用新的系统提示词字符串，更新我们的翻译助手。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oGQhulyMmvZD"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 🧩 将系统提示词注入到聊天节点，确保助手先遵循翻译指令\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": langchain_system_prompt\n",
    "}\n",
    "\n",
    "def chatbot(state: State):\n",
    "    messages_with_system_prompt = [system_prompt] + state[\"messages\"]\n",
    "    response = llm.invoke(messages_with_system_prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYd7wbttm2ec",
    "outputId": "fdc18797-3b3b-4cae-9ebb-8b9d946494c1"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 📡 初始化 Langfuse 回调处理器（用于追踪翻译助手的调用）\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# 🔗 将回调挂载到图的 stream 方法中，实时查看翻译结果与追踪记录\n",
    "for s in graph.stream({\"messages\": [HumanMessage(content=\"请把“Langfuse 是什么？”翻译成西班牙语。\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 LangGraph 追踪中添加自定义 Span\n",
    "\n",
    "某些场景下，我们希望在追踪中插入自定义 span，以标记关键步骤或附加额外上下文。可以参考这个 [GitHub 讨论贴](https://github.com/orgs/langfuse/discussions/2988#discussioncomment-11634600) 中给出的示例实现方式。\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}