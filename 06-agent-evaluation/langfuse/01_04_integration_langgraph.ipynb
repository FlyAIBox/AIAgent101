{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlyAIBox/AIAgent101/blob/main/06-agent-evaluation/langfuse/01_04_integration_langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlCI9KeX4Zn4"
      },
      "source": [
        "## 什么是 LangGraph？\n",
        "\n",
        "[LangGraph](https://langchain-ai.github.io/langgraph/) 是由 LangChain 团队开源的框架，用于基于大语言模型（LLM）构建复杂、有状态的多智能体应用。LangGraph 内置了持久化能力，可保存与恢复状态，从而支持错误恢复与包含“人机交互”（Human-in-the-loop, HITL）的工作流。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o8L1qPcaZeC"
      },
      "source": [
        "## 本实践手册的目标\n",
        "\n",
        "本手册演示如何借助 [Langfuse](https://langfuse.com/docs)，通过其与 [LangChain 的集成](https://langfuse.com/integrations/frameworks/langchain)，对你的 LangGraph 应用进行调试、分析与迭代优化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPTaMtxH4eHV"
      },
      "source": [
        "**完成本手册后，你将能够：**\n",
        "\n",
        "- 自动通过 Langfuse 集成对 LangGraph 应用进行追踪（tracing）\n",
        "- 监控复杂的多智能体（multi-agent）方案\n",
        "- 添加评分（例如用户反馈）\n",
        "- 使用 Langfuse 管理 LangGraph 中使用的提示词（prompt）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sSIS88y9Ewm"
      },
      "source": [
        "## 初始化 Langfuse\n",
        "\n",
        "在 Langfuse 控制台项目设置页获取你的 [API 密钥](https://langfuse.com/faq/all/where-are-langfuse-api-keys)，并将其加入到运行环境变量中以初始化 Langfuse 客户端。\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"⚠️\" -->\n",
        "_**注意：** 本笔记使用 Langfuse Python SDK v3。_\n",
        "<!-- CALLOUT_END -->\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"ℹ️\" -->\n",
        "_**注意：** 需要至少 Python 3.11（参见 [GitHub Issue](https://github.com/langfuse/langfuse/issues/1926)）。_\n",
        "<!-- CALLOUT_END -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C85BK1vJ5yD3",
        "outputId": "618fb8cc-9395-45e0-ee15-549cf7ac3c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langfuse==3.3.0\n",
            "  Downloading langfuse-3.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai==0.3.31\n",
            "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain_community==0.3.27\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langgraph==0.6.7\n",
            "  Downloading langgraph-0.6.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting backoff>=1.10.0 (from langfuse==3.3.0)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse==3.3.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.36.0)\n",
            "Requirement already satisfied: packaging<26.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.32.4)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.17.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.4.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.31) (1.107.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.31) (0.11.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community==0.3.27)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (2.0.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph==0.6.7)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph==0.6.7)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph==0.6.7)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.3.0) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.15.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.6.7)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph==0.6.7) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.24.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from langfuse==3.3.0)\n",
            "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (5.29.5)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from langfuse==3.3.0)\n",
            "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.3.0)\n",
            "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community==0.3.27) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.31) (2024.11.6)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langfuse-3.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.6.7-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: ormsgpack, opentelemetry-proto, mypy-extensions, marshmallow, backoff, typing-inspect, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, langgraph-sdk, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langfuse, langgraph, langchain_community\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.36.0\n",
            "    Uninstalling opentelemetry-api-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.36.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.57b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.57b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.36.0\n",
            "    Uninstalling opentelemetry-sdk-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.36.0\n",
            "Successfully installed backoff-2.2.1 dataclasses-json-0.6.7 langchain-openai-0.3.31 langchain_community-0.3.27 langfuse-3.3.0 langgraph-0.6.7 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 marshmallow-3.26.1 mypy-extensions-1.1.0 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-http-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 ormsgpack-1.10.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse==3.3.0 langchain==0.3.27 langchain-openai==0.3.31 langchain_community==0.3.27 langgraph==0.6.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 Langfuse 控制台的项目设置页获取 API Key，初始化 Langfuse 客户端，并将其设置到环境变量中。"
      ],
      "metadata": {
        "id": "sUbRmJ-Kh0rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔐 环境变量配置 - 安全存储敏感信息\n",
        "# 环境变量是存储API密钥等敏感信息的最佳实践\n",
        "# 避免在代码中硬编码密钥，防止泄露\n",
        "\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    \"\"\"\n",
        "    安全地设置环境变量\n",
        "    如果环境变量不存在，会提示用户输入\n",
        "    使用getpass模块隐藏输入内容，防止密码泄露\n",
        "    \"\"\"\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "# 🤖 OpenAI API 配置\n",
        "# OpenAI API密钥：从 https://platform.openai.com/api-keys 获取\n",
        "# 这是调用GPT模型必需的认证信息\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "# API代理地址：如果你使用第三方代理服务（如国内代理）\n",
        "# 示例：https://api.apiyi.com/v1\n",
        "# 如果直接使用OpenAI官方API，可以留空\n",
        "_set_env(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# 🌐 Langfuse 配置\n",
        "# Langfuse是一个可观测性平台，需要注册账户获取密钥\n",
        "# 注册地址：https://cloud.langfuse.com\n",
        "\n",
        "# 公开密钥：用于标识你的项目\n",
        "_set_env(\"LANGFUSE_PUBLIC_KEY\")\n",
        "\n",
        "# 秘密密钥：用于认证，请妥善保管\n",
        "_set_env(\"LANGFUSE_SECRET_KEY\")\n",
        "\n",
        "# 服务器地址：选择离你最近的区域\n",
        "# 🇪🇺 欧盟区域(推荐) https://cloud.langfuse.com\n",
        "# 🇺🇸 美国区域（不推荐） https://us.cloud.langfuse.com\n",
        "_set_env(\"LANGFUSE_HOST\")\n",
        "\n",
        "# 💡 初学者提示：\n",
        "# 1. 环境变量存储在操作系统中，重启后需要重新设置\n",
        "# 2. 生产环境中建议使用.env文件或云服务配置\n",
        "# 3. 永远不要在代码中硬编码API密钥！\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vku6l5CLhwdC",
        "outputId": "2b08d8c8-36b9-496a-8bfd-247a64893f8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "OPENAI_BASE_URL: ··········\n",
            "LANGFUSE_PUBLIC_KEY: ··········\n",
            "LANGFUSE_SECRET_KEY: ··········\n",
            "LANGFUSE_HOST: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agy0r3Byg-Aw"
      },
      "source": [
        "在环境变量设置完成后，我们即可初始化 Langfuse 客户端。`get_client()` 会使用环境变量中提供的凭据来初始化 Langfuse 客户端。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LVLs2A8g-Ax",
        "outputId": "4cf4f89d-2ff0-4e97-d627-2043009554eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Langfuse 客户端已通过身份验证，准备就绪！\n",
            "🔧 现在可以开始使用追踪功能了\n"
          ]
        }
      ],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "# 🚀 初始化 Langfuse 客户端\n",
        "# get_client() 会自动读取环境变量中的配置信息\n",
        "langfuse = get_client()\n",
        "\n",
        "# 🔍 验证客户端连接状态\n",
        "# 这个步骤非常重要，确保后续的追踪功能能够正常工作\n",
        "if langfuse.auth_check():\n",
        "    print(\"✅ Langfuse 客户端已通过身份验证，准备就绪！\")\n",
        "    print(\"🔧 现在可以开始使用追踪功能了\")\n",
        "else:\n",
        "    print(\"❌ 身份验证失败！\")\n",
        "    print(\"🔍 请检查以下配置项：\")\n",
        "    print(\"   - LANGFUSE_PUBLIC_KEY 是否正确\")\n",
        "    print(\"   - LANGFUSE_SECRET_KEY 是否正确\")\n",
        "    print(\"   - LANGFUSE_HOST 是否可访问\")\n",
        "    print(\"   - 网络连接是否正常\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqYMmi6n9Nh1"
      },
      "source": [
        "## 示例 1：使用 LangGraph 构建简单聊天应用\n",
        "\n",
        "**本节将完成：**\n",
        "\n",
        "- 在 LangGraph 中构建一个可回答常见问题的客服聊天机器人\n",
        "- 使用 Langfuse 对机器人的输入与输出进行追踪（tracing）\n",
        "\n",
        "我们先从一个基础机器人入手，随后在下一节扩展为更高级的多智能体（multi-agent）设置，并在过程中介绍关键的 LangGraph 概念。\n",
        "\n",
        "### 创建智能体（Agent）\n",
        "\n",
        "首先创建一个 `StateGraph`。`StateGraph` 定义了聊天机器人的状态机结构。我们会添加节点来表示 LLM 以及机器人可调用的函数，并通过边（edge）定义机器人在这些函数之间的状态流转。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aGIxgPww6VX6"
      },
      "outputs": [],
      "source": [
        "# 🔧 导入 LangGraph 构建智能体所需的核心模块\n",
        "from typing import Annotated\n",
        "from langchain_openai import ChatOpenAI  # OpenAI 聊天模型\n",
        "from langchain_core.messages import HumanMessage  # 人类消息类型\n",
        "from typing_extensions import TypedDict  # 类型化字典\n",
        "from langgraph.graph import StateGraph  # LangGraph 状态图\n",
        "from langgraph.graph.message import add_messages  # 消息添加函数\n",
        "\n",
        "# 📋 定义智能体的状态结构\n",
        "# State 是一个类型化字典，定义了智能体在执行过程中需要维护的状态信息\n",
        "class State(TypedDict):\n",
        "    # 💬 消息列表：存储对话历史\n",
        "    # Annotated[list, add_messages] 的含义：\n",
        "    # - list: 消息的数据类型是列表\n",
        "    # - add_messages: 指定状态更新策略，新消息会追加到列表末尾而不是覆盖整个列表\n",
        "    # 这种设计确保了对话历史的完整保存\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# 🏗️ 创建状态图构建器\n",
        "# StateGraph 是 LangGraph 的核心组件，用于定义智能体的工作流程\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# 🤖 初始化语言模型\n",
        "# 选择 GPT-4o 模型，temperature=0.2 确保输出相对稳定但仍有一定创造性\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
        "\n",
        "# 🔄 定义聊天机器人节点函数\n",
        "# 这是 LangGraph 节点函数的基本模式：接收当前状态，返回更新后的状态\n",
        "def chatbot(state: State):\n",
        "    \"\"\"\n",
        "    聊天机器人节点的核心逻辑\n",
        "\n",
        "    参数:\n",
        "        state (State): 当前的智能体状态，包含消息历史\n",
        "\n",
        "    返回:\n",
        "        dict: 包含新生成消息的状态更新\n",
        "\n",
        "    工作流程:\n",
        "    1. 获取当前的消息历史\n",
        "    2. 将消息历史发送给语言模型\n",
        "    3. 接收模型生成的回复\n",
        "    4. 将回复包装成状态更新返回\n",
        "    \"\"\"\n",
        "    # 调用语言模型处理当前对话历史，生成回复\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "\n",
        "    # 返回状态更新：将模型的回复添加到消息列表中\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# 🔗 向图中添加\"chatbot\"节点\n",
        "# 节点代表工作单元，通常是普通的 Python 函数\n",
        "# 每个节点负责特定的处理逻辑，如调用 LLM、处理工具、数据转换等\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# 🚀 设置图的入口点\n",
        "# 告诉图每次运行时从哪个节点开始执行\n",
        "# 在这个简单示例中，我们直接从 chatbot 节点开始\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# 🏁 设置图的结束点\n",
        "# 指示图\"当这个节点运行完成后，可以退出执行\"\n",
        "# 对于简单的单轮对话，chatbot 节点执行完就可以结束\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "\n",
        "# ⚙️ 编译图形为可执行对象\n",
        "# compile() 方法将图构建器转换为 CompiledGraph\n",
        "# CompiledGraph 是可以实际运行的图形对象，支持 invoke、stream 等方法\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# 💡 理解 LangGraph 的核心概念：\n",
        "# 🏗️ StateGraph: 定义智能体的状态和工作流程\n",
        "# 🔄 Node: 执行具体任务的函数，如调用 LLM、使用工具等\n",
        "# 🔗 Edge: 连接节点，定义执行顺序和条件跳转\n",
        "# 📊 State: 智能体运行过程中维护的数据结构\n",
        "# ⚙️ CompiledGraph: 编译后的可执行图形对象"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW2SJcRgh7Xo"
      },
      "source": [
        "### 在调用时添加 Langfuse 回调\n",
        "\n",
        "现在，为了追踪应用执行过程，我们将添加 [面向 LangChain 的 Langfuse 回调处理器](https://langfuse.com/integrations/frameworks/langchain)：`config={\"callbacks\": [langfuse_handler]}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PxEc455-KYM",
        "outputId": "b4c73185-7d54-4e1d-b140-80685ea5b9c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 智能体开始运行，正在处理问题……\n",
            "❓ 用户提问：什么是 Langfuse？\n",
            "📋 执行过程:\n",
            "📤 节点执行结果：{'chatbot': {'messages': [AIMessage(content='Langfuse 是一个专注于语言模型和自然语言处理（NLP）应用的工具或平台，旨在帮助开发者和企业更有效地构建、管理和优化基于语言模型的应用程序。虽然具体的功能和应用场景可能会因版本和具体实现而有所不同，但以下是一些可能的主要功能和典型应用场景：\\n\\n### 主要功能\\n\\n1. **模型管理**：\\n   - 提供对多种语言模型的支持，包括开源模型和商业模型。\\n   - 允许用户上传和管理自定义模型。\\n\\n2. **数据处理与分析**：\\n   - 提供数据预处理工具，以便更好地准备训练数据。\\n   - 提供分析工具来评估模型性能，包括准确性、召回率、F1分数等。\\n\\n3. **训练与优化**：\\n   - 支持模型训练和微调，帮助用户根据特定需求优化模型。\\n   - 提供超参数调优工具，以提高模型的效率和效果。\\n\\n4. **部署与集成**：\\n   - 提供简单的API接口，方便将模型集成到现有应用中。\\n   - 支持云端和本地部署，满足不同的业务需求。\\n\\n5. **监控与日志**：\\n   - 提供实时监控工具，帮助用户跟踪模型的使用情况和性能。\\n   - 提供日志记录功能，以便进行故障排查和性能分析。\\n\\n6. **安全与合规**：\\n   - 提供数据加密和访问控制，确保数据安全。\\n   - 确保平台符合相关法律法规，如GDPR等。\\n\\n### 典型应用场景\\n\\n1. **客服与聊天机器人**：\\n   - 使用Langfuse构建智能客服系统，能够理解和响应客户的自然语言查询。\\n   - 提供24/7的客户支持，提高客户满意度。\\n\\n2. **内容生成与编辑**：\\n   - 帮助内容创作者生成高质量的文本内容，如文章、报告等。\\n   - 提供自动化的文本校对和编辑功能。\\n\\n3. **翻译与语言转换**：\\n   - 提供高效的机器翻译服务，支持多语言转换。\\n   - 帮助企业实现全球化运营，支持多语言客户。\\n\\n4. **情感分析与市场调研**：\\n   - 分析社交媒体、客户反馈中的情感倾向，帮助企业了解市场动态。\\n   - 提供数据驱动的市场决策支持。\\n\\n5. **个性化推荐系统**：\\n   - 基于用户的历史行为和偏好，提供个性化的产品或内容推荐。\\n   - 提高用户参与度和转化率。\\n\\nLangfuse 作为一个综合性的平台，能够为各种需要自然语言处理的应用提供支持，从而帮助企业和开发者更高效地利用语言模型的强大功能。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 624, 'prompt_tokens': 26, 'total_tokens': 650, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-CIXdYUV27bcw2APXT9Ho5L9hZ9Hzb', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--94ba4345-2e2b-46ac-8d71-6f41ec1598de-0', usage_metadata={'input_tokens': 26, 'output_tokens': 624, 'total_tokens': 650, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
            "✅ 智能体执行完成！\n",
            "🔍 请前往 Langfuse 控制台查看完整的追踪记录。\n"
          ]
        }
      ],
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# 🛎️ 初始化 Langfuse 回调处理器\n",
        "# 该处理器会自动捕获 LangChain/LangGraph 的执行细节，用于：\n",
        "# - 🕒 记录每个节点的耗时与延迟\n",
        "# - 📝 保存输入、输出及中间状态\n",
        "# - 💰 统计 token 消耗和 API 调用成本\n",
        "# - 🐞 收集异常信息，便于排错\n",
        "# - 📈 在 Langfuse 中生成可视化调用链\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "# 🚀 运行智能体并启用 Langfuse 追踪\n",
        "print(\"🤖 智能体开始运行，正在处理问题……\")\n",
        "print(\"❓ 用户提问：什么是 Langfuse？\")\n",
        "print(\"📋 执行过程:\")\n",
        "\n",
        "# 使用 stream 方法可以实时查看智能体的执行步骤\n",
        "for step_result in graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"什么是 Langfuse？请详细介绍其主要功能和典型应用场景。\")]},\n",
        "    config={\"callbacks\": [langfuse_handler]}\n",
        "):\n",
        "    print(f\"📤 节点执行结果：{step_result}\")\n",
        "\n",
        "print(\n",
        "\"✅ 智能体执行完成！\")\n",
        "print(\"🔍 请前往 Langfuse 控制台查看完整的追踪记录。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdf3ZRnWGZ0N"
      },
      "source": [
        "### 在 Langfuse 中查看追踪结果\n",
        "\n",
        "示例追踪：https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=cbc8503a9a111fc5eadb5e914be3fa7a&timestamp=2025-09-22T03%3A48%3A16.647Z&observation=47245ec86916a5d1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Aq7u6_LBR6"
      },
      "source": [
        "![在 Langfuse 中查看聊天应用的追踪](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509221150147.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3yyVtGKhMPU"
      },
      "source": [
        "### 可视化聊天应用\n",
        "\n",
        "你可以使用 `get_graph` 方法配合相应的 “draw” 方法对图进行可视化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "MKkM6mw47kIy",
        "outputId": "8eae221d-3838-4fbe-ac94-8fe2c1e18513"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY0HW5xISntw"
      },
      "source": [
        "```mermaid\n",
        "graph TD;\n",
        "\t__start__([__start__]):::first\n",
        "\tchatbot(chatbot)\n",
        "\t__end__([__end__]):::last\n",
        "\t__start__ --> chatbot;\n",
        "\tchatbot --> __end__;\n",
        "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
        "\tclassDef first fill-opacity:0\n",
        "\tclassDef last fill:#bfb6fc\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F4Tt_E4g-A0"
      },
      "source": [
        "### 在 LangGraph Server 中使用 Langfuse\n",
        "\n",
        "#### 🖥️ LangGraph Server 简介\n",
        "\n",
        "[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/) 是 LangGraph 提供的服务器部署方案，用于将本地构建的图工作流发布为可扩展的在线服务，具备以下能力：\n",
        "\n",
        "- 🌐 **HTTP API 接口**：将 LangGraph 智能体封装为 REST API，便于与业务系统集成\n",
        "- 🚀 **生产级运行**：支持高并发、负载均衡与容器化交付\n",
        "- 🔧 **运维友好**：自动处理请求路由、状态恢复与错误重试\n",
        "- 📊 **监控集成**：兼容主流监控与追踪体系，便于观测运行状况\n",
        "- 🔒 **安全管控**：内置身份认证与授权机制，满足企业安全需求\n",
        "\n",
        "#### 💡 为什么要在 Server 环境接入 Langfuse？\n",
        "\n",
        "- 🏭 **生产可观测性**：实时查看线上请求的调用链与状态\n",
        "- 🐛 **远程调试**：无需复现场景即可还原问题细节\n",
        "- 📈 **性能洞察**：量化每个节点的耗时与成本\n",
        "- 💰 **费用治理**：准确统计第三方 API 的调用量与费用\n",
        "- 👥 **团队协作**：共享追踪记录，支持跨职能协同排查\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTW_X4rFg-A0"
      },
      "source": [
        "#### 🔧 配置方法说明\n",
        "\n",
        "使用 LangGraph Server 时，智能体图的调用由服务器自动处理，用户无法在每次请求时手动指定回调处理器。\n",
        "\n",
        "**关键差异：**\n",
        "- 🏠 **本地开发**：可以在每次调用时添加 `config={\"callbacks\": [langfuse_handler]}`\n",
        "- 🖥️ **服务器部署**：需要在图编译时预先配置回调处理器\n",
        "\n",
        "**解决方案：**\n",
        "在声明和编译图时就添加 Langfuse 回调，这样服务器上的所有请求都会自动启用追踪功能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J5UKXzbUg-A0"
      },
      "outputs": [],
      "source": [
        "# 🔧 导入服务器部署所需的模块\n",
        "from typing import Annotated\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# 📋 定义与前文一致的智能体状态结构\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# 🏗️ 构建图形结构\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# 🤖 初始化语言模型\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
        "\n",
        "# 🔄 定义聊天机器人节点\n",
        "def chatbot(state: State):\n",
        "    \"\"\"处理用户消息并生成回复。\"\"\"\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "# 🔗 组装图形结构\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "\n",
        "# 🔄 初始化 Langfuse 回调处理器（服务器模式）\n",
        "# 在服务器环境中，此处理器会自动追踪所有请求的执行情况\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "# ⚙️ 编译图形并预配置回调处理器\n",
        "# 🎯 核心方法：with_config()\n",
        "# - compile()：编译图形，生成可执行的 CompiledGraph\n",
        "# - with_config()：为编译后的图形设置默认配置（如回调处理器）\n",
        "#\n",
        "# 💡 工作流程：\n",
        "# 1. 编译图形得到 CompiledGraph 对象\n",
        "# 2. 调用 with_config() 注入 Langfuse 回调\n",
        "# 3. 返回一个已内置追踪能力的新图对象\n",
        "#\n",
        "# 🚀 优势：\n",
        "# - 无需在每次请求时手动添加回调配置\n",
        "# - 所有 API 请求都会自动写入 Langfuse 追踪\n",
        "# - 简化生产环境的部署与运维\n",
        "graph = graph_builder.compile().with_config({\"callbacks\": [langfuse_handler]})\n",
        "\n",
        "# 💡 部署提示：\n",
        "# 在 LangGraph Server 中直接引用此 graph，即可立即获得完整的追踪数据\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2W94eY19TR1"
      },
      "source": [
        "## 示例 2：基于 LangGraph 的多智能体应用\n",
        "\n",
        "**本节将完成：**\n",
        "\n",
        "- 构建 2 个执行智能体：一个研究智能体使用 LangChain 的 WikipediaAPIWrapper 搜索维基百科，另一个使用自定义工具获取当前时间\n",
        "- 构建一个智能体监督者（supervisor），用于将用户问题分配给上述智能体\n",
        "- 添加 Langfuse 回调以追踪监督者与执行智能体的步骤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WfnrswDdjYTV",
        "outputId": "4da1756a-98bd-43a4-fdb0-c0314c1aaaaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langfuse==3.3.0 in /usr/local/lib/python3.12/dist-packages (3.3.0)\n",
            "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-openai==0.3.31 in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langgraph==0.6.7 in /usr/local/lib/python3.12/dist-packages (0.6.7)\n",
            "Collecting langchain_experimental\n",
            "  Using cached langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.2.1)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.37.0)\n",
            "Requirement already satisfied: packaging<26.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.32.4)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.17.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.4.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.31) (1.107.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.31) (0.11.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7) (3.5.0)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_experimental) (0.3.27)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.3.0) (0.16.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.15.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.6.7) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph==0.6.7) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.24.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.37.0)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (5.29.5)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.3.0) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.31) (2024.11.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=a0cfe403b04fc6787680c1491497e5a56409f7981aeb7d862e3ef66fdcb371bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia, langchain_experimental\n",
            "Successfully installed langchain_experimental-0.3.4 wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse==3.3.0 langchain==0.3.27 langchain-openai==0.3.31 langgraph==0.6.7 langchain_experimental==0.3.4 pandas==2.2.2 wikipedia==1.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip show langchain_experimental pandas wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDwN3meK4vHk",
        "outputId": "6d2b5be8-7c7b-4fa4-93ef-c6c3822b75d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain-experimental\n",
            "Version: 0.3.4\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain-experimental\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-community, langchain-core\n",
            "Required-by: \n",
            "---\n",
            "Name: pandas\n",
            "Version: 2.2.2\n",
            "Summary: Powerful data structures for data analysis, time series, and statistics\n",
            "Home-page: https://pandas.pydata.org\n",
            "Author: \n",
            "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
            "License: BSD 3-Clause License\n",
            "\n",
            "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
            "All rights reserved.\n",
            "\n",
            "Copyright (c) 2011-2023, Open source contributors.\n",
            "\n",
            "Redistribution and use in source and binary forms, with or without\n",
            "modification, are permitted provided that the following conditions are met:\n",
            "\n",
            "* Redistributions of source code must retain the above copyright notice, this\n",
            "  list of conditions and the following disclaimer.\n",
            "\n",
            "* Redistributions in binary form must reproduce the above copyright notice,\n",
            "  this list of conditions and the following disclaimer in the documentation\n",
            "  and/or other materials provided with the distribution.\n",
            "\n",
            "* Neither the name of the copyright holder nor the names of its\n",
            "  contributors may be used to endorse or promote products derived from\n",
            "  this software without specific prior written permission.\n",
            "\n",
            "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
            "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
            "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: numpy, python-dateutil, pytz, tzdata\n",
            "Required-by: arviz, bigframes, bigquery-magics, bokeh, bqplot, cmdstanpy, cudf-cu12, cufflinks, dask-cuda, dask-cudf-cu12, datasets, db-dtypes, dopamine_rl, fastai, geemap, geopandas, google-colab, gradio, gspread-dataframe, holoviews, libpysal, mizani, mlxtend, pandas-datareader, pandas-gbq, panel, plotnine, prophet, pymc, seaborn, shap, sklearn-pandas, statsmodels, tensorflow_decision_forests, tsfresh, vega-datasets, xarray, yfinance\n",
            "---\n",
            "Name: wikipedia\n",
            "Version: 1.4.0\n",
            "Summary: Wikipedia API for Python\n",
            "Home-page: https://github.com/goldsmith/Wikipedia\n",
            "Author: Jonathan Goldsmith\n",
            "Author-email: jhghank@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: beautifulsoup4, requests\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tciUQ62IEVec"
      },
      "source": [
        "### 创建工具\n",
        "\n",
        "在本示例中，我们将构建一个用于维基百科检索的智能体，以及一个用于告知当前时间的智能体。先定义它们将使用的工具："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Cet0loyp9p-T"
      },
      "outputs": [],
      "source": [
        "# 🔧 导入多智能体系统所需的工具和模块\n",
        "from typing import Annotated\n",
        "from langchain_community.tools import WikipediaQueryRun  # 维基百科查询工具\n",
        "from langchain_community.utilities import WikipediaAPIWrapper  # 维基百科 API 封装\n",
        "from datetime import datetime  # 时间处理模块\n",
        "from langchain.tools import Tool  # 通用工具定义类\n",
        "\n",
        "# 🔍 定义维基百科搜索工具\n",
        "# 功能：根据查询词在维基百科中搜索相关信息\n",
        "# 适用场景：回答百科知识、历史事件、人物传记等问题\n",
        "wikipedia_tool = WikipediaQueryRun(\n",
        "    api_wrapper=WikipediaAPIWrapper(\n",
        "        top_k_results=2,  # 返回最相关的2个搜索结果\n",
        "        doc_content_chars_max=1000  # 限制每个结果的字符数，避免信息过载\n",
        "    )\n",
        ")\n",
        "\n",
        "# ⏰ 定义当前时间查询工具\n",
        "# 功能：返回当前的日期和时间信息\n",
        "# 适用场景：回答\"现在几点\"、\"今天是什么日期\"等时间相关问题\n",
        "datetime_tool = Tool(\n",
        "    name=\"Datetime\",  # 工具名称，智能体会通过这个名称调用工具\n",
        "    func=lambda x: datetime.now().isoformat(),  # 工具函数：返回 ISO 格式的当前时间\n",
        "    description=\"返回当前的日期和时间信息（ISO 格式）\",  # 工具描述，帮助智能体理解何时使用此工具\n",
        ")\n",
        "\n",
        "# 💡 工具设计原则：\n",
        "# 1. 🎯 单一职责：每个工具只负责一个特定功能\n",
        "# 2. 📝 清晰描述：description 要准确描述工具的功能和使用场景\n",
        "# 3. 🔒 错误处理：生产环境中应该添加异常处理逻辑\n",
        "# 4. ⚡ 性能考虑：限制返回数据的大小，避免影响整体性能"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31uhDy_mEqr6"
      },
      "source": [
        "### 🛠️ 辅助工具函数\n",
        "\n",
        "#### 📝 功能说明\n",
        "下面定义的辅助函数用于简化添加新的智能体工作节点。这些函数封装了创建智能体和节点的通用逻辑，提高代码的可重用性和可维护性。\n",
        "\n",
        "#### 🎯 设计目标\n",
        "- **减少重复代码**：避免为每个智能体重复编写相同的初始化逻辑\n",
        "- **标准化接口**：确保所有智能体节点具有一致的输入输出格式\n",
        "- **简化扩展**：新增智能体时只需关注业务逻辑，无需处理框架细节"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "75atiExdqd4P"
      },
      "outputs": [],
      "source": [
        "# 🔧 导入智能体构建所需的核心组件\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent  # 智能体执行器和创建函数\n",
        "from langchain_core.messages import BaseMessage, HumanMessage  # 消息基类和人类消息\n",
        "from langchain_openai import ChatOpenAI  # OpenAI 聊天模型\n",
        "\n",
        "def create_agent(llm: ChatOpenAI, system_prompt: str, tools: list):\n",
        "    \"\"\"\n",
        "    🏭 智能体工厂函数：创建具有特定能力的工作智能体\n",
        "\n",
        "    参数:\n",
        "        llm (ChatOpenAI): 语言模型实例\n",
        "        system_prompt (str): 系统提示词，定义智能体的角色和行为规范\n",
        "        tools (list): 智能体可使用的工具列表\n",
        "\n",
        "    返回:\n",
        "        AgentExecutor: 配置完成的智能体执行器\n",
        "\n",
        "    🔄 工作流程:\n",
        "    1. 构建提示模板（包含系统角色、对话历史、工具使用记录）\n",
        "    2. 创建支持工具调用的 OpenAI 智能体\n",
        "    3. 包装为执行器，处理工具调用和状态管理\n",
        "    \"\"\"\n",
        "    # 📋 构建智能体的提示模板\n",
        "    # 包含三个关键部分：系统角色、对话历史、工具使用记录\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        # 🎭 系统消息：定义智能体的角色、能力和行为规范\n",
        "        (\"system\", system_prompt),\n",
        "        # 💬 消息历史：保存与用户和其他智能体的对话记录\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        # 🔧 工具记录：记录智能体使用工具的过程和结果\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ])\n",
        "\n",
        "    # 🤖 创建支持 OpenAI 工具调用的智能体\n",
        "    # 这个智能体能够理解何时需要使用工具，以及如何解释工具的返回结果\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # 🎮 创建智能体执行器\n",
        "    # 执行器负责：工具调用、错误处理、状态管理、结果整合\n",
        "    executor = AgentExecutor(\n",
        "        agent=agent,\n",
        "        tools=tools,\n",
        "        verbose=True,  # 显示详细的执行过程，便于调试\n",
        "        handle_parsing_errors=True  # 自动处理解析错误，提高稳定性\n",
        "    )\n",
        "\n",
        "    return executor\n",
        "\n",
        "def agent_node(state, agent, name):\n",
        "    \"\"\"\n",
        "    🔄 智能体节点适配器：将智能体包装为 LangGraph 节点\n",
        "\n",
        "    参数:\n",
        "        state: 当前的图状态，包含消息历史和其他上下文信息\n",
        "        agent: 智能体执行器实例\n",
        "        name: 智能体的名称，用于在多智能体系统中标识消息来源\n",
        "\n",
        "    返回:\n",
        "        dict: 包含智能体响应的状态更新\n",
        "\n",
        "    🔄 适配过程:\n",
        "    1. 调用智能体处理当前状态\n",
        "    2. 提取智能体的输出结果\n",
        "    3. 包装为带有发送者身份的消息\n",
        "    4. 返回状态更新\n",
        "    \"\"\"\n",
        "    # 📤 调用智能体处理当前状态\n",
        "    # agent.invoke() 会处理对话历史，决定是否使用工具，并生成最终回复\n",
        "    result = agent.invoke(state)\n",
        "\n",
        "    # 🏷️ 将智能体的输出包装为带有身份标识的消息\n",
        "    # name 参数让系统知道这条消息来自哪个智能体\n",
        "    return {\n",
        "        \"messages\": [HumanMessage(\n",
        "            content=result[\"output\"],  # 智能体生成的文本内容\n",
        "            name=name  # 消息发送者的身份标识\n",
        "        )]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bZqwU6FCOa"
      },
      "source": [
        "### 🎯 创建智能体监督者\n",
        "\n",
        "#### 📋 监督者的核心职责\n",
        "智能体监督者是多智能体系统的\"大脑\"，负责：\n",
        "\n",
        "- 🧠 **任务理解**：分析用户请求，理解任务的性质和需求\n",
        "- 🎯 **智能体选择**：根据任务特点选择最适合的工作智能体\n",
        "- 🔄 **流程控制**：决定何时切换智能体，何时结束处理流程\n",
        "- 📊 **结果整合**：汇总各个智能体的工作成果\n",
        "\n",
        "#### 🔧 技术实现方式\n",
        "监督者使用 **函数调用（Function Calling）** 技术来实现决策：\n",
        "\n",
        "- 📞 **函数调用**：通过结构化的函数调用来表达决策结果\n",
        "- 🎛️ **选择机制**：在可用的工作节点中选择下一个执行者\n",
        "- 🏁 **终止条件**：判断何时任务已完成，可以结束处理流程\n",
        "\n",
        "#### 💡 设计优势\n",
        "- **精确控制**：避免随机或不确定的路由决策\n",
        "- **可解释性**：每个决策都有明确的逻辑依据\n",
        "- **可扩展性**：容易添加新的工作智能体和决策规则"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Hu8MzgihrHdF"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "members = [\"Researcher\", \"CurrentTime\"]\n",
        "system_prompt = (\n",
        "    \"你是一名监督者，\"\n",
        "    \" 负责管理以下工作人员之间的对话：{members}。\"\n",
        "    \" 根据以下用户请求，回复下一个要采取行动的工作人员。\"\n",
        "    \" 每个工作人员将执行一项任务，并回复其结果和状态。\"\n",
        "    \" 完成后，回复 FINISH。\"\n",
        ")\n",
        "# 🧭 监督者节点由 LLM 扮演，负责选择下一位执行的智能体并判断流程是否结束\n",
        "options = [\"FINISH\"] + members\n",
        "\n",
        "# 🔁 使用 OpenAI Function Calling，可让结构化输出和解析更加稳定\n",
        "function_def = {\n",
        "    \"name\": \"route\",\n",
        "    \"description\": \"选择下一个角色\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"routeSchema\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"next\": {\n",
        "                \"title\": \"Next\",\n",
        "                \"anyOf\": [\n",
        "                    {\"enum\": options},\n",
        "                ],\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"next\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# 📜 构建监督者提示模板\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"system\",\n",
        "            \"根据上面的对话，接下来谁应该行动？\"\n",
        "            \" 或者我们应该FINISH吗？请从以下选项中选择一个：{options}\",\n",
        "        ),\n",
        "    ]\n",
        ").partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# 🔗 构建监督者智能体的执行链\n",
        "supervisor_chain = (\n",
        "    prompt\n",
        "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "    | JsonOutputFunctionsParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ognuMaIeFVh7"
      },
      "source": [
        "### 构建图形结构\n",
        "\n",
        "现在可以开始搭建整张图。下面使用刚刚定义的函数指定状态和各个工作节点，并连接图中的所有边。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_LwtCmw_rHVz"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import operator\n",
        "from typing import Sequence, TypedDict\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "# 🗂️ 智能体状态会作为每个节点的输入数据\n",
        "class AgentState(TypedDict):\n",
        "    # Annotated 告诉图：新的消息会追加到现有消息列表中\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    # next 字段指示下一步要跳转到哪个节点\n",
        "    next: str\n",
        "\n",
        "# 🧑‍💻 使用辅助函数创建研究智能体\n",
        "research_agent = create_agent(llm, \"您是一位网络研究员。\", [wikipedia_tool])\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
        "\n",
        "# 🕰️ 创建报时智能体\n",
        "currenttime_agent = create_agent(llm, \"Y您是一位报时助手，负责告知当前时间。\", [datetime_tool])\n",
        "currenttime_node = functools.partial(agent_node, agent=currenttime_agent, name=\"CurrentTime\")\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# 📦 注册节点：节点代表具体的工作单元，通常是 Python 函数\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"CurrentTime\", currenttime_node)\n",
        "workflow.add_node(\"supervisor\", supervisor_chain)\n",
        "\n",
        "# 🔂 强制所有工作节点在完成后回到监督者\n",
        "for member in members:\n",
        "    workflow.add_edge(member, \"supervisor\")\n",
        "\n",
        "# 🔀 条件边根据当前状态决定后续路由\n",
        "# 该函数读取状态并返回要执行的下一个节点名称\n",
        "conditional_map = {k: k for k in members}\n",
        "conditional_map[\"FINISH\"] = END\n",
        "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
        "\n",
        "# 🚪 设置入口节点，确定图在运行时从哪里开始\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "\n",
        "# ⚙️ 编译图形，得到可调用的 CompiledGraph\n",
        "graph_2 = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3xfJLJyFwBG"
      },
      "source": [
        "### 在调用中挂载 Langfuse 回调\n",
        "\n",
        "在执行 `graph_2.stream` 时增加 [Langfuse 回调处理器](https://langfuse.com/integrations/frameworks/langchain)：`config={\"callbacks\": [langfuse_handler]}`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "QsX1gw9kryGP",
        "outputId": "357b67b7-f525-4332-fdbb-3bd9420b08b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'supervisor': {'next': 'Researcher'}}\n",
            "----\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for ToolAgentAction\ntool_call_id\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-920772648.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 🔗 将回调处理器挂载到图的 stream 调用中；可选的 run_name 会作为追踪名称展示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m for s in graph_2.stream({\"messages\": [HumanMessage(content=\"光合作用是如何进行的？\")]},\n\u001b[0m\u001b[1;32m      8\u001b[0m                        config={\"callbacks\": [langfuse_handler]}):\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1676007668.py\u001b[0m in \u001b[0;36magent_node\u001b[0;34m(state, agent, name)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# 📤 调用智能体处理当前状态\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# agent.invoke() 会处理对话历史，决定是否使用工具，并生成最终回复\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# 🏷️ 将智能体的输出包装为带有身份标识的消息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             outputs = (\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m   1626\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1323\u001b[0m     ) -> Union[AgentFinish, list[tuple[AgentAction, str]]]:\n\u001b[1;32m   1324\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1325\u001b[0;31m             list(\n\u001b[0m\u001b[1;32m   1326\u001b[0m                 self._iter_next_step(\n\u001b[1;32m   1327\u001b[0m                     \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0;31m# Call the LLM to see what to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             output = self._action_agent.plan(\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0mintermediate_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36mplan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;31m# Because the response from the plan is not a generator, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# accumulate the output into final output and return that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfinal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 3473\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3457\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3458\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 3459\u001b[0;31m         yield from self._transform_stream_with_config(\n\u001b[0m\u001b[1;32m   3460\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_transform_stream_with_config\u001b[0;34m(self, inputs, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   2231\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m                         \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mfinal_output_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, inputs, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3419\u001b[0m                 \u001b[0mfinal_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3421\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3423\u001b[0m     async def _atransform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgot_first_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m     async def atransform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \"\"\"\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     async def astream(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     ) -> T:\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m    198\u001b[0m                 lambda inner_input: self.parse_result(\n\u001b[1;32m    199\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m                 output = cast(\n\u001b[1;32m   1952\u001b[0m                     \u001b[0;34m\"Output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m                     context.run(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                         \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             return self._call_with_config(\n\u001b[0;32m--> 198\u001b[0;31m                 lambda inner_input: self.parse_result(\n\u001b[0m\u001b[1;32m    199\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 ),\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/output_parsers/openai_tools.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: TRY004\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparse_ai_message_to_openai_tool_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAgentAction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgentFinish\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/output_parsers/openai_tools.py\u001b[0m in \u001b[0;36mparse_ai_message_to_openai_tool_action\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     18\u001b[0m ) -> Union[list[AgentAction], AgentFinish]:\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"\"\"Parse an AI message potentially containing tool_calls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtool_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_ai_message_to_tool_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgentFinish\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtool_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/output_parsers/tools.py\u001b[0m in \u001b[0;36mparse_ai_message_to_tool_action\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\nInvoking: `{function_name}` with `{tool_input}`\\n{content_msg}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         actions.append(\n\u001b[0;32m---> 69\u001b[0;31m             ToolAgentAction(\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mtool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mtool_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/agents.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tool, tool_input, log, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mto\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ToolAgentAction\ntool_call_id\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
          ]
        }
      ],
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# 📡 初始化 Langfuse 回调处理器，用于记录 LangChain 的执行轨迹\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "# 🔗 将回调处理器挂载到图的 stream 调用中；可选的 run_name 会作为追踪名称展示\n",
        "for s in graph_2.stream({\"messages\": [HumanMessage(content=\"光合作用是如何进行的？\")]},\n",
        "                       config={\"callbacks\": [langfuse_handler]}):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqJnMtP5HDql"
      },
      "outputs": [],
      "source": [
        "# 🔗 同样地，为其他查询挂载 Langfuse 回调\n",
        "for s in graph_2.stream({\"messages\": [HumanMessage(content=\"现在的准确时间是多少？\")]},\n",
        "                       config={\"callbacks\": [langfuse_handler]}):\n",
        "    print(s)\n",
        "    print(\"----\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4XjtNenH9GF"
      },
      "source": [
        "### 在 Langfuse 中查看多智能体追踪\n",
        "\n",
        "以下链接展示了本节多智能体示例在 Langfuse 中生成的追踪记录：\n",
        "\n",
        "1. [光合作用是如何进行的？](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7d5f970573b8214d1ca891251e42282c)\n",
        "2. [现在几点？](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/3a69fe4998df50d42054f8944bd6a8d9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-5EEBZAIbwc"
      },
      "source": [
        "![在 Langfuse 中查看多智能体追踪](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_multiagent_traces.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCEzabn_jhbf"
      },
      "source": [
        "### 可视化该智能体\n",
        "\n",
        "你可以使用 `get_graph` 方法配合相应的 “draw” 方法进行图形可视化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "notlPjnl-HXV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(graph_2.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mESkG2IJS8OY"
      },
      "source": [
        "```mermaid\n",
        "graph TD;\n",
        "\t__start__([__start__]):::first\n",
        "\tResearcher(Researcher)\n",
        "\tCurrentTime(CurrentTime)\n",
        "\tsupervisor(supervisor)\n",
        "\t__end__([__end__]):::last\n",
        "\tCurrentTime --> supervisor;\n",
        "\tResearcher --> supervisor;\n",
        "\t__start__ --> supervisor;\n",
        "\tsupervisor -.-> Researcher;\n",
        "\tsupervisor -.-> CurrentTime;\n",
        "\tsupervisor -. &nbspFINISH&nbsp .-> __end__;\n",
        "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
        "\tclassDef first fill-opacity:0\n",
        "\tclassDef last fill:#bfb6fc\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvKULkc6g-BE"
      },
      "source": [
        "## 多个 LangGraph 智能体的协同\n",
        "\n",
        "在某些架构中，一个 LangGraph 智能体会调用一个或多个其他 LangGraph 智能体。若想让整套执行链在 Langfuse 中聚合为同一条追踪，可显式传入自定义的 `trace_id`。\n",
        "\n",
        "首先生成一个共享的 `trace_id`，供主智能体与子智能体共用，以便在 Langfuse 中合并为同一条记录。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS8hQBHVg-BE"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client, Langfuse\n",
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "# 🔐 从外部系统生成一个确定性的 trace_id，便于跨服务聚合\n",
        "predefined_trace_id = Langfuse.create_trace_id()\n",
        "\n",
        "# 📡 初始化 Langfuse 回调处理器，用于采集 LangChain 的执行数据\n",
        "langfuse_handler = CallbackHandler()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73necyiUg-BE"
      },
      "source": [
        "接下来，构建子智能体的逻辑。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWS82VWsg-BE"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "sub_agent = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIcpIMEPg-BF"
      },
      "source": [
        "随后，将该子智能体封装成工具，供主流程调用并复用。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0KC8Gvzg-BF"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def langgraph_research(question):\n",
        "  \"\"\"Conducts research for various topics.\"\"\"\n",
        "\n",
        "  with langfuse.start_as_current_span(\n",
        "      name=\"🤖-sub-research-agent\",\n",
        "      trace_context={\"trace_id\": predefined_trace_id}\n",
        "  ) as span:\n",
        "      span.update_trace(input=question)\n",
        "\n",
        "      response = sub_agent.invoke({\"messages\": [HumanMessage(content = question)]},\n",
        "                        config={\"callbacks\": [langfuse_handler]})\n",
        "\n",
        "      span.update_trace(output= response[\"messages\"][1].content)\n",
        "\n",
        "  return response[\"messages\"][1].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PpMQXiEg-BF"
      },
      "source": [
        "最后，创建第二个 LangGraph 智能体，通过前面新增的 `langgraph_research` 工具完成协作。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhNPp4pmg-BF"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
        "\n",
        "main_agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[langgraph_research]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyeGvSf3g-BF"
      },
      "outputs": [],
      "source": [
        "user_question = \"什么是 Langfuse？\"\n",
        "\n",
        "# 🧭 使用预生成的 trace_id（通过 trace_context 注入）\n",
        "with langfuse.start_as_current_span(\n",
        "    name=\"🤖-main-agent\",\n",
        "    trace_context={\"trace_id\": predefined_trace_id}\n",
        ") as span:\n",
        "    span.update_trace(input=user_question)\n",
        "\n",
        "    # 此处的 LangChain 执行都会归属于同一条追踪\n",
        "    response = main_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_question}]},\n",
        "                            config={\"callbacks\": [langfuse_handler]})\n",
        "\n",
        "    span.update_trace(output=response[\"messages\"][1].content)\n",
        "\n",
        "print(f\"Trace ID: {predefined_trace_id}\")  # 可在后续评分或排查时使用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzPZIqUWg-BF"
      },
      "source": [
        "### 在 Langfuse 中查看多智能体追踪\n",
        "\n",
        "![多智能体追踪示例](https://langfuse.com/images/cookbook/integration-langgraph/a2a_langgraph.png)\n",
        "\n",
        "示例追踪链接：https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uybP4h8wGvWw"
      },
      "source": [
        "## 为追踪添加评分\n",
        "\n",
        "[评分（Score）](https://langfuse.com/docs/scores/overview) 用于评价单个观测（observation）或整条追踪（trace），可帮助你在运行时执行自定义质量检查，或配合人工审核流程。\n",
        "\n",
        "下面的示例演示如何：\n",
        "\n",
        "- 为某个 span 记录一个数值型评分（如 `relevance`）\n",
        "- 为整条追踪记录一个分类型评分（如 `feedback`）\n",
        "\n",
        "这有助于系统化地评估与改进应用质量。\n",
        "\n",
        "**→ 想深入了解？请参阅 [Langfuse 自定义评分指南](https://langfuse.com/docs/scores/custom)。**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgAqYnQuGwCL"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "# 方案一：使用上下文管理器返回的 span 对象进行评分\n",
        "with langfuse.start_as_current_span(\n",
        "    name=\"langgraph-request\") as span:\n",
        "    # ... 此处执行 LangGraph 逻辑 ...\n",
        "\n",
        "    # 直接通过 span.score_trace 记录评分\n",
        "    span.score_trace(\n",
        "        name=\"user-feedback\",\n",
        "        value=1,\n",
        "        data_type=\"NUMERIC\",\n",
        "        comment=\"This was correct, thank you\"\n",
        "    )\n",
        "\n",
        "# 方案二：在仍位于上下文时调用 score_current_trace()\n",
        "with langfuse.start_as_current_span(name=\"langgraph-request\") as span:\n",
        "    # ... LangGraph execution ...\n",
        "\n",
        "    langfuse.score_current_trace(\n",
        "        name=\"user-feedback\",\n",
        "        value=1,\n",
        "        data_type=\"NUMERIC\"\n",
        "    )\n",
        "\n",
        "# 方案三：若已离开上下文，可使用 trace_id 直接创建评分\n",
        "langfuse.create_score(\n",
        "    trace_id=predefined_trace_id,\n",
        "    name=\"user-feedback\",\n",
        "    value=1,\n",
        "    data_type=\"NUMERIC\",\n",
        "    comment=\"This was correct, thank you\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq_DeCcXSxwq"
      },
      "source": [
        "### 在 Langfuse 中查看带评分的追踪\n",
        "\n",
        "示例追踪：https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e60a078b828d4fdc7ea22c73193b0fe4\n",
        "\n",
        "![包含评分的追踪展示](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_score.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cIQVrYZJVMO"
      },
      "source": [
        "## 使用 Langfuse 管理提示词\n",
        "\n",
        "通过 [Langfuse Prompt Management](https://langfuse.com/docs/prompts/example-langchain) 可以对提示词进行统一的版本管理。在本示例中，我们演示如何通过 SDK 新增一条提示词；在实际生产环境中，更推荐直接在 Langfuse UI 中创建和发布。\n",
        "\n",
        "Langfuse 的提示词管理相当于一个“提示词内容管理系统（Prompt CMS）”。你可以在 UI 中编辑、预览与发布；也可在代码中通过 SDK 读取或更新。\n",
        "\n",
        "配置提示词时通常需要指定：\n",
        "\n",
        "* `name`：在 Langfuse 中唯一标识该提示词的名称\n",
        "* `prompt`：包含模板占位符（如 `{{input variables}}`）的文本内容\n",
        "* `labels`：可设置为 `production`，让该版本立即成为默认选项\n",
        "\n",
        "在本例中，我们创建一个系统提示词，让助手将所有用户输入翻译成西班牙语。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0J8-nbhUUz6"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "langfuse.create_prompt(\n",
        "    name=\"translator_system-prompt\",\n",
        "    prompt=\"You are a translator that translates every input text into Spanish.\",\n",
        "    labels=[\"production\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dullp4XDXhzg"
      },
      "source": [
        "![在 Langfuse UI 中查看提示词](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_prompt_example.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNboOjf2YQpD"
      },
      "source": [
        "使用工具方法 `.get_langchain_prompt()` 可以将 Langfuse 中的提示词转换为 LangChain 可直接使用的字符串。\n",
        "\n",
        "**背景说明：** Langfuse 在提示模板中使用双花括号（`{{input variable}}`）声明变量；而 LangChain 的 `PromptTemplate` 使用单花括号（`{input variable}`）。`.get_langchain_prompt()` 会自动完成格式转换。当前示例的提示词没有占位变量，但仍可以统一通过该方法处理。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z49I82blYeXy"
      },
      "outputs": [],
      "source": [
        "# 读取生产环境中最新的提示词版本，并转换为 LangChain 可直接使用的字符串\n",
        "langfuse_system_prompt = langfuse.get_prompt(\"translator_system-prompt\")\n",
        "langchain_system_prompt = langfuse_system_prompt.get_langchain_prompt()\n",
        "\n",
        "print(langchain_system_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3zBULfCt0Wq"
      },
      "source": [
        "现在可以使用新的系统提示词字符串，更新我们的翻译助手。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGQhulyMmvZD"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
        "\n",
        "# 🧩 将系统提示词注入到聊天节点，确保助手先遵循翻译指令\n",
        "system_prompt = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": langchain_system_prompt\n",
        "}\n",
        "\n",
        "def chatbot(state: State):\n",
        "    messages_with_system_prompt = [system_prompt] + state[\"messages\"]\n",
        "    response = llm.invoke(messages_with_system_prompt)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "graph = graph_builder.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYd7wbttm2ec"
      },
      "outputs": [],
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# 📡 初始化 Langfuse 回调处理器（用于追踪翻译助手的调用）\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "# 🔗 将回调挂载到图的 stream 方法中，实时查看翻译结果与追踪记录\n",
        "for s in graph.stream({\"messages\": [HumanMessage(content=\"请把“Langfuse 是什么？”翻译成西班牙语。\")]},\n",
        "                      config={\"callbacks\": [langfuse_handler]}):\n",
        "    print(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsAfdodAg-BH"
      },
      "source": [
        "## 在 LangGraph 追踪中添加自定义 Span\n",
        "\n",
        "某些场景下，我们希望在追踪中插入自定义 span，以标记关键步骤或附加额外上下文。可以参考这个 [GitHub 讨论贴](https://github.com/orgs/langfuse/discussions/2988#discussioncomment-11634600) 中给出的示例实现方式。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}