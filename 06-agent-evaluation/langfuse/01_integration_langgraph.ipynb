{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPJsVa1O4TuL"
   },
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Open Source Observability for LangGraph\" description: \"Learn how to use Langfuse for open source observability/tracing in your LangGraph application (Python).\" category: \"Integrations\" -->\n",
    "\n",
    "# 实践手册：LangGraph 集成\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlCI9KeX4Zn4"
   },
   "source": [
    "## 什么是 LangGraph？\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) 是由 LangChain 团队开源的框架，用于基于大语言模型（LLM）构建复杂、有状态的多智能体应用。LangGraph 内置了持久化能力，可保存与恢复状态，从而支持错误恢复与包含“人类参与”（Human-in-the-loop, HITL）的工作流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o8L1qPcaZeC"
   },
   "source": [
    "## 本实践手册的目标\n",
    "\n",
    "本手册演示如何借助 [Langfuse](https://langfuse.com/docs)，通过其与 [LangChain 的集成](https://langfuse.com/integrations/frameworks/langchain)，对你的 LangGraph 应用进行调试、分析与迭代优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPTaMtxH4eHV"
   },
   "source": [
    "**完成本手册后，你将能够：**\n",
    "\n",
    "- 自动通过 Langfuse 集成对 LangGraph 应用进行追踪（tracing）\n",
    "- 监控复杂的多智能体（multi-agent）方案\n",
    "- 添加评分（例如用户反馈）\n",
    "- 使用 Langfuse 管理 LangGraph 中使用的提示词（prompt）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sSIS88y9Ewm"
   },
   "source": [
    "## 初始化 Langfuse\n",
    "\n",
    "在 Langfuse 控制台项目设置页获取你的 [API 密钥](https://langfuse.com/faq/all/where-are-langfuse-api-keys)，并将其加入到运行环境变量中以初始化 Langfuse 客户端。\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"⚠️\" -->\n",
    "_**注意：** 本笔记使用 Langfuse Python SDK v3。若你仍在使用 v2，请参阅我们的[旧版 LangGraph 集成指南](https://github.com/langfuse/langfuse-docs/blob/662509b3296daddcddb292f14b10a62e7c39407d/cookbook/integration_langgraph.ipynb)。_\n",
    "<!-- CALLOUT_END -->\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"ℹ️\" -->\n",
    "_**注意：** 需要至少 Python 3.11（参见 [GitHub Issue](https://github.com/langfuse/langfuse/issues/1926)）。_\n",
    "<!-- CALLOUT_END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C85BK1vJ5yD3",
    "outputId": "73f44b09-ae33-4bd0-8e92-9c1bfc8a1e7c"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse langchain langgraph langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1yglQ464VD-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 🔑 从项目设置页面获取您的 API 密钥：https://cloud.langfuse.com\n",
    "# 📋 配置步骤：\n",
    "# 1. 登录 Langfuse 控制台\n",
    "# 2. 选择或创建项目\n",
    "# 3. 进入项目设置页面\n",
    "# 4. 复制公钥和私钥\n",
    "\n",
    "# Langfuse 公钥（用于客户端身份验证）\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "\n",
    "# Langfuse 私钥（用于服务端身份验证，请妥善保管）\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "\n",
    "# Langfuse 服务器地址（选择离您最近的区域以获得更好性能）\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # 🇪🇺 欧洲区域\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"  # 🇺🇸 美国区域\n",
    "\n",
    "# 🤖 OpenAI API 密钥\n",
    "# 从 https://platform.openai.com/api-keys 获取\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "\n",
    "# 🚨 安全提醒：\n",
    "# - 不要将真实密钥提交到公共代码仓库\n",
    "# - 生产环境建议使用环境变量文件（.env）或密钥管理服务\n",
    "# - 定期轮换 API 密钥以提高安全性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在环境变量设置完成后，我们即可初始化 Langfuse 客户端。`get_client()` 会使用环境变量中提供的凭据来初始化 Langfuse 客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "# 🚀 初始化 Langfuse 客户端\n",
    "# get_client() 会自动读取环境变量中的配置信息\n",
    "langfuse = get_client()\n",
    " \n",
    "# 🔍 验证客户端连接状态\n",
    "# 这个步骤非常重要，确保后续的追踪功能能够正常工作\n",
    "if langfuse.auth_check():\n",
    "    print(\"✅ Langfuse 客户端已通过身份验证，准备就绪！\")\n",
    "    print(\"🔧 现在可以开始使用追踪功能了\")\n",
    "else:\n",
    "    print(\"❌ 身份验证失败！\")\n",
    "    print(\"🔍 请检查以下配置项：\")\n",
    "    print(\"   - LANGFUSE_PUBLIC_KEY 是否正确\")\n",
    "    print(\"   - LANGFUSE_SECRET_KEY 是否正确\") \n",
    "    print(\"   - LANGFUSE_HOST 是否可访问\")\n",
    "    print(\"   - 网络连接是否正常\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqYMmi6n9Nh1"
   },
   "source": [
    "## 示例 1：使用 LangGraph 构建简单聊天应用\n",
    "\n",
    "**本节将完成：**\n",
    "\n",
    "- 在 LangGraph 中构建一个可回答常见问题的客服聊天机器人\n",
    "- 使用 Langfuse 对机器人的输入与输出进行追踪（tracing）\n",
    "\n",
    "我们先从一个基础机器人入手，随后在下一节扩展为更高级的多智能体（multi-agent）设置，并在过程中介绍关键的 LangGraph 概念。\n",
    "\n",
    "### 创建智能体（Agent）\n",
    "\n",
    "首先创建一个 `StateGraph`。`StateGraph` 定义了聊天机器人的状态机结构。我们会添加节点来表示 LLM 以及机器人可调用的函数，并通过边（edge）定义机器人在这些函数之间的状态流转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGIxgPww6VX6"
   },
   "outputs": [],
   "source": [
    "# 🔧 导入 LangGraph 构建智能体所需的核心模块\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI  # OpenAI 聊天模型\n",
    "from langchain_core.messages import HumanMessage  # 人类消息类型\n",
    "from typing_extensions import TypedDict  # 类型化字典\n",
    "from langgraph.graph import StateGraph  # LangGraph 状态图\n",
    "from langgraph.graph.message import add_messages  # 消息添加函数\n",
    "\n",
    "# 📋 定义智能体的状态结构\n",
    "# State 是一个类型化字典，定义了智能体在执行过程中需要维护的状态信息\n",
    "class State(TypedDict):\n",
    "    # 💬 消息列表：存储对话历史\n",
    "    # Annotated[list, add_messages] 的含义：\n",
    "    # - list: 消息的数据类型是列表\n",
    "    # - add_messages: 指定状态更新策略，新消息会追加到列表末尾而不是覆盖整个列表\n",
    "    # 这种设计确保了对话历史的完整保存\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 创建状态图构建器\n",
    "# StateGraph 是 LangGraph 的核心组件，用于定义智能体的工作流程\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 初始化语言模型\n",
    "# 选择 GPT-4o 模型，temperature=0.2 确保输出相对稳定但仍有一定创造性\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 🔄 定义聊天机器人节点函数\n",
    "# 这是 LangGraph 节点函数的基本模式：接收当前状态，返回更新后的状态\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    聊天机器人节点的核心逻辑\n",
    "    \n",
    "    参数:\n",
    "        state (State): 当前的智能体状态，包含消息历史\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含新生成消息的状态更新\n",
    "    \n",
    "    工作流程:\n",
    "    1. 获取当前的消息历史\n",
    "    2. 将消息历史发送给语言模型\n",
    "    3. 接收模型生成的回复\n",
    "    4. 将回复包装成状态更新返回\n",
    "    \"\"\"\n",
    "    # 调用语言模型处理当前对话历史，生成回复\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    # 返回状态更新：将模型的回复添加到消息列表中\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 🔗 向图中添加\"chatbot\"节点\n",
    "# 节点代表工作单元，通常是普通的 Python 函数\n",
    "# 每个节点负责特定的处理逻辑，如调用 LLM、处理工具、数据转换等\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 🚀 设置图的入口点\n",
    "# 告诉图每次运行时从哪个节点开始执行\n",
    "# 在这个简单示例中，我们直接从 chatbot 节点开始\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# 🏁 设置图的结束点\n",
    "# 指示图\"当这个节点运行完成后，可以退出执行\"\n",
    "# 对于简单的单轮对话，chatbot 节点执行完就可以结束\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# ⚙️ 编译图形为可执行对象\n",
    "# compile() 方法将图构建器转换为 CompiledGraph\n",
    "# CompiledGraph 是可以实际运行的图形对象，支持 invoke、stream 等方法\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 💡 理解 LangGraph 的核心概念：\n",
    "# 🏗️ StateGraph: 定义智能体的状态和工作流程\n",
    "# 🔄 Node: 执行具体任务的函数，如调用 LLM、使用工具等\n",
    "# 🔗 Edge: 连接节点，定义执行顺序和条件跳转\n",
    "# 📊 State: 智能体运行过程中维护的数据结构\n",
    "# ⚙️ CompiledGraph: 编译后的可执行图形对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW2SJcRgh7Xo"
   },
   "source": [
    "### 在调用时添加 Langfuse 回调\n",
    "\n",
    "现在，为了追踪应用执行过程，我们将添加 [面向 LangChain 的 Langfuse 回调处理器](https://langfuse.com/integrations/frameworks/langchain)：`config={\"callbacks\": [langfuse_handler]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PxEc455-KYM",
    "outputId": "0d1a6a04-a024-47b8-d320-72cd25b7aefd"
   },
   "outputs": [],
   "source": [
    "# 🔧 导入 Langfuse 的 LangChain 回调处理器\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 🔄 初始化 Langfuse 回调处理器\n",
    "# 这个处理器将自动捕获 LangChain/LangGraph 的执行过程，实现以下功能：\n",
    "# - 🕒 记录每个节点的执行时间和延迟\n",
    "# - 📝 捕获输入输出数据和中间状态\n",
    "# - 💰 跟踪 token 使用量和 API 调用成本\n",
    "# - 🐛 记录错误信息和异常堆栈（如果有）\n",
    "# - 📊 生成完整的执行链路追踪图\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# 🚀 运行智能体并启用 Langfuse 追踪\n",
    "# 示例对话：询问 Langfuse 的相关信息\n",
    "print(\"🤖 智能体开始运行，正在处理问题...\")\n",
    "print(\"❓ 问题：什么是 Langfuse？\")\n",
    "print(\"\\n📋 执行过程：\")\n",
    "\n",
    "# 使用 stream 方法可以实时查看智能体的执行步骤\n",
    "# graph.stream() 返回每个节点执行后的状态快照\n",
    "for step_result in graph.stream(\n",
    "    # 输入状态：包含用户的问题\n",
    "    {\"messages\": [HumanMessage(content=\"什么是 Langfuse？请详细介绍其主要功能和应用场景。\")]},\n",
    "    # 配置回调处理器以启用 Langfuse 追踪\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    "):\n",
    "    print(f\"📤 节点执行结果: {step_result}\")\n",
    "\n",
    "print(\"\\n✅ 智能体执行完成！\")\n",
    "print(\"🔍 您可以在 Langfuse 控制台中查看完整的执行追踪记录\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fdf3ZRnWGZ0N"
   },
   "source": [
    "### 在 Langfuse 中查看追踪结果\n",
    "\n",
    "示例追踪：`https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17Aq7u6_LBR6"
   },
   "source": [
    "![在 Langfuse 中查看聊天应用的追踪](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_chatapp_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3yyVtGKhMPU"
   },
   "source": [
    "### 可视化聊天应用\n",
    "\n",
    "你可以使用 `get_graph` 方法配合相应的 “draw” 方法对图进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "MKkM6mw47kIy",
    "outputId": "9cf8a453-05e0-4193-fc77-81cb176d9ef4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY0HW5xISntw"
   },
   "source": [
    "```mermaid\n",
    "graph TD;\n",
    "\t__start__([__start__]):::first\n",
    "\tchatbot(chatbot)\n",
    "\t__end__([__end__]):::last\n",
    "\t__start__ --> chatbot;\n",
    "\tchatbot --> __end__;\n",
    "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
    "\tclassDef first fill-opacity:0\n",
    "\tclassDef last fill:#bfb6fc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在 LangGraph Server 中使用 Langfuse\n",
    "\n",
    "#### 🖥️ 什么是 LangGraph Server？\n",
    "\n",
    "[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/) 是 LangGraph 的服务器部署解决方案，它提供：\n",
    "\n",
    "- 🌐 **HTTP API 接口**：将 LangGraph 智能体包装为 REST API 服务\n",
    "- 🚀 **生产就绪**：支持高并发、负载均衡和容器化部署\n",
    "- 🔧 **简化运维**：自动处理请求路由、状态管理和错误处理\n",
    "- 📊 **监控集成**：原生支持各种监控和追踪工具\n",
    "- 🔒 **安全性**：内置身份验证和授权机制\n",
    "\n",
    "#### 💡 为什么在 Server 中使用 Langfuse？\n",
    "\n",
    "在服务器环境中，Langfuse 追踪变得更加重要：\n",
    "\n",
    "- 🏭 **生产监控**：实时监控服务器上智能体的运行状态\n",
    "- 🐛 **远程调试**：无需本地复现即可分析线上问题\n",
    "- 📈 **性能分析**：识别性能瓶颈和优化机会\n",
    "- 💰 **成本控制**：精确跟踪 API 使用量和费用\n",
    "- 👥 **团队协作**：多人共享追踪数据和分析结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔧 配置方法说明\n",
    "\n",
    "使用 LangGraph Server 时，智能体图的调用由服务器自动处理，用户无法在每次请求时手动指定回调处理器。\n",
    "\n",
    "**关键差异：**\n",
    "- 🏠 **本地开发**：可以在每次调用时添加 `config={\"callbacks\": [langfuse_handler]}`\n",
    "- 🖥️ **服务器部署**：需要在图编译时预先配置回调处理器\n",
    "\n",
    "**解决方案：**\n",
    "在声明和编译图时就添加 Langfuse 回调，这样服务器上的所有请求都会自动启用追踪功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 导入服务器部署所需的模块\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 📋 定义智能体状态（与之前相同）\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 构建图形结构\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 初始化语言模型\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 🔄 定义聊天机器人节点\n",
    "def chatbot(state: State):\n",
    "    \"\"\"处理用户消息并生成回复\"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 🔗 组装图形结构\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# 🔄 初始化 Langfuse 回调处理器（服务器模式）\n",
    "# 在服务器环境中，这个处理器会自动处理所有请求的追踪\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# ⚙️ 编译图形并预配置回调处理器\n",
    "# 🎯 关键方法：with_config()\n",
    "# - compile(): 编译图形为可执行对象\n",
    "# - with_config(): 为编译后的图形添加默认配置\n",
    "# \n",
    "# 💡 工作原理：\n",
    "# 1. 首先编译图形，得到 CompiledGraph 对象\n",
    "# 2. 然后使用 with_config() 为图形设置默认回调\n",
    "# 3. 返回新的 CompiledGraph，内置了 Langfuse 追踪功能\n",
    "# \n",
    "# 🚀 优势：\n",
    "# - 无需在每次调用时手动添加回调配置\n",
    "# - 服务器上的所有请求都会自动启用追踪\n",
    "# - 简化了生产环境的部署和维护\n",
    "graph = graph_builder.compile().with_config({\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "# 💡 部署提示：\n",
    "# 在 LangGraph Server 中，可以直接使用这个预配置的 graph 对象\n",
    "# 所有通过 API 发送的请求都会自动记录到 Langfuse 中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2W94eY19TR1"
   },
   "source": [
    "## 示例 2：基于 LangGraph 的多智能体应用\n",
    "\n",
    "**本节将完成：**\n",
    "\n",
    "- 构建 2 个执行智能体：一个研究智能体使用 LangChain 的 WikipediaAPIWrapper 搜索维基百科，另一个使用自定义工具获取当前时间\n",
    "- 构建一个智能体监督者（supervisor），用于将用户问题分配给上述智能体\n",
    "- 添加 Langfuse 回调以追踪监督者与执行智能体的步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WfnrswDdjYTV",
    "outputId": "0d938cb1-9fd2-4ed3-cfdd-c84a9ad3ed82"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse langgraph langchain langchain_openai langchain_experimental pandas wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tciUQ62IEVec"
   },
   "source": [
    "### 创建工具\n",
    "\n",
    "在本示例中，我们将构建一个用于维基百科检索的智能体，以及一个用于告知当前时间的智能体。先定义它们将使用的工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cet0loyp9p-T"
   },
   "outputs": [],
   "source": [
    "# 🔧 导入多智能体系统所需的工具和模块\n",
    "from typing import Annotated\n",
    "from langchain_community.tools import WikipediaQueryRun  # 维基百科查询工具\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # 维基百科 API 封装\n",
    "from datetime import datetime  # 时间处理模块\n",
    "from langchain.tools import Tool  # 通用工具定义类\n",
    "\n",
    "# 🔍 定义维基百科搜索工具\n",
    "# 功能：根据查询词在维基百科中搜索相关信息\n",
    "# 适用场景：回答百科知识、历史事件、人物传记等问题\n",
    "wikipedia_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=2,  # 返回最相关的2个搜索结果\n",
    "        doc_content_chars_max=1000  # 限制每个结果的字符数，避免信息过载\n",
    "    )\n",
    ")\n",
    "\n",
    "# ⏰ 定义当前时间查询工具  \n",
    "# 功能：返回当前的日期和时间信息\n",
    "# 适用场景：回答\"现在几点\"、\"今天是什么日期\"等时间相关问题\n",
    "datetime_tool = Tool(\n",
    "    name=\"Datetime\",  # 工具名称，智能体会通过这个名称调用工具\n",
    "    func=lambda x: datetime.now().isoformat(),  # 工具函数：返回 ISO 格式的当前时间\n",
    "    description=\"返回当前的日期和时间信息（ISO 格式）\",  # 工具描述，帮助智能体理解何时使用此工具\n",
    ")\n",
    "\n",
    "# 💡 工具设计原则：\n",
    "# 1. 🎯 单一职责：每个工具只负责一个特定功能\n",
    "# 2. 📝 清晰描述：description 要准确描述工具的功能和使用场景\n",
    "# 3. 🔒 错误处理：生产环境中应该添加异常处理逻辑\n",
    "# 4. ⚡ 性能考虑：限制返回数据的大小，避免影响整体性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uhDy_mEqr6"
   },
   "source": [
    "### 🛠️ 辅助工具函数\n",
    "\n",
    "#### 📝 功能说明\n",
    "下面定义的辅助函数用于简化添加新的智能体工作节点。这些函数封装了创建智能体和节点的通用逻辑，提高代码的可重用性和可维护性。\n",
    "\n",
    "#### 🎯 设计目标\n",
    "- **减少重复代码**：避免为每个智能体重复编写相同的初始化逻辑\n",
    "- **标准化接口**：确保所有智能体节点具有一致的输入输出格式\n",
    "- **简化扩展**：新增智能体时只需关注业务逻辑，无需处理框架细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75atiExdqd4P"
   },
   "outputs": [],
   "source": [
    "# 🔧 导入智能体构建所需的核心组件\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent  # 智能体执行器和创建函数\n",
    "from langchain_core.messages import BaseMessage, HumanMessage  # 消息基类和人类消息\n",
    "from langchain_openai import ChatOpenAI  # OpenAI 聊天模型\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, system_prompt: str, tools: list):\n",
    "    \"\"\"\n",
    "    🏭 智能体工厂函数：创建具有特定能力的工作智能体\n",
    "    \n",
    "    参数:\n",
    "        llm (ChatOpenAI): 语言模型实例\n",
    "        system_prompt (str): 系统提示词，定义智能体的角色和行为规范\n",
    "        tools (list): 智能体可使用的工具列表\n",
    "    \n",
    "    返回:\n",
    "        AgentExecutor: 配置完成的智能体执行器\n",
    "    \n",
    "    🔄 工作流程:\n",
    "    1. 构建提示模板（包含系统角色、对话历史、工具使用记录）\n",
    "    2. 创建支持工具调用的 OpenAI 智能体\n",
    "    3. 包装为执行器，处理工具调用和状态管理\n",
    "    \"\"\"\n",
    "    # 📋 构建智能体的提示模板\n",
    "    # 包含三个关键部分：系统角色、对话历史、工具使用记录\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        # 🎭 系统消息：定义智能体的角色、能力和行为规范\n",
    "        (\"system\", system_prompt),\n",
    "        # 💬 消息历史：保存与用户和其他智能体的对话记录\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        # 🔧 工具记录：记录智能体使用工具的过程和结果\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    \n",
    "    # 🤖 创建支持 OpenAI 工具调用的智能体\n",
    "    # 这个智能体能够理解何时需要使用工具，以及如何解释工具的返回结果\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    \n",
    "    # 🎮 创建智能体执行器\n",
    "    # 执行器负责：工具调用、错误处理、状态管理、结果整合\n",
    "    executor = AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        verbose=True,  # 显示详细的执行过程，便于调试\n",
    "        handle_parsing_errors=True  # 自动处理解析错误，提高稳定性\n",
    "    )\n",
    "    \n",
    "    return executor\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    \"\"\"\n",
    "    🔄 智能体节点适配器：将智能体包装为 LangGraph 节点\n",
    "    \n",
    "    参数:\n",
    "        state: 当前的图状态，包含消息历史和其他上下文信息\n",
    "        agent: 智能体执行器实例\n",
    "        name: 智能体的名称，用于在多智能体系统中标识消息来源\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含智能体响应的状态更新\n",
    "    \n",
    "    🔄 适配过程:\n",
    "    1. 调用智能体处理当前状态\n",
    "    2. 提取智能体的输出结果\n",
    "    3. 包装为带有发送者身份的消息\n",
    "    4. 返回状态更新\n",
    "    \"\"\"\n",
    "    # 📤 调用智能体处理当前状态\n",
    "    # agent.invoke() 会处理对话历史，决定是否使用工具，并生成最终回复\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # 🏷️ 将智能体的输出包装为带有身份标识的消息\n",
    "    # name 参数让系统知道这条消息来自哪个智能体\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(\n",
    "            content=result[\"output\"],  # 智能体生成的文本内容\n",
    "            name=name  # 消息发送者的身份标识\n",
    "        )]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74bZqwU6FCOa"
   },
   "source": [
    "### 🎯 创建智能体监督者\n",
    "\n",
    "#### 📋 监督者的核心职责\n",
    "智能体监督者是多智能体系统的\"大脑\"，负责：\n",
    "\n",
    "- 🧠 **任务理解**：分析用户请求，理解任务的性质和需求\n",
    "- 🎯 **智能体选择**：根据任务特点选择最适合的工作智能体\n",
    "- 🔄 **流程控制**：决定何时切换智能体，何时结束处理流程\n",
    "- 📊 **结果整合**：汇总各个智能体的工作成果\n",
    "\n",
    "#### 🔧 技术实现方式\n",
    "监督者使用 **函数调用（Function Calling）** 技术来实现决策：\n",
    "\n",
    "- 📞 **函数调用**：通过结构化的函数调用来表达决策结果\n",
    "- 🎛️ **选择机制**：在可用的工作节点中选择下一个执行者\n",
    "- 🏁 **终止条件**：判断何时任务已完成，可以结束处理流程\n",
    "\n",
    "#### 💡 设计优势\n",
    "- **精确控制**：避免随机或不确定的路由决策\n",
    "- **可解释性**：每个决策都有明确的逻辑依据\n",
    "- **可扩展性**：容易添加新的工作智能体和决策规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu8MzgihrHdF"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Researcher\", \"CurrentTime\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create the prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Construction of the chain for the supervisor agent\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ognuMaIeFVh7"
   },
   "source": [
    "### Construct graph\n",
    "\n",
    "Now we are ready to start building the graph. Below, define the state and worker nodes using the function we just defined. Then we connect all the edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_LwtCmw_rHVz"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Sequence, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "# Add the research agent using the create_agent helper function\n",
    "research_agent = create_agent(llm, \"You are a web researcher.\", [wikipedia_tool])\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# Add the time agent using the create_agent helper function\n",
    "currenttime_agent = create_agent(llm, \"You can tell the current time at\", [datetime_tool])\n",
    "currenttime_node = functools.partial(agent_node, agent=currenttime_agent, name = \"CurrentTime\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add a \"chatbot\" node. Nodes represent units of work. They are typically regular python functions.\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"CurrentTime\", currenttime_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)\n",
    "\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "# Conditional edges usually contain \"if\" statements to route to different nodes depending on the current graph state.\n",
    "# These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Add an entry point. This tells our graph where to start its work each time we run it.\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "# To be able to run our graph, call \"compile()\" on the graph builder. This creates a \"CompiledGraph\" we can use invoke on our state.\n",
    "graph_2 = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3xfJLJyFwBG"
   },
   "source": [
    "### Add Langfuse as callback to the invocation\n",
    "\n",
    "Add [Langfuse handler](https://langfuse.com/integrations/frameworks/langchain) as callback: `config={\"callbacks\": [langfuse_handler]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsX1gw9kryGP",
    "outputId": "65d94f3c-17e7-4ad8-88b5-f837676d206b"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
    "# You can also set an optional 'run_name' that will be used as the trace name in Langfuse\n",
    "for s in graph_2.stream({\"messages\": [HumanMessage(content = \"How does photosynthesis work?\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqJnMtP5HDql",
    "outputId": "69c3d5d6-d44c-4784-a484-c66e4748b522"
   },
   "outputs": [],
   "source": [
    "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
    "for s in graph_2.stream({\"messages\": [HumanMessage(content = \"What time is it?\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4XjtNenH9GF"
   },
   "source": [
    "### See traces in Langfuse\n",
    "\n",
    "Example traces in Langfuse:\n",
    "\n",
    "1. [How does photosynthesis work?](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7d5f970573b8214d1ca891251e42282c)\n",
    "2. [What time is it?](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/3a69fe4998df50d42054f8944bd6a8d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-5EEBZAIbwc"
   },
   "source": [
    "![在 Langfuse 中查看多智能体追踪](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_multiagent_traces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCEzabn_jhbf"
   },
   "source": [
    "### 可视化该智能体\n",
    "\n",
    "你可以使用 `get_graph` 方法配合相应的 “draw” 方法进行图形可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "notlPjnl-HXV",
    "outputId": "17d6c6db-92af-4a6e-b1af-61b68e9cc87a"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph_2.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mESkG2IJS8OY"
   },
   "source": [
    "```mermaid\n",
    "graph TD;\n",
    "\t__start__([__start__]):::first\n",
    "\tResearcher(Researcher)\n",
    "\tCurrentTime(CurrentTime)\n",
    "\tsupervisor(supervisor)\n",
    "\t__end__([__end__]):::last\n",
    "\tCurrentTime --> supervisor;\n",
    "\tResearcher --> supervisor;\n",
    "\t__start__ --> supervisor;\n",
    "\tsupervisor -.-> Researcher;\n",
    "\tsupervisor -.-> CurrentTime;\n",
    "\tsupervisor -. &nbspFINISH&nbsp .-> __end__;\n",
    "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
    "\tclassDef first fill-opacity:0\n",
    "\tclassDef last fill:#bfb6fc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple LangGraph Agents\n",
    "\n",
    "There are setups where one LangGraph agent uses one or multiple other LangGraph agents. To combine all corresponding spans in one single trace for the multi agent execution, we can pass a custom `trace_id`. \n",
    "\n",
    "First, we generate a trace_id that can be used for both agents to group the agent executions together in one Langfuse trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client, Langfuse\n",
    "from langfuse.langchain import CallbackHandler\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "# Generate deterministic trace ID from external system\n",
    "predefined_trace_id = Langfuse.create_trace_id()\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the sub-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "sub_agent = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we set the tool that uses the research-sub-agent to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def langgraph_research(question):\n",
    "  \"\"\"Conducts research for various topics.\"\"\"\n",
    "\n",
    "  with langfuse.start_as_current_span(\n",
    "      name=\"🤖-sub-research-agent\",\n",
    "      trace_context={\"trace_id\": predefined_trace_id}\n",
    "  ) as span:\n",
    "      span.update_trace(input=question)\n",
    "\n",
    "      response = sub_agent.invoke({\"messages\": [HumanMessage(content = question)]},\n",
    "                        config={\"callbacks\": [langfuse_handler]})\n",
    "    \n",
    "      span.update_trace(output= response[\"messages\"][1].content)\n",
    "\n",
    "  return response[\"messages\"][1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a second simple LangGraph agent that uses the new `langgraph_research`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "main_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[langgraph_research]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is Langfuse?\"\n",
    "\n",
    "# Use the predefined trace ID with trace_context\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"🤖-main-agent\",\n",
    "    trace_context={\"trace_id\": predefined_trace_id}\n",
    ") as span:\n",
    "    span.update_trace(input=user_question)\n",
    " \n",
    "    # LangChain execution will be part of this trace\n",
    "    response = main_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_question}]},\n",
    "                            config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "    span.update_trace(output=response[\"messages\"][1].content)\n",
    " \n",
    "print(f\"Trace ID: {predefined_trace_id}\")  # Use this for scoring later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View traces in Langfuse\n",
    "\n",
    "![multi agent trace in Langfuse](https://langfuse.com/images/cookbook/integration-langgraph/a2a_langgraph.png)\n",
    "\n",
    "Example trace in Langfuse: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uybP4h8wGvWw"
   },
   "source": [
    "## Adding scores to traces as scores\n",
    "\n",
    "[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single observations or entire traces. They enable you to implement custom quality checks at runtime or facilitate human-in-the-loop evaluation processes.\n",
    "\n",
    "In the example below, we demonstrate how to score a specific span for `relevance` (a numeric score) and the overall trace for `feedback` (a categorical score). This helps in systematically assessing and improving your application.\n",
    "\n",
    "**→ Learn more about [Custom Scores in Langfuse](https://langfuse.com/docs/scores/custom).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgAqYnQuGwCL",
    "outputId": "11e14766-b25b-44b4-c3d4-d980f3d111cc"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "# Option 1: Use the yielded span object from the context manager\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"langgraph-request\") as span:\n",
    "    # ... LangGraph execution ...\n",
    " \n",
    "    # Score using the span object\n",
    "    span.score_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\",\n",
    "        comment=\"This was correct, thank you\"\n",
    "    )\n",
    " \n",
    "# Option 2: Use langfuse.score_current_trace() if still in context\n",
    "with langfuse.start_as_current_span(name=\"langgraph-request\") as span:\n",
    "    # ... LangGraph execution ...\n",
    " \n",
    "    # Score using current context\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\"\n",
    "    )\n",
    " \n",
    "# Option 3: Use create_score() with trace ID (when outside context)\n",
    "langfuse.create_score(\n",
    "    trace_id=predefined_trace_id,\n",
    "    name=\"user-feedback\",\n",
    "    value=1,\n",
    "    data_type=\"NUMERIC\",\n",
    "    comment=\"This was correct, thank you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq_DeCcXSxwq"
   },
   "source": [
    "### View trace with score in Langfuse\n",
    "\n",
    "Example trace: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e60a078b828d4fdc7ea22c73193b0fe4\n",
    "\n",
    "![Trace view including added score](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cIQVrYZJVMO"
   },
   "source": [
    "## Manage prompts with Langfuse\n",
    "\n",
    "Use [Langfuse prompt management](https://langfuse.com/docs/prompts/example-langchain) to effectively manage and version your prompts. We add the prompt used in this example via the SDK. In production, however, users would update and manage the prompts via the Langfuse UI instead of using the SDK.\n",
    "\n",
    "Langfuse prompt management is basically a Prompt CMS (Content Management System). Alternatively, you can also edit and version the prompt in the Langfuse UI.\n",
    "\n",
    "*   `Name` that identifies the prompt in Langfuse Prompt Management\n",
    "*   Prompt with prompt template incl. `{{input variables}}`\n",
    "*   `labels` to include `production` to immediately use prompt as the default\n",
    "\n",
    "In this example, we create a system prompt for an assistant that translates every user message into Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0J8-nbhUUz6",
    "outputId": "ee71e43d-9f77-451d-b71c-f77cd297b065"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    "\n",
    "langfuse.create_prompt(\n",
    "    name=\"translator_system-prompt\",\n",
    "    prompt=\"You are a translator that translates every input text into Spanish.\",\n",
    "    labels=[\"production\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dullp4XDXhzg"
   },
   "source": [
    "![View prompt in Langfuse UI](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_prompt_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNboOjf2YQpD"
   },
   "source": [
    "Use the utility method `.get_langchain_prompt()` to transform the Langfuse prompt into a string that can be used in Langchain.\n",
    "\n",
    "\n",
    "**Context:** Langfuse declares input variables in prompt templates using double brackets (`{{input variable}}`). Langchain uses single brackets for declaring input variables in PromptTemplates (`{input variable}`). The utility method `.get_langchain_prompt()` replaces the double brackets with single brackets. In this example, however, we don't use any variables in our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z49I82blYeXy",
    "outputId": "6cf7cd23-6dde-4e7b-ae50-e369db37c2d0"
   },
   "outputs": [],
   "source": [
    "# Get current production version of prompt and transform the Langfuse prompt into a string that can be used in Langchain\n",
    "langfuse_system_prompt = langfuse.get_prompt(\"translator_system-prompt\")\n",
    "langchain_system_prompt = langfuse_system_prompt.get_langchain_prompt()\n",
    "\n",
    "print(langchain_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3zBULfCt0Wq"
   },
   "source": [
    "Now we can use the new system prompt string to update our assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oGQhulyMmvZD"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "# Add the system prompt for our translator assistent\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": langchain_system_prompt\n",
    "}\n",
    "\n",
    "def chatbot(state: State):\n",
    "    messages_with_system_prompt = [system_prompt] + state[\"messages\"]\n",
    "    response = llm.invoke(messages_with_system_prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYd7wbttm2ec",
    "outputId": "fdc18797-3b3b-4cae-9ebb-8b9d946494c1"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
    "for s in graph.stream({\"messages\": [HumanMessage(content = \"What is Langfuse?\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add custom spans to a LangGraph trace\n",
    "\n",
    "Sometimes it is helpful to add custom spans to a LangGraph trace. This [GitHub discussion thread](https://github.com/orgs/langfuse/discussions/2988#discussioncomment-11634600) provides an example of how to do this."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
