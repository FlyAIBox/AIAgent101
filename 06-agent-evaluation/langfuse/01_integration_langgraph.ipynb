{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPJsVa1O4TuL"
   },
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Open Source Observability for LangGraph\" description: \"Learn how to use Langfuse for open source observability/tracing in your LangGraph application (Python).\" category: \"Integrations\" -->\n",
    "\n",
    "# å®è·µæ‰‹å†Œï¼šLangGraph é›†æˆ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlCI9KeX4Zn4"
   },
   "source": [
    "## ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) æ˜¯ç”± LangChain å›¢é˜Ÿå¼€æºçš„æ¡†æ¶ï¼Œç”¨äºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºå¤æ‚ã€æœ‰çŠ¶æ€çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚LangGraph å†…ç½®äº†æŒä¹…åŒ–èƒ½åŠ›ï¼Œå¯ä¿å­˜ä¸æ¢å¤çŠ¶æ€ï¼Œä»è€Œæ”¯æŒé”™è¯¯æ¢å¤ä¸åŒ…å«â€œäººç±»å‚ä¸â€ï¼ˆHuman-in-the-loop, HITLï¼‰çš„å·¥ä½œæµã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o8L1qPcaZeC"
   },
   "source": [
    "## æœ¬å®è·µæ‰‹å†Œçš„ç›®æ ‡\n",
    "\n",
    "æœ¬æ‰‹å†Œæ¼”ç¤ºå¦‚ä½•å€ŸåŠ© [Langfuse](https://langfuse.com/docs)ï¼Œé€šè¿‡å…¶ä¸ [LangChain çš„é›†æˆ](https://langfuse.com/integrations/frameworks/langchain)ï¼Œå¯¹ä½ çš„ LangGraph åº”ç”¨è¿›è¡Œè°ƒè¯•ã€åˆ†æä¸è¿­ä»£ä¼˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPTaMtxH4eHV"
   },
   "source": [
    "**å®Œæˆæœ¬æ‰‹å†Œåï¼Œä½ å°†èƒ½å¤Ÿï¼š**\n",
    "\n",
    "- è‡ªåŠ¨é€šè¿‡ Langfuse é›†æˆå¯¹ LangGraph åº”ç”¨è¿›è¡Œè¿½è¸ªï¼ˆtracingï¼‰\n",
    "- ç›‘æ§å¤æ‚çš„å¤šæ™ºèƒ½ä½“ï¼ˆmulti-agentï¼‰æ–¹æ¡ˆ\n",
    "- æ·»åŠ è¯„åˆ†ï¼ˆä¾‹å¦‚ç”¨æˆ·åé¦ˆï¼‰\n",
    "- ä½¿ç”¨ Langfuse ç®¡ç† LangGraph ä¸­ä½¿ç”¨çš„æç¤ºè¯ï¼ˆpromptï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sSIS88y9Ewm"
   },
   "source": [
    "## åˆå§‹åŒ– Langfuse\n",
    "\n",
    "åœ¨ Langfuse æ§åˆ¶å°é¡¹ç›®è®¾ç½®é¡µè·å–ä½ çš„ [API å¯†é’¥](https://langfuse.com/faq/all/where-are-langfuse-api-keys)ï¼Œå¹¶å°†å…¶åŠ å…¥åˆ°è¿è¡Œç¯å¢ƒå˜é‡ä¸­ä»¥åˆå§‹åŒ– Langfuse å®¢æˆ·ç«¯ã€‚\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"âš ï¸\" -->\n",
    "_**æ³¨æ„ï¼š** æœ¬ç¬”è®°ä½¿ç”¨ Langfuse Python SDK v3ã€‚è‹¥ä½ ä»åœ¨ä½¿ç”¨ v2ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„[æ—§ç‰ˆ LangGraph é›†æˆæŒ‡å—](https://github.com/langfuse/langfuse-docs/blob/662509b3296daddcddb292f14b10a62e7c39407d/cookbook/integration_langgraph.ipynb)ã€‚_\n",
    "<!-- CALLOUT_END -->\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"â„¹ï¸\" -->\n",
    "_**æ³¨æ„ï¼š** éœ€è¦è‡³å°‘ Python 3.11ï¼ˆå‚è§ [GitHub Issue](https://github.com/langfuse/langfuse/issues/1926)ï¼‰ã€‚_\n",
    "<!-- CALLOUT_END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C85BK1vJ5yD3",
    "outputId": "73f44b09-ae33-4bd0-8e92-9c1bfc8a1e7c"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse langchain langgraph langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1yglQ464VD-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ğŸ”‘ ä»é¡¹ç›®è®¾ç½®é¡µé¢è·å–æ‚¨çš„ API å¯†é’¥ï¼šhttps://cloud.langfuse.com\n",
    "# ğŸ“‹ é…ç½®æ­¥éª¤ï¼š\n",
    "# 1. ç™»å½• Langfuse æ§åˆ¶å°\n",
    "# 2. é€‰æ‹©æˆ–åˆ›å»ºé¡¹ç›®\n",
    "# 3. è¿›å…¥é¡¹ç›®è®¾ç½®é¡µé¢\n",
    "# 4. å¤åˆ¶å…¬é’¥å’Œç§é’¥\n",
    "\n",
    "# Langfuse å…¬é’¥ï¼ˆç”¨äºå®¢æˆ·ç«¯èº«ä»½éªŒè¯ï¼‰\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "\n",
    "# Langfuse ç§é’¥ï¼ˆç”¨äºæœåŠ¡ç«¯èº«ä»½éªŒè¯ï¼Œè¯·å¦¥å–„ä¿ç®¡ï¼‰\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "\n",
    "# Langfuse æœåŠ¡å™¨åœ°å€ï¼ˆé€‰æ‹©ç¦»æ‚¨æœ€è¿‘çš„åŒºåŸŸä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # ğŸ‡ªğŸ‡º æ¬§æ´²åŒºåŸŸ\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"  # ğŸ‡ºğŸ‡¸ ç¾å›½åŒºåŸŸ\n",
    "\n",
    "# ğŸ¤– OpenAI API å¯†é’¥\n",
    "# ä» https://platform.openai.com/api-keys è·å–\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "\n",
    "# ğŸš¨ å®‰å…¨æé†’ï¼š\n",
    "# - ä¸è¦å°†çœŸå®å¯†é’¥æäº¤åˆ°å…¬å…±ä»£ç ä»“åº“\n",
    "# - ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡æ–‡ä»¶ï¼ˆ.envï¼‰æˆ–å¯†é’¥ç®¡ç†æœåŠ¡\n",
    "# - å®šæœŸè½®æ¢ API å¯†é’¥ä»¥æé«˜å®‰å…¨æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆåï¼Œæˆ‘ä»¬å³å¯åˆå§‹åŒ– Langfuse å®¢æˆ·ç«¯ã€‚`get_client()` ä¼šä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­æä¾›çš„å‡­æ®æ¥åˆå§‹åŒ– Langfuse å®¢æˆ·ç«¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "# ğŸš€ åˆå§‹åŒ– Langfuse å®¢æˆ·ç«¯\n",
    "# get_client() ä¼šè‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®ä¿¡æ¯\n",
    "langfuse = get_client()\n",
    " \n",
    "# ğŸ” éªŒè¯å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€\n",
    "# è¿™ä¸ªæ­¥éª¤éå¸¸é‡è¦ï¼Œç¡®ä¿åç»­çš„è¿½è¸ªåŠŸèƒ½èƒ½å¤Ÿæ­£å¸¸å·¥ä½œ\n",
    "if langfuse.auth_check():\n",
    "    print(\"âœ… Langfuse å®¢æˆ·ç«¯å·²é€šè¿‡èº«ä»½éªŒè¯ï¼Œå‡†å¤‡å°±ç»ªï¼\")\n",
    "    print(\"ğŸ”§ ç°åœ¨å¯ä»¥å¼€å§‹ä½¿ç”¨è¿½è¸ªåŠŸèƒ½äº†\")\n",
    "else:\n",
    "    print(\"âŒ èº«ä»½éªŒè¯å¤±è´¥ï¼\")\n",
    "    print(\"ğŸ” è¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®é¡¹ï¼š\")\n",
    "    print(\"   - LANGFUSE_PUBLIC_KEY æ˜¯å¦æ­£ç¡®\")\n",
    "    print(\"   - LANGFUSE_SECRET_KEY æ˜¯å¦æ­£ç¡®\") \n",
    "    print(\"   - LANGFUSE_HOST æ˜¯å¦å¯è®¿é—®\")\n",
    "    print(\"   - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqYMmi6n9Nh1"
   },
   "source": [
    "## ç¤ºä¾‹ 1ï¼šä½¿ç”¨ LangGraph æ„å»ºç®€å•èŠå¤©åº”ç”¨\n",
    "\n",
    "**æœ¬èŠ‚å°†å®Œæˆï¼š**\n",
    "\n",
    "- åœ¨ LangGraph ä¸­æ„å»ºä¸€ä¸ªå¯å›ç­”å¸¸è§é—®é¢˜çš„å®¢æœèŠå¤©æœºå™¨äºº\n",
    "- ä½¿ç”¨ Langfuse å¯¹æœºå™¨äººçš„è¾“å…¥ä¸è¾“å‡ºè¿›è¡Œè¿½è¸ªï¼ˆtracingï¼‰\n",
    "\n",
    "æˆ‘ä»¬å…ˆä»ä¸€ä¸ªåŸºç¡€æœºå™¨äººå…¥æ‰‹ï¼Œéšååœ¨ä¸‹ä¸€èŠ‚æ‰©å±•ä¸ºæ›´é«˜çº§çš„å¤šæ™ºèƒ½ä½“ï¼ˆmulti-agentï¼‰è®¾ç½®ï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­ä»‹ç»å…³é”®çš„ LangGraph æ¦‚å¿µã€‚\n",
    "\n",
    "### åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆAgentï¼‰\n",
    "\n",
    "é¦–å…ˆåˆ›å»ºä¸€ä¸ª `StateGraph`ã€‚`StateGraph` å®šä¹‰äº†èŠå¤©æœºå™¨äººçš„çŠ¶æ€æœºç»“æ„ã€‚æˆ‘ä»¬ä¼šæ·»åŠ èŠ‚ç‚¹æ¥è¡¨ç¤º LLM ä»¥åŠæœºå™¨äººå¯è°ƒç”¨çš„å‡½æ•°ï¼Œå¹¶é€šè¿‡è¾¹ï¼ˆedgeï¼‰å®šä¹‰æœºå™¨äººåœ¨è¿™äº›å‡½æ•°ä¹‹é—´çš„çŠ¶æ€æµè½¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGIxgPww6VX6"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ å¯¼å…¥ LangGraph æ„å»ºæ™ºèƒ½ä½“æ‰€éœ€çš„æ ¸å¿ƒæ¨¡å—\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI  # OpenAI èŠå¤©æ¨¡å‹\n",
    "from langchain_core.messages import HumanMessage  # äººç±»æ¶ˆæ¯ç±»å‹\n",
    "from typing_extensions import TypedDict  # ç±»å‹åŒ–å­—å…¸\n",
    "from langgraph.graph import StateGraph  # LangGraph çŠ¶æ€å›¾\n",
    "from langgraph.graph.message import add_messages  # æ¶ˆæ¯æ·»åŠ å‡½æ•°\n",
    "\n",
    "# ğŸ“‹ å®šä¹‰æ™ºèƒ½ä½“çš„çŠ¶æ€ç»“æ„\n",
    "# State æ˜¯ä¸€ä¸ªç±»å‹åŒ–å­—å…¸ï¼Œå®šä¹‰äº†æ™ºèƒ½ä½“åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­éœ€è¦ç»´æŠ¤çš„çŠ¶æ€ä¿¡æ¯\n",
    "class State(TypedDict):\n",
    "    # ğŸ’¬ æ¶ˆæ¯åˆ—è¡¨ï¼šå­˜å‚¨å¯¹è¯å†å²\n",
    "    # Annotated[list, add_messages] çš„å«ä¹‰ï¼š\n",
    "    # - list: æ¶ˆæ¯çš„æ•°æ®ç±»å‹æ˜¯åˆ—è¡¨\n",
    "    # - add_messages: æŒ‡å®šçŠ¶æ€æ›´æ–°ç­–ç•¥ï¼Œæ–°æ¶ˆæ¯ä¼šè¿½åŠ åˆ°åˆ—è¡¨æœ«å°¾è€Œä¸æ˜¯è¦†ç›–æ•´ä¸ªåˆ—è¡¨\n",
    "    # è¿™ç§è®¾è®¡ç¡®ä¿äº†å¯¹è¯å†å²çš„å®Œæ•´ä¿å­˜\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# ğŸ—ï¸ åˆ›å»ºçŠ¶æ€å›¾æ„å»ºå™¨\n",
    "# StateGraph æ˜¯ LangGraph çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå®šä¹‰æ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# ğŸ¤– åˆå§‹åŒ–è¯­è¨€æ¨¡å‹\n",
    "# é€‰æ‹© GPT-4o æ¨¡å‹ï¼Œtemperature=0.2 ç¡®ä¿è¾“å‡ºç›¸å¯¹ç¨³å®šä½†ä»æœ‰ä¸€å®šåˆ›é€ æ€§\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# ğŸ”„ å®šä¹‰èŠå¤©æœºå™¨äººèŠ‚ç‚¹å‡½æ•°\n",
    "# è¿™æ˜¯ LangGraph èŠ‚ç‚¹å‡½æ•°çš„åŸºæœ¬æ¨¡å¼ï¼šæ¥æ”¶å½“å‰çŠ¶æ€ï¼Œè¿”å›æ›´æ–°åçš„çŠ¶æ€\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    èŠå¤©æœºå™¨äººèŠ‚ç‚¹çš„æ ¸å¿ƒé€»è¾‘\n",
    "    \n",
    "    å‚æ•°:\n",
    "        state (State): å½“å‰çš„æ™ºèƒ½ä½“çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å†å²\n",
    "    \n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æ–°ç”Ÿæˆæ¶ˆæ¯çš„çŠ¶æ€æ›´æ–°\n",
    "    \n",
    "    å·¥ä½œæµç¨‹:\n",
    "    1. è·å–å½“å‰çš„æ¶ˆæ¯å†å²\n",
    "    2. å°†æ¶ˆæ¯å†å²å‘é€ç»™è¯­è¨€æ¨¡å‹\n",
    "    3. æ¥æ”¶æ¨¡å‹ç”Ÿæˆçš„å›å¤\n",
    "    4. å°†å›å¤åŒ…è£…æˆçŠ¶æ€æ›´æ–°è¿”å›\n",
    "    \"\"\"\n",
    "    # è°ƒç”¨è¯­è¨€æ¨¡å‹å¤„ç†å½“å‰å¯¹è¯å†å²ï¼Œç”Ÿæˆå›å¤\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    # è¿”å›çŠ¶æ€æ›´æ–°ï¼šå°†æ¨¡å‹çš„å›å¤æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# ğŸ”— å‘å›¾ä¸­æ·»åŠ \"chatbot\"èŠ‚ç‚¹\n",
    "# èŠ‚ç‚¹ä»£è¡¨å·¥ä½œå•å…ƒï¼Œé€šå¸¸æ˜¯æ™®é€šçš„ Python å‡½æ•°\n",
    "# æ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£ç‰¹å®šçš„å¤„ç†é€»è¾‘ï¼Œå¦‚è°ƒç”¨ LLMã€å¤„ç†å·¥å…·ã€æ•°æ®è½¬æ¢ç­‰\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# ğŸš€ è®¾ç½®å›¾çš„å…¥å£ç‚¹\n",
    "# å‘Šè¯‰å›¾æ¯æ¬¡è¿è¡Œæ—¶ä»å“ªä¸ªèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ\n",
    "# åœ¨è¿™ä¸ªç®€å•ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥ä» chatbot èŠ‚ç‚¹å¼€å§‹\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# ğŸ è®¾ç½®å›¾çš„ç»“æŸç‚¹\n",
    "# æŒ‡ç¤ºå›¾\"å½“è¿™ä¸ªèŠ‚ç‚¹è¿è¡Œå®Œæˆåï¼Œå¯ä»¥é€€å‡ºæ‰§è¡Œ\"\n",
    "# å¯¹äºç®€å•çš„å•è½®å¯¹è¯ï¼Œchatbot èŠ‚ç‚¹æ‰§è¡Œå®Œå°±å¯ä»¥ç»“æŸ\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# âš™ï¸ ç¼–è¯‘å›¾å½¢ä¸ºå¯æ‰§è¡Œå¯¹è±¡\n",
    "# compile() æ–¹æ³•å°†å›¾æ„å»ºå™¨è½¬æ¢ä¸º CompiledGraph\n",
    "# CompiledGraph æ˜¯å¯ä»¥å®é™…è¿è¡Œçš„å›¾å½¢å¯¹è±¡ï¼Œæ”¯æŒ invokeã€stream ç­‰æ–¹æ³•\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# ğŸ’¡ ç†è§£ LangGraph çš„æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "# ğŸ—ï¸ StateGraph: å®šä¹‰æ™ºèƒ½ä½“çš„çŠ¶æ€å’Œå·¥ä½œæµç¨‹\n",
    "# ğŸ”„ Node: æ‰§è¡Œå…·ä½“ä»»åŠ¡çš„å‡½æ•°ï¼Œå¦‚è°ƒç”¨ LLMã€ä½¿ç”¨å·¥å…·ç­‰\n",
    "# ğŸ”— Edge: è¿æ¥èŠ‚ç‚¹ï¼Œå®šä¹‰æ‰§è¡Œé¡ºåºå’Œæ¡ä»¶è·³è½¬\n",
    "# ğŸ“Š State: æ™ºèƒ½ä½“è¿è¡Œè¿‡ç¨‹ä¸­ç»´æŠ¤çš„æ•°æ®ç»“æ„\n",
    "# âš™ï¸ CompiledGraph: ç¼–è¯‘åçš„å¯æ‰§è¡Œå›¾å½¢å¯¹è±¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW2SJcRgh7Xo"
   },
   "source": [
    "### åœ¨è°ƒç”¨æ—¶æ·»åŠ  Langfuse å›è°ƒ\n",
    "\n",
    "ç°åœ¨ï¼Œä¸ºäº†è¿½è¸ªåº”ç”¨æ‰§è¡Œè¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†æ·»åŠ  [é¢å‘ LangChain çš„ Langfuse å›è°ƒå¤„ç†å™¨](https://langfuse.com/integrations/frameworks/langchain)ï¼š`config={\"callbacks\": [langfuse_handler]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PxEc455-KYM",
    "outputId": "0d1a6a04-a024-47b8-d320-72cd25b7aefd"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ å¯¼å…¥ Langfuse çš„ LangChain å›è°ƒå¤„ç†å™¨\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# ğŸ”„ åˆå§‹åŒ– Langfuse å›è°ƒå¤„ç†å™¨\n",
    "# è¿™ä¸ªå¤„ç†å™¨å°†è‡ªåŠ¨æ•è· LangChain/LangGraph çš„æ‰§è¡Œè¿‡ç¨‹ï¼Œå®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š\n",
    "# - ğŸ•’ è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´å’Œå»¶è¿Ÿ\n",
    "# - ğŸ“ æ•è·è¾“å…¥è¾“å‡ºæ•°æ®å’Œä¸­é—´çŠ¶æ€\n",
    "# - ğŸ’° è·Ÿè¸ª token ä½¿ç”¨é‡å’Œ API è°ƒç”¨æˆæœ¬\n",
    "# - ğŸ› è®°å½•é”™è¯¯ä¿¡æ¯å’Œå¼‚å¸¸å †æ ˆï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "# - ğŸ“Š ç”Ÿæˆå®Œæ•´çš„æ‰§è¡Œé“¾è·¯è¿½è¸ªå›¾\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# ğŸš€ è¿è¡Œæ™ºèƒ½ä½“å¹¶å¯ç”¨ Langfuse è¿½è¸ª\n",
    "# ç¤ºä¾‹å¯¹è¯ï¼šè¯¢é—® Langfuse çš„ç›¸å…³ä¿¡æ¯\n",
    "print(\"ğŸ¤– æ™ºèƒ½ä½“å¼€å§‹è¿è¡Œï¼Œæ­£åœ¨å¤„ç†é—®é¢˜...\")\n",
    "print(\"â“ é—®é¢˜ï¼šä»€ä¹ˆæ˜¯ Langfuseï¼Ÿ\")\n",
    "print(\"\\nğŸ“‹ æ‰§è¡Œè¿‡ç¨‹ï¼š\")\n",
    "\n",
    "# ä½¿ç”¨ stream æ–¹æ³•å¯ä»¥å®æ—¶æŸ¥çœ‹æ™ºèƒ½ä½“çš„æ‰§è¡Œæ­¥éª¤\n",
    "# graph.stream() è¿”å›æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåçš„çŠ¶æ€å¿«ç…§\n",
    "for step_result in graph.stream(\n",
    "    # è¾“å…¥çŠ¶æ€ï¼šåŒ…å«ç”¨æˆ·çš„é—®é¢˜\n",
    "    {\"messages\": [HumanMessage(content=\"ä»€ä¹ˆæ˜¯ Langfuseï¼Ÿè¯·è¯¦ç»†ä»‹ç»å…¶ä¸»è¦åŠŸèƒ½å’Œåº”ç”¨åœºæ™¯ã€‚\")]},\n",
    "    # é…ç½®å›è°ƒå¤„ç†å™¨ä»¥å¯ç”¨ Langfuse è¿½è¸ª\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    "):\n",
    "    print(f\"ğŸ“¤ èŠ‚ç‚¹æ‰§è¡Œç»“æœ: {step_result}\")\n",
    "\n",
    "print(\"\\nâœ… æ™ºèƒ½ä½“æ‰§è¡Œå®Œæˆï¼\")\n",
    "print(\"ğŸ” æ‚¨å¯ä»¥åœ¨ Langfuse æ§åˆ¶å°ä¸­æŸ¥çœ‹å®Œæ•´çš„æ‰§è¡Œè¿½è¸ªè®°å½•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fdf3ZRnWGZ0N"
   },
   "source": [
    "### åœ¨ Langfuse ä¸­æŸ¥çœ‹è¿½è¸ªç»“æœ\n",
    "\n",
    "ç¤ºä¾‹è¿½è¸ªï¼š`https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17Aq7u6_LBR6"
   },
   "source": [
    "![åœ¨ Langfuse ä¸­æŸ¥çœ‹èŠå¤©åº”ç”¨çš„è¿½è¸ª](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_chatapp_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3yyVtGKhMPU"
   },
   "source": [
    "### å¯è§†åŒ–èŠå¤©åº”ç”¨\n",
    "\n",
    "ä½ å¯ä»¥ä½¿ç”¨ `get_graph` æ–¹æ³•é…åˆç›¸åº”çš„ â€œdrawâ€ æ–¹æ³•å¯¹å›¾è¿›è¡Œå¯è§†åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "MKkM6mw47kIy",
    "outputId": "9cf8a453-05e0-4193-fc77-81cb176d9ef4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY0HW5xISntw"
   },
   "source": [
    "```mermaid\n",
    "graph TD;\n",
    "\t__start__([__start__]):::first\n",
    "\tchatbot(chatbot)\n",
    "\t__end__([__end__]):::last\n",
    "\t__start__ --> chatbot;\n",
    "\tchatbot --> __end__;\n",
    "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
    "\tclassDef first fill-opacity:0\n",
    "\tclassDef last fill:#bfb6fc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åœ¨ LangGraph Server ä¸­ä½¿ç”¨ Langfuse\n",
    "\n",
    "#### ğŸ–¥ï¸ ä»€ä¹ˆæ˜¯ LangGraph Serverï¼Ÿ\n",
    "\n",
    "[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/) æ˜¯ LangGraph çš„æœåŠ¡å™¨éƒ¨ç½²è§£å†³æ–¹æ¡ˆï¼Œå®ƒæä¾›ï¼š\n",
    "\n",
    "- ğŸŒ **HTTP API æ¥å£**ï¼šå°† LangGraph æ™ºèƒ½ä½“åŒ…è£…ä¸º REST API æœåŠ¡\n",
    "- ğŸš€ **ç”Ÿäº§å°±ç»ª**ï¼šæ”¯æŒé«˜å¹¶å‘ã€è´Ÿè½½å‡è¡¡å’Œå®¹å™¨åŒ–éƒ¨ç½²\n",
    "- ğŸ”§ **ç®€åŒ–è¿ç»´**ï¼šè‡ªåŠ¨å¤„ç†è¯·æ±‚è·¯ç”±ã€çŠ¶æ€ç®¡ç†å’Œé”™è¯¯å¤„ç†\n",
    "- ğŸ“Š **ç›‘æ§é›†æˆ**ï¼šåŸç”Ÿæ”¯æŒå„ç§ç›‘æ§å’Œè¿½è¸ªå·¥å…·\n",
    "- ğŸ”’ **å®‰å…¨æ€§**ï¼šå†…ç½®èº«ä»½éªŒè¯å’Œæˆæƒæœºåˆ¶\n",
    "\n",
    "#### ğŸ’¡ ä¸ºä»€ä¹ˆåœ¨ Server ä¸­ä½¿ç”¨ Langfuseï¼Ÿ\n",
    "\n",
    "åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­ï¼ŒLangfuse è¿½è¸ªå˜å¾—æ›´åŠ é‡è¦ï¼š\n",
    "\n",
    "- ğŸ­ **ç”Ÿäº§ç›‘æ§**ï¼šå®æ—¶ç›‘æ§æœåŠ¡å™¨ä¸Šæ™ºèƒ½ä½“çš„è¿è¡ŒçŠ¶æ€\n",
    "- ğŸ› **è¿œç¨‹è°ƒè¯•**ï¼šæ— éœ€æœ¬åœ°å¤ç°å³å¯åˆ†æçº¿ä¸Šé—®é¢˜\n",
    "- ğŸ“ˆ **æ€§èƒ½åˆ†æ**ï¼šè¯†åˆ«æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼š\n",
    "- ğŸ’° **æˆæœ¬æ§åˆ¶**ï¼šç²¾ç¡®è·Ÿè¸ª API ä½¿ç”¨é‡å’Œè´¹ç”¨\n",
    "- ğŸ‘¥ **å›¢é˜Ÿåä½œ**ï¼šå¤šäººå…±äº«è¿½è¸ªæ•°æ®å’Œåˆ†æç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ”§ é…ç½®æ–¹æ³•è¯´æ˜\n",
    "\n",
    "ä½¿ç”¨ LangGraph Server æ—¶ï¼Œæ™ºèƒ½ä½“å›¾çš„è°ƒç”¨ç”±æœåŠ¡å™¨è‡ªåŠ¨å¤„ç†ï¼Œç”¨æˆ·æ— æ³•åœ¨æ¯æ¬¡è¯·æ±‚æ—¶æ‰‹åŠ¨æŒ‡å®šå›è°ƒå¤„ç†å™¨ã€‚\n",
    "\n",
    "**å…³é”®å·®å¼‚ï¼š**\n",
    "- ğŸ  **æœ¬åœ°å¼€å‘**ï¼šå¯ä»¥åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æ·»åŠ  `config={\"callbacks\": [langfuse_handler]}`\n",
    "- ğŸ–¥ï¸ **æœåŠ¡å™¨éƒ¨ç½²**ï¼šéœ€è¦åœ¨å›¾ç¼–è¯‘æ—¶é¢„å…ˆé…ç½®å›è°ƒå¤„ç†å™¨\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆï¼š**\n",
    "åœ¨å£°æ˜å’Œç¼–è¯‘å›¾æ—¶å°±æ·»åŠ  Langfuse å›è°ƒï¼Œè¿™æ ·æœåŠ¡å™¨ä¸Šçš„æ‰€æœ‰è¯·æ±‚éƒ½ä¼šè‡ªåŠ¨å¯ç”¨è¿½è¸ªåŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ å¯¼å…¥æœåŠ¡å™¨éƒ¨ç½²æ‰€éœ€çš„æ¨¡å—\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# ğŸ“‹ å®šä¹‰æ™ºèƒ½ä½“çŠ¶æ€ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# ğŸ—ï¸ æ„å»ºå›¾å½¢ç»“æ„\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# ğŸ¤– åˆå§‹åŒ–è¯­è¨€æ¨¡å‹\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# ğŸ”„ å®šä¹‰èŠå¤©æœºå™¨äººèŠ‚ç‚¹\n",
    "def chatbot(state: State):\n",
    "    \"\"\"å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå›å¤\"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# ğŸ”— ç»„è£…å›¾å½¢ç»“æ„\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# ğŸ”„ åˆå§‹åŒ– Langfuse å›è°ƒå¤„ç†å™¨ï¼ˆæœåŠ¡å™¨æ¨¡å¼ï¼‰\n",
    "# åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­ï¼Œè¿™ä¸ªå¤„ç†å™¨ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¯·æ±‚çš„è¿½è¸ª\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# âš™ï¸ ç¼–è¯‘å›¾å½¢å¹¶é¢„é…ç½®å›è°ƒå¤„ç†å™¨\n",
    "# ğŸ¯ å…³é”®æ–¹æ³•ï¼šwith_config()\n",
    "# - compile(): ç¼–è¯‘å›¾å½¢ä¸ºå¯æ‰§è¡Œå¯¹è±¡\n",
    "# - with_config(): ä¸ºç¼–è¯‘åçš„å›¾å½¢æ·»åŠ é»˜è®¤é…ç½®\n",
    "# \n",
    "# ğŸ’¡ å·¥ä½œåŸç†ï¼š\n",
    "# 1. é¦–å…ˆç¼–è¯‘å›¾å½¢ï¼Œå¾—åˆ° CompiledGraph å¯¹è±¡\n",
    "# 2. ç„¶åä½¿ç”¨ with_config() ä¸ºå›¾å½¢è®¾ç½®é»˜è®¤å›è°ƒ\n",
    "# 3. è¿”å›æ–°çš„ CompiledGraphï¼Œå†…ç½®äº† Langfuse è¿½è¸ªåŠŸèƒ½\n",
    "# \n",
    "# ğŸš€ ä¼˜åŠ¿ï¼š\n",
    "# - æ— éœ€åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æ‰‹åŠ¨æ·»åŠ å›è°ƒé…ç½®\n",
    "# - æœåŠ¡å™¨ä¸Šçš„æ‰€æœ‰è¯·æ±‚éƒ½ä¼šè‡ªåŠ¨å¯ç”¨è¿½è¸ª\n",
    "# - ç®€åŒ–äº†ç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²å’Œç»´æŠ¤\n",
    "graph = graph_builder.compile().with_config({\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "# ğŸ’¡ éƒ¨ç½²æç¤ºï¼š\n",
    "# åœ¨ LangGraph Server ä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªé¢„é…ç½®çš„ graph å¯¹è±¡\n",
    "# æ‰€æœ‰é€šè¿‡ API å‘é€çš„è¯·æ±‚éƒ½ä¼šè‡ªåŠ¨è®°å½•åˆ° Langfuse ä¸­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2W94eY19TR1"
   },
   "source": [
    "## ç¤ºä¾‹ 2ï¼šåŸºäº LangGraph çš„å¤šæ™ºèƒ½ä½“åº”ç”¨\n",
    "\n",
    "**æœ¬èŠ‚å°†å®Œæˆï¼š**\n",
    "\n",
    "- æ„å»º 2 ä¸ªæ‰§è¡Œæ™ºèƒ½ä½“ï¼šä¸€ä¸ªç ”ç©¶æ™ºèƒ½ä½“ä½¿ç”¨ LangChain çš„ WikipediaAPIWrapper æœç´¢ç»´åŸºç™¾ç§‘ï¼Œå¦ä¸€ä¸ªä½¿ç”¨è‡ªå®šä¹‰å·¥å…·è·å–å½“å‰æ—¶é—´\n",
    "- æ„å»ºä¸€ä¸ªæ™ºèƒ½ä½“ç›‘ç£è€…ï¼ˆsupervisorï¼‰ï¼Œç”¨äºå°†ç”¨æˆ·é—®é¢˜åˆ†é…ç»™ä¸Šè¿°æ™ºèƒ½ä½“\n",
    "- æ·»åŠ  Langfuse å›è°ƒä»¥è¿½è¸ªç›‘ç£è€…ä¸æ‰§è¡Œæ™ºèƒ½ä½“çš„æ­¥éª¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WfnrswDdjYTV",
    "outputId": "0d938cb1-9fd2-4ed3-cfdd-c84a9ad3ed82"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse langgraph langchain langchain_openai langchain_experimental pandas wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tciUQ62IEVec"
   },
   "source": [
    "### åˆ›å»ºå·¥å…·\n",
    "\n",
    "åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç”¨äºç»´åŸºç™¾ç§‘æ£€ç´¢çš„æ™ºèƒ½ä½“ï¼Œä»¥åŠä¸€ä¸ªç”¨äºå‘ŠçŸ¥å½“å‰æ—¶é—´çš„æ™ºèƒ½ä½“ã€‚å…ˆå®šä¹‰å®ƒä»¬å°†ä½¿ç”¨çš„å·¥å…·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cet0loyp9p-T"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ å¯¼å…¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ‰€éœ€çš„å·¥å…·å’Œæ¨¡å—\n",
    "from typing import Annotated\n",
    "from langchain_community.tools import WikipediaQueryRun  # ç»´åŸºç™¾ç§‘æŸ¥è¯¢å·¥å…·\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # ç»´åŸºç™¾ç§‘ API å°è£…\n",
    "from datetime import datetime  # æ—¶é—´å¤„ç†æ¨¡å—\n",
    "from langchain.tools import Tool  # é€šç”¨å·¥å…·å®šä¹‰ç±»\n",
    "\n",
    "# ğŸ” å®šä¹‰ç»´åŸºç™¾ç§‘æœç´¢å·¥å…·\n",
    "# åŠŸèƒ½ï¼šæ ¹æ®æŸ¥è¯¢è¯åœ¨ç»´åŸºç™¾ç§‘ä¸­æœç´¢ç›¸å…³ä¿¡æ¯\n",
    "# é€‚ç”¨åœºæ™¯ï¼šå›ç­”ç™¾ç§‘çŸ¥è¯†ã€å†å²äº‹ä»¶ã€äººç‰©ä¼ è®°ç­‰é—®é¢˜\n",
    "wikipedia_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=2,  # è¿”å›æœ€ç›¸å…³çš„2ä¸ªæœç´¢ç»“æœ\n",
    "        doc_content_chars_max=1000  # é™åˆ¶æ¯ä¸ªç»“æœçš„å­—ç¬¦æ•°ï¼Œé¿å…ä¿¡æ¯è¿‡è½½\n",
    "    )\n",
    ")\n",
    "\n",
    "# â° å®šä¹‰å½“å‰æ—¶é—´æŸ¥è¯¢å·¥å…·  \n",
    "# åŠŸèƒ½ï¼šè¿”å›å½“å‰çš„æ—¥æœŸå’Œæ—¶é—´ä¿¡æ¯\n",
    "# é€‚ç”¨åœºæ™¯ï¼šå›ç­”\"ç°åœ¨å‡ ç‚¹\"ã€\"ä»Šå¤©æ˜¯ä»€ä¹ˆæ—¥æœŸ\"ç­‰æ—¶é—´ç›¸å…³é—®é¢˜\n",
    "datetime_tool = Tool(\n",
    "    name=\"Datetime\",  # å·¥å…·åç§°ï¼Œæ™ºèƒ½ä½“ä¼šé€šè¿‡è¿™ä¸ªåç§°è°ƒç”¨å·¥å…·\n",
    "    func=lambda x: datetime.now().isoformat(),  # å·¥å…·å‡½æ•°ï¼šè¿”å› ISO æ ¼å¼çš„å½“å‰æ—¶é—´\n",
    "    description=\"è¿”å›å½“å‰çš„æ—¥æœŸå’Œæ—¶é—´ä¿¡æ¯ï¼ˆISO æ ¼å¼ï¼‰\",  # å·¥å…·æè¿°ï¼Œå¸®åŠ©æ™ºèƒ½ä½“ç†è§£ä½•æ—¶ä½¿ç”¨æ­¤å·¥å…·\n",
    ")\n",
    "\n",
    "# ğŸ’¡ å·¥å…·è®¾è®¡åŸåˆ™ï¼š\n",
    "# 1. ğŸ¯ å•ä¸€èŒè´£ï¼šæ¯ä¸ªå·¥å…·åªè´Ÿè´£ä¸€ä¸ªç‰¹å®šåŠŸèƒ½\n",
    "# 2. ğŸ“ æ¸…æ™°æè¿°ï¼šdescription è¦å‡†ç¡®æè¿°å·¥å…·çš„åŠŸèƒ½å’Œä½¿ç”¨åœºæ™¯\n",
    "# 3. ğŸ”’ é”™è¯¯å¤„ç†ï¼šç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥æ·»åŠ å¼‚å¸¸å¤„ç†é€»è¾‘\n",
    "# 4. âš¡ æ€§èƒ½è€ƒè™‘ï¼šé™åˆ¶è¿”å›æ•°æ®çš„å¤§å°ï¼Œé¿å…å½±å“æ•´ä½“æ€§èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uhDy_mEqr6"
   },
   "source": [
    "### ğŸ› ï¸ è¾…åŠ©å·¥å…·å‡½æ•°\n",
    "\n",
    "#### ğŸ“ åŠŸèƒ½è¯´æ˜\n",
    "ä¸‹é¢å®šä¹‰çš„è¾…åŠ©å‡½æ•°ç”¨äºç®€åŒ–æ·»åŠ æ–°çš„æ™ºèƒ½ä½“å·¥ä½œèŠ‚ç‚¹ã€‚è¿™äº›å‡½æ•°å°è£…äº†åˆ›å»ºæ™ºèƒ½ä½“å’ŒèŠ‚ç‚¹çš„é€šç”¨é€»è¾‘ï¼Œæé«˜ä»£ç çš„å¯é‡ç”¨æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚\n",
    "\n",
    "#### ğŸ¯ è®¾è®¡ç›®æ ‡\n",
    "- **å‡å°‘é‡å¤ä»£ç **ï¼šé¿å…ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“é‡å¤ç¼–å†™ç›¸åŒçš„åˆå§‹åŒ–é€»è¾‘\n",
    "- **æ ‡å‡†åŒ–æ¥å£**ï¼šç¡®ä¿æ‰€æœ‰æ™ºèƒ½ä½“èŠ‚ç‚¹å…·æœ‰ä¸€è‡´çš„è¾“å…¥è¾“å‡ºæ ¼å¼\n",
    "- **ç®€åŒ–æ‰©å±•**ï¼šæ–°å¢æ™ºèƒ½ä½“æ—¶åªéœ€å…³æ³¨ä¸šåŠ¡é€»è¾‘ï¼Œæ— éœ€å¤„ç†æ¡†æ¶ç»†èŠ‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75atiExdqd4P"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ å¯¼å…¥æ™ºèƒ½ä½“æ„å»ºæ‰€éœ€çš„æ ¸å¿ƒç»„ä»¶\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent  # æ™ºèƒ½ä½“æ‰§è¡Œå™¨å’Œåˆ›å»ºå‡½æ•°\n",
    "from langchain_core.messages import BaseMessage, HumanMessage  # æ¶ˆæ¯åŸºç±»å’Œäººç±»æ¶ˆæ¯\n",
    "from langchain_openai import ChatOpenAI  # OpenAI èŠå¤©æ¨¡å‹\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, system_prompt: str, tools: list):\n",
    "    \"\"\"\n",
    "    ğŸ­ æ™ºèƒ½ä½“å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå…·æœ‰ç‰¹å®šèƒ½åŠ›çš„å·¥ä½œæ™ºèƒ½ä½“\n",
    "    \n",
    "    å‚æ•°:\n",
    "        llm (ChatOpenAI): è¯­è¨€æ¨¡å‹å®ä¾‹\n",
    "        system_prompt (str): ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰æ™ºèƒ½ä½“çš„è§’è‰²å’Œè¡Œä¸ºè§„èŒƒ\n",
    "        tools (list): æ™ºèƒ½ä½“å¯ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨\n",
    "    \n",
    "    è¿”å›:\n",
    "        AgentExecutor: é…ç½®å®Œæˆçš„æ™ºèƒ½ä½“æ‰§è¡Œå™¨\n",
    "    \n",
    "    ğŸ”„ å·¥ä½œæµç¨‹:\n",
    "    1. æ„å»ºæç¤ºæ¨¡æ¿ï¼ˆåŒ…å«ç³»ç»Ÿè§’è‰²ã€å¯¹è¯å†å²ã€å·¥å…·ä½¿ç”¨è®°å½•ï¼‰\n",
    "    2. åˆ›å»ºæ”¯æŒå·¥å…·è°ƒç”¨çš„ OpenAI æ™ºèƒ½ä½“\n",
    "    3. åŒ…è£…ä¸ºæ‰§è¡Œå™¨ï¼Œå¤„ç†å·¥å…·è°ƒç”¨å’ŒçŠ¶æ€ç®¡ç†\n",
    "    \"\"\"\n",
    "    # ğŸ“‹ æ„å»ºæ™ºèƒ½ä½“çš„æç¤ºæ¨¡æ¿\n",
    "    # åŒ…å«ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼šç³»ç»Ÿè§’è‰²ã€å¯¹è¯å†å²ã€å·¥å…·ä½¿ç”¨è®°å½•\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        # ğŸ­ ç³»ç»Ÿæ¶ˆæ¯ï¼šå®šä¹‰æ™ºèƒ½ä½“çš„è§’è‰²ã€èƒ½åŠ›å’Œè¡Œä¸ºè§„èŒƒ\n",
    "        (\"system\", system_prompt),\n",
    "        # ğŸ’¬ æ¶ˆæ¯å†å²ï¼šä¿å­˜ä¸ç”¨æˆ·å’Œå…¶ä»–æ™ºèƒ½ä½“çš„å¯¹è¯è®°å½•\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        # ğŸ”§ å·¥å…·è®°å½•ï¼šè®°å½•æ™ºèƒ½ä½“ä½¿ç”¨å·¥å…·çš„è¿‡ç¨‹å’Œç»“æœ\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    \n",
    "    # ğŸ¤– åˆ›å»ºæ”¯æŒ OpenAI å·¥å…·è°ƒç”¨çš„æ™ºèƒ½ä½“\n",
    "    # è¿™ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿç†è§£ä½•æ—¶éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œä»¥åŠå¦‚ä½•è§£é‡Šå·¥å…·çš„è¿”å›ç»“æœ\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    \n",
    "    # ğŸ® åˆ›å»ºæ™ºèƒ½ä½“æ‰§è¡Œå™¨\n",
    "    # æ‰§è¡Œå™¨è´Ÿè´£ï¼šå·¥å…·è°ƒç”¨ã€é”™è¯¯å¤„ç†ã€çŠ¶æ€ç®¡ç†ã€ç»“æœæ•´åˆ\n",
    "    executor = AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        verbose=True,  # æ˜¾ç¤ºè¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹ï¼Œä¾¿äºè°ƒè¯•\n",
    "        handle_parsing_errors=True  # è‡ªåŠ¨å¤„ç†è§£æé”™è¯¯ï¼Œæé«˜ç¨³å®šæ€§\n",
    "    )\n",
    "    \n",
    "    return executor\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    \"\"\"\n",
    "    ğŸ”„ æ™ºèƒ½ä½“èŠ‚ç‚¹é€‚é…å™¨ï¼šå°†æ™ºèƒ½ä½“åŒ…è£…ä¸º LangGraph èŠ‚ç‚¹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        state: å½“å‰çš„å›¾çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å†å²å’Œå…¶ä»–ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "        agent: æ™ºèƒ½ä½“æ‰§è¡Œå™¨å®ä¾‹\n",
    "        name: æ™ºèƒ½ä½“çš„åç§°ï¼Œç”¨äºåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æ ‡è¯†æ¶ˆæ¯æ¥æº\n",
    "    \n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æ™ºèƒ½ä½“å“åº”çš„çŠ¶æ€æ›´æ–°\n",
    "    \n",
    "    ğŸ”„ é€‚é…è¿‡ç¨‹:\n",
    "    1. è°ƒç”¨æ™ºèƒ½ä½“å¤„ç†å½“å‰çŠ¶æ€\n",
    "    2. æå–æ™ºèƒ½ä½“çš„è¾“å‡ºç»“æœ\n",
    "    3. åŒ…è£…ä¸ºå¸¦æœ‰å‘é€è€…èº«ä»½çš„æ¶ˆæ¯\n",
    "    4. è¿”å›çŠ¶æ€æ›´æ–°\n",
    "    \"\"\"\n",
    "    # ğŸ“¤ è°ƒç”¨æ™ºèƒ½ä½“å¤„ç†å½“å‰çŠ¶æ€\n",
    "    # agent.invoke() ä¼šå¤„ç†å¯¹è¯å†å²ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆå›å¤\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # ğŸ·ï¸ å°†æ™ºèƒ½ä½“çš„è¾“å‡ºåŒ…è£…ä¸ºå¸¦æœ‰èº«ä»½æ ‡è¯†çš„æ¶ˆæ¯\n",
    "    # name å‚æ•°è®©ç³»ç»ŸçŸ¥é“è¿™æ¡æ¶ˆæ¯æ¥è‡ªå“ªä¸ªæ™ºèƒ½ä½“\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(\n",
    "            content=result[\"output\"],  # æ™ºèƒ½ä½“ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹\n",
    "            name=name  # æ¶ˆæ¯å‘é€è€…çš„èº«ä»½æ ‡è¯†\n",
    "        )]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74bZqwU6FCOa"
   },
   "source": [
    "### ğŸ¯ åˆ›å»ºæ™ºèƒ½ä½“ç›‘ç£è€…\n",
    "\n",
    "#### ğŸ“‹ ç›‘ç£è€…çš„æ ¸å¿ƒèŒè´£\n",
    "æ™ºèƒ½ä½“ç›‘ç£è€…æ˜¯å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„\"å¤§è„‘\"ï¼Œè´Ÿè´£ï¼š\n",
    "\n",
    "- ğŸ§  **ä»»åŠ¡ç†è§£**ï¼šåˆ†æç”¨æˆ·è¯·æ±‚ï¼Œç†è§£ä»»åŠ¡çš„æ€§è´¨å’Œéœ€æ±‚\n",
    "- ğŸ¯ **æ™ºèƒ½ä½“é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©æœ€é€‚åˆçš„å·¥ä½œæ™ºèƒ½ä½“\n",
    "- ğŸ”„ **æµç¨‹æ§åˆ¶**ï¼šå†³å®šä½•æ—¶åˆ‡æ¢æ™ºèƒ½ä½“ï¼Œä½•æ—¶ç»“æŸå¤„ç†æµç¨‹\n",
    "- ğŸ“Š **ç»“æœæ•´åˆ**ï¼šæ±‡æ€»å„ä¸ªæ™ºèƒ½ä½“çš„å·¥ä½œæˆæœ\n",
    "\n",
    "#### ğŸ”§ æŠ€æœ¯å®ç°æ–¹å¼\n",
    "ç›‘ç£è€…ä½¿ç”¨ **å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰** æŠ€æœ¯æ¥å®ç°å†³ç­–ï¼š\n",
    "\n",
    "- ğŸ“ **å‡½æ•°è°ƒç”¨**ï¼šé€šè¿‡ç»“æ„åŒ–çš„å‡½æ•°è°ƒç”¨æ¥è¡¨è¾¾å†³ç­–ç»“æœ\n",
    "- ğŸ›ï¸ **é€‰æ‹©æœºåˆ¶**ï¼šåœ¨å¯ç”¨çš„å·¥ä½œèŠ‚ç‚¹ä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªæ‰§è¡Œè€…\n",
    "- ğŸ **ç»ˆæ­¢æ¡ä»¶**ï¼šåˆ¤æ–­ä½•æ—¶ä»»åŠ¡å·²å®Œæˆï¼Œå¯ä»¥ç»“æŸå¤„ç†æµç¨‹\n",
    "\n",
    "#### ğŸ’¡ è®¾è®¡ä¼˜åŠ¿\n",
    "- **ç²¾ç¡®æ§åˆ¶**ï¼šé¿å…éšæœºæˆ–ä¸ç¡®å®šçš„è·¯ç”±å†³ç­–\n",
    "- **å¯è§£é‡Šæ€§**ï¼šæ¯ä¸ªå†³ç­–éƒ½æœ‰æ˜ç¡®çš„é€»è¾‘ä¾æ®\n",
    "- **å¯æ‰©å±•æ€§**ï¼šå®¹æ˜“æ·»åŠ æ–°çš„å·¥ä½œæ™ºèƒ½ä½“å’Œå†³ç­–è§„åˆ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu8MzgihrHdF"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Researcher\", \"CurrentTime\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create the prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Construction of the chain for the supervisor agent\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ognuMaIeFVh7"
   },
   "source": [
    "### Construct graph\n",
    "\n",
    "Now we are ready to start building the graph. Below, define the state and worker nodes using the function we just defined. Then we connect all the edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_LwtCmw_rHVz"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Sequence, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "# Add the research agent using the create_agent helper function\n",
    "research_agent = create_agent(llm, \"You are a web researcher.\", [wikipedia_tool])\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# Add the time agent using the create_agent helper function\n",
    "currenttime_agent = create_agent(llm, \"You can tell the current time at\", [datetime_tool])\n",
    "currenttime_node = functools.partial(agent_node, agent=currenttime_agent, name = \"CurrentTime\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add a \"chatbot\" node. Nodes represent units of work. They are typically regular python functions.\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"CurrentTime\", currenttime_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)\n",
    "\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "# Conditional edges usually contain \"if\" statements to route to different nodes depending on the current graph state.\n",
    "# These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Add an entry point. This tells our graph where to start its work each time we run it.\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "# To be able to run our graph, call \"compile()\" on the graph builder. This creates a \"CompiledGraph\" we can use invoke on our state.\n",
    "graph_2 = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3xfJLJyFwBG"
   },
   "source": [
    "### Add Langfuse as callback to the invocation\n",
    "\n",
    "Add [Langfuse handler](https://langfuse.com/integrations/frameworks/langchain) as callback: `config={\"callbacks\": [langfuse_handler]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsX1gw9kryGP",
    "outputId": "65d94f3c-17e7-4ad8-88b5-f837676d206b"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
    "# You can also set an optional 'run_name' that will be used as the trace name in Langfuse\n",
    "for s in graph_2.stream({\"messages\": [HumanMessage(content = \"How does photosynthesis work?\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqJnMtP5HDql",
    "outputId": "69c3d5d6-d44c-4784-a484-c66e4748b522"
   },
   "outputs": [],
   "source": [
    "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
    "for s in graph_2.stream({\"messages\": [HumanMessage(content = \"What time is it?\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4XjtNenH9GF"
   },
   "source": [
    "### See traces in Langfuse\n",
    "\n",
    "Example traces in Langfuse:\n",
    "\n",
    "1. [How does photosynthesis work?](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7d5f970573b8214d1ca891251e42282c)\n",
    "2. [What time is it?](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/3a69fe4998df50d42054f8944bd6a8d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-5EEBZAIbwc"
   },
   "source": [
    "![åœ¨ Langfuse ä¸­æŸ¥çœ‹å¤šæ™ºèƒ½ä½“è¿½è¸ª](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_multiagent_traces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCEzabn_jhbf"
   },
   "source": [
    "### å¯è§†åŒ–è¯¥æ™ºèƒ½ä½“\n",
    "\n",
    "ä½ å¯ä»¥ä½¿ç”¨ `get_graph` æ–¹æ³•é…åˆç›¸åº”çš„ â€œdrawâ€ æ–¹æ³•è¿›è¡Œå›¾å½¢å¯è§†åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "notlPjnl-HXV",
    "outputId": "17d6c6db-92af-4a6e-b1af-61b68e9cc87a"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph_2.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mESkG2IJS8OY"
   },
   "source": [
    "```mermaid\n",
    "graph TD;\n",
    "\t__start__([__start__]):::first\n",
    "\tResearcher(Researcher)\n",
    "\tCurrentTime(CurrentTime)\n",
    "\tsupervisor(supervisor)\n",
    "\t__end__([__end__]):::last\n",
    "\tCurrentTime --> supervisor;\n",
    "\tResearcher --> supervisor;\n",
    "\t__start__ --> supervisor;\n",
    "\tsupervisor -.-> Researcher;\n",
    "\tsupervisor -.-> CurrentTime;\n",
    "\tsupervisor -. &nbspFINISH&nbsp .-> __end__;\n",
    "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
    "\tclassDef first fill-opacity:0\n",
    "\tclassDef last fill:#bfb6fc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple LangGraph Agents\n",
    "\n",
    "There are setups where one LangGraph agent uses one or multiple other LangGraph agents. To combine all corresponding spans in one single trace for the multi agent execution, we can pass a custom `trace_id`. \n",
    "\n",
    "First, we generate a trace_id that can be used for both agents to group the agent executions together in one Langfuse trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client, Langfuse\n",
    "from langfuse.langchain import CallbackHandler\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "# Generate deterministic trace ID from external system\n",
    "predefined_trace_id = Langfuse.create_trace_id()\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the sub-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "sub_agent = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we set the tool that uses the research-sub-agent to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def langgraph_research(question):\n",
    "  \"\"\"Conducts research for various topics.\"\"\"\n",
    "\n",
    "  with langfuse.start_as_current_span(\n",
    "      name=\"ğŸ¤–-sub-research-agent\",\n",
    "      trace_context={\"trace_id\": predefined_trace_id}\n",
    "  ) as span:\n",
    "      span.update_trace(input=question)\n",
    "\n",
    "      response = sub_agent.invoke({\"messages\": [HumanMessage(content = question)]},\n",
    "                        config={\"callbacks\": [langfuse_handler]})\n",
    "    \n",
    "      span.update_trace(output= response[\"messages\"][1].content)\n",
    "\n",
    "  return response[\"messages\"][1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a second simple LangGraph agent that uses the new `langgraph_research`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "main_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[langgraph_research]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is Langfuse?\"\n",
    "\n",
    "# Use the predefined trace ID with trace_context\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"ğŸ¤–-main-agent\",\n",
    "    trace_context={\"trace_id\": predefined_trace_id}\n",
    ") as span:\n",
    "    span.update_trace(input=user_question)\n",
    " \n",
    "    # LangChain execution will be part of this trace\n",
    "    response = main_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_question}]},\n",
    "                            config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "    span.update_trace(output=response[\"messages\"][1].content)\n",
    " \n",
    "print(f\"Trace ID: {predefined_trace_id}\")  # Use this for scoring later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View traces in Langfuse\n",
    "\n",
    "![multi agent trace in Langfuse](https://langfuse.com/images/cookbook/integration-langgraph/a2a_langgraph.png)\n",
    "\n",
    "Example trace in Langfuse: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uybP4h8wGvWw"
   },
   "source": [
    "## Adding scores to traces as scores\n",
    "\n",
    "[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single observations or entire traces. They enable you to implement custom quality checks at runtime or facilitate human-in-the-loop evaluation processes.\n",
    "\n",
    "In the example below, we demonstrate how to score a specific span for `relevance` (a numeric score) and the overall trace for `feedback` (a categorical score). This helps in systematically assessing and improving your application.\n",
    "\n",
    "**â†’ Learn more about [Custom Scores in Langfuse](https://langfuse.com/docs/scores/custom).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgAqYnQuGwCL",
    "outputId": "11e14766-b25b-44b4-c3d4-d980f3d111cc"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "# Option 1: Use the yielded span object from the context manager\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"langgraph-request\") as span:\n",
    "    # ... LangGraph execution ...\n",
    " \n",
    "    # Score using the span object\n",
    "    span.score_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\",\n",
    "        comment=\"This was correct, thank you\"\n",
    "    )\n",
    " \n",
    "# Option 2: Use langfuse.score_current_trace() if still in context\n",
    "with langfuse.start_as_current_span(name=\"langgraph-request\") as span:\n",
    "    # ... LangGraph execution ...\n",
    " \n",
    "    # Score using current context\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\"\n",
    "    )\n",
    " \n",
    "# Option 3: Use create_score() with trace ID (when outside context)\n",
    "langfuse.create_score(\n",
    "    trace_id=predefined_trace_id,\n",
    "    name=\"user-feedback\",\n",
    "    value=1,\n",
    "    data_type=\"NUMERIC\",\n",
    "    comment=\"This was correct, thank you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq_DeCcXSxwq"
   },
   "source": [
    "### View trace with score in Langfuse\n",
    "\n",
    "Example trace: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e60a078b828d4fdc7ea22c73193b0fe4\n",
    "\n",
    "![Trace view including added score](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cIQVrYZJVMO"
   },
   "source": [
    "## Manage prompts with Langfuse\n",
    "\n",
    "Use [Langfuse prompt management](https://langfuse.com/docs/prompts/example-langchain) to effectively manage and version your prompts. We add the prompt used in this example via the SDK. In production, however, users would update and manage the prompts via the Langfuse UI instead of using the SDK.\n",
    "\n",
    "Langfuse prompt management is basically a Prompt CMS (Content Management System). Alternatively, you can also edit and version the prompt in the Langfuse UI.\n",
    "\n",
    "*   `Name` that identifies the prompt in Langfuse Prompt Management\n",
    "*   Prompt with prompt template incl. `{{input variables}}`\n",
    "*   `labels` to include `production` to immediately use prompt as the default\n",
    "\n",
    "In this example, we create a system prompt for an assistant that translates every user message into Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0J8-nbhUUz6",
    "outputId": "ee71e43d-9f77-451d-b71c-f77cd297b065"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    "\n",
    "langfuse.create_prompt(\n",
    "    name=\"translator_system-prompt\",\n",
    "    prompt=\"You are a translator that translates every input text into Spanish.\",\n",
    "    labels=[\"production\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dullp4XDXhzg"
   },
   "source": [
    "![View prompt in Langfuse UI](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_prompt_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNboOjf2YQpD"
   },
   "source": [
    "Use the utility method `.get_langchain_prompt()` to transform the Langfuse prompt into a string that can be used in Langchain.\n",
    "\n",
    "\n",
    "**Context:** Langfuse declares input variables in prompt templates using double brackets (`{{input variable}}`). Langchain uses single brackets for declaring input variables in PromptTemplates (`{input variable}`). The utility method `.get_langchain_prompt()` replaces the double brackets with single brackets. In this example, however, we don't use any variables in our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z49I82blYeXy",
    "outputId": "6cf7cd23-6dde-4e7b-ae50-e369db37c2d0"
   },
   "outputs": [],
   "source": [
    "# Get current production version of prompt and transform the Langfuse prompt into a string that can be used in Langchain\n",
    "langfuse_system_prompt = langfuse.get_prompt(\"translator_system-prompt\")\n",
    "langchain_system_prompt = langfuse_system_prompt.get_langchain_prompt()\n",
    "\n",
    "print(langchain_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3zBULfCt0Wq"
   },
   "source": [
    "Now we can use the new system prompt string to update our assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oGQhulyMmvZD"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
    "\n",
    "# Add the system prompt for our translator assistent\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": langchain_system_prompt\n",
    "}\n",
    "\n",
    "def chatbot(state: State):\n",
    "    messages_with_system_prompt = [system_prompt] + state[\"messages\"]\n",
    "    response = llm.invoke(messages_with_system_prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYd7wbttm2ec",
    "outputId": "fdc18797-3b3b-4cae-9ebb-8b9d946494c1"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
    "for s in graph.stream({\"messages\": [HumanMessage(content = \"What is Langfuse?\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add custom spans to a LangGraph trace\n",
    "\n",
    "Sometimes it is helpful to add custom spans to a LangGraph trace. This [GitHub discussion thread](https://github.com/orgs/langfuse/discussions/2988#discussioncomment-11634600) provides an example of how to do this."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
