{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒå®Œæ•´æ•™ç¨‹ - é¢å‘åˆå­¦è€…\n",
        "\n",
        "## ğŸ“– æ•™ç¨‹æ¦‚è¿°\n",
        "\n",
        "æœ¬æ•™ç¨‹ä¸“ä¸ºå¤§æ¨¡å‹å¾®è°ƒåˆå­¦è€…è®¾è®¡ï¼Œé€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ **SMS åƒåœ¾çŸ­ä¿¡åˆ†ç±»ä»»åŠ¡**ï¼Œæ·±å…¥æµ…å‡ºåœ°è®²è§£ **LoRAï¼ˆLow-Rank Adaptationï¼‰** å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯çš„**åŸç†ã€å®ç°å’Œåº”ç”¨**ã€‚\n",
        "\n",
        "### ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ\n",
        "\n",
        "**ç†è®ºå±‚é¢**ï¼š\n",
        "- âœ… **LoRA æ ¸å¿ƒåŸç†**ï¼šä¸ºä»€ä¹ˆä½ç§©åˆ†è§£èƒ½å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Ÿ\n",
        "- âœ… **æ•°å­¦å…¬å¼è¯¦è§£**ï¼šä»çº¿æ€§ä»£æ•°è§’åº¦ç†è§£ `Î”W â‰ˆ A Ã— B` çš„å«ä¹‰\n",
        "- âœ… **è¶…å‚æ•°è°ƒä¼˜**ï¼šrankã€alpha ç­‰å…³é”®å‚æ•°å¦‚ä½•å½±å“å¾®è°ƒæ•ˆæœï¼Ÿ\n",
        "- âœ… **ä¼˜åŠ¿å¯¹æ¯”**ï¼šç›¸æ¯”å…¨é‡å¾®è°ƒï¼ŒLoRA çš„ä¼˜åŠ¿åœ¨å“ªé‡Œï¼Ÿ\n",
        "\n",
        "**å®è·µå±‚é¢**ï¼š\n",
        "- âœ… **å®Œæ•´å·¥ä½œæµ**ï¼šä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²çš„ç«¯åˆ°ç«¯æµç¨‹\n",
        "- âœ… **ä»£ç å®ç°**ï¼šæ‰‹æŠŠæ‰‹å®ç° LoRA å±‚æ›¿æ¢å’Œè®­ç»ƒ\n",
        "- âœ… **æ€§èƒ½è¯„ä¼°**ï¼šå¦‚ä½•è¯„ä¼°å¾®è°ƒæ•ˆæœå’Œå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
        "- âœ… **å®é™…åº”ç”¨**ï¼šå°†é€šç”¨è¯­è¨€æ¨¡å‹é€‚é…ä¸ºåƒåœ¾çŸ­ä¿¡åˆ†ç±»å™¨\n",
        "\n",
        "### ğŸ” LoRA æŠ€æœ¯ä¼˜åŠ¿\n",
        "\n",
        "| å¯¹æ¯”ç»´åº¦ | å…¨é‡å¾®è°ƒ | LoRA å¾®è°ƒ |\n",
        "|---------|----------|----------|\n",
        "| **è®­ç»ƒå‚æ•°é‡** | å…¨éƒ¨æ¨¡å‹å‚æ•°ï¼ˆæ•°åäº¿ï¼‰ | ä»… LoRA å‚æ•°ï¼ˆæ•°ç™¾ä¸‡ï¼‰ |\n",
        "| **æ˜¾å­˜å ç”¨** | éå¸¸é«˜ | æ˜¾è‘—é™ä½ |\n",
        "| **è®­ç»ƒæ—¶é—´** | è¾ƒé•¿ | è¾ƒçŸ­ |\n",
        "| **éƒ¨ç½²æˆæœ¬** | éœ€è¦å®Œæ•´æ¨¡å‹ | åªéœ€å¢é‡å‚æ•° |\n",
        "| **å¤šä»»åŠ¡é€‚é…** | éœ€è¦å¤šä¸ªå®Œæ•´æ¨¡å‹ | å…±äº«åŸºåº§+å¤šä¸ª LoRA |\n",
        "\n",
        "### ğŸ“ é¢„æœŸå­¦ä¹ æ•ˆæœ\n",
        "\n",
        "**æ€§èƒ½æŒ‡æ ‡**ï¼š\n",
        "- ğŸ“Š **å‡†ç¡®ç‡æå‡**ï¼šä»éšæœºçŒœæµ‹ï¼ˆ~50%ï¼‰æå‡è‡³ 95%+\n",
        "- âš¡ **è®­ç»ƒæ•ˆç‡**ï¼š3-5 ä¸ª epoch å³å¯æ”¶æ•›\n",
        "- ğŸ’¾ **å‚æ•°æ•ˆç‡**ï¼šä»…è®­ç»ƒ <1% çš„æ¨¡å‹å‚æ•°\n",
        "- ğŸš€ **éƒ¨ç½²å‹å¥½**ï¼šLoRA æ–‡ä»¶ä»…å‡  MBï¼Œä¾¿äºåˆ†å‘\n",
        "\n",
        "**æŠ€èƒ½æŒæ¡**ï¼š\n",
        "- ğŸ”§ ç†è§£å¹¶å®ç° LoRA ç®—æ³•\n",
        "- ğŸ“ˆ æŒæ¡æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ–¹æ³•\n",
        "- ğŸ¯ èƒ½å¤Ÿå°†ç†è®ºåº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­\n",
        "- ğŸ’¡ å…·å¤‡è§£å†³ç±»ä¼¼å¾®è°ƒä»»åŠ¡çš„èƒ½åŠ›\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—‚ï¸ æ•™ç¨‹ç›®å½•ç»“æ„\n",
        "\n",
        "```\n",
        "ğŸ“š LoRA å¾®è°ƒæ•™ç¨‹\n",
        "â”œâ”€â”€ ğŸ”§ 1. ç¯å¢ƒå‡†å¤‡å’Œä¾èµ–å®‰è£…\n",
        "â”œâ”€â”€ ğŸ“– 2. LoRA ç†è®ºè¯¦è§£ï¼ˆæ ¸å¿ƒåŸç† + æ•°å­¦å…¬å¼ï¼‰\n",
        "â”œâ”€â”€ ğŸ“Š 3. æ•°æ®é›†å‡†å¤‡å’Œé¢„å¤„ç†\n",
        "â”œâ”€â”€ ğŸ¤– 4. GPT-2 æ¨¡å‹åŠ è½½å’Œé€‚é…\n",
        "â”œâ”€â”€ âš¡ 5. LoRA å±‚å®ç°å’Œæ›¿æ¢\n",
        "â”œâ”€â”€ ğŸš€ 6. æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–\n",
        "â”œâ”€â”€ ğŸ“ˆ 7. æ€§èƒ½è¯„ä¼°å’Œå¯è§†åŒ–\n",
        "â””â”€â”€ ğŸ¯ 8. å®é™…åº”ç”¨å’Œéƒ¨ç½²\n",
        "```\n",
        "\n",
        "ğŸ’¡ **å­¦ä¹ å»ºè®®**ï¼šå»ºè®®æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ªç« èŠ‚ï¼Œç†è®ºå’Œå®è·µç›¸ç»“åˆï¼Œé‡åˆ°é—®é¢˜åŠæ—¶æŸ¥é˜…æ³¨é‡Šå’Œæ–‡æ¡£ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”§ ç¬¬1ç« ï¼šç¯å¢ƒå‡†å¤‡å’Œä¾èµ–å®‰è£…\n",
        "\n",
        "## ğŸ’» è¿è¡Œç¯å¢ƒè¦æ±‚\n",
        "\n",
        "### ğŸ–¥ï¸ ç¡¬ä»¶è¦æ±‚\n",
        "- **CPU**ï¼šæ”¯æŒç°ä»£æŒ‡ä»¤é›†çš„å¤šæ ¸å¤„ç†å™¨\n",
        "- **å†…å­˜**ï¼šå»ºè®® 8GB+ RAMï¼ˆ16GB+ æ›´ä½³ï¼‰\n",
        "- **GPU**ï¼šæ¨èæ”¯æŒ CUDA çš„ NVIDIA GPUï¼ˆå¯é€‰ï¼Œä½†èƒ½æ˜¾è‘—åŠ é€Ÿè®­ç»ƒï¼‰\n",
        "- **å­˜å‚¨**ï¼šè‡³å°‘ 2GB å¯ç”¨ç©ºé—´\n",
        "\n",
        "### ğŸ è½¯ä»¶è¦æ±‚\n",
        "- **Python**ï¼š3.8+ ç‰ˆæœ¬\n",
        "- **æ“ä½œç³»ç»Ÿ**ï¼šWindows 10+/macOS 10.15+/Ubuntu 18.04+\n",
        "- **Jupyter**ï¼šæ”¯æŒ Notebook ç¯å¢ƒ\n",
        "\n",
        "### ğŸ“¦ æ ¸å¿ƒä¾èµ–åŒ…\n",
        "æˆ‘ä»¬å°†å®‰è£…ä»¥ä¸‹æ ¸å¿ƒä¾èµ–åŒ…ï¼Œæ¯ä¸ªéƒ½æœ‰å…¶ç‰¹å®šç”¨é€”ï¼š\n",
        "\n",
        "- ğŸ”¥ **PyTorch**ï¼šæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›å¼ é‡è®¡ç®—å’Œè‡ªåŠ¨å¾®åˆ†\n",
        "- ğŸ”¤ **tiktoken**ï¼šOpenAI çš„ BPE åˆ†è¯å™¨ï¼Œç”¨äº GPT-2 æ–‡æœ¬é¢„å¤„ç†\n",
        "- ğŸ“Š **pandas**ï¼šæ•°æ®å¤„ç†åº“ï¼Œç”¨äº CSV æ–‡ä»¶æ“ä½œå’Œæ•°æ®åˆ†æ\n",
        "- ğŸ“ˆ **matplotlib**ï¼šå¯è§†åŒ–åº“ï¼Œç”¨äºç»˜åˆ¶è®­ç»ƒæ›²çº¿å’Œç»“æœå›¾è¡¨\n",
        "- ğŸ”¢ **numpy**ï¼šæ•°å€¼è®¡ç®—åŸºç¡€åº“ï¼Œæä¾›å¤šç»´æ•°ç»„æ“ä½œ\n",
        "- ğŸ¨ **seaborn**ï¼šé«˜çº§å¯è§†åŒ–åº“ï¼Œæä¾›ç¾è§‚çš„ç»Ÿè®¡å›¾è¡¨\n",
        "- ğŸ“š **transformers**ï¼šHugging Face æ¨¡å‹åº“ï¼ˆè¾…åŠ©åŠŸèƒ½ï¼‰\n",
        "- â³ **tqdm**ï¼šè¿›åº¦æ¡åº“ï¼Œæ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
        "\n",
        "## ğŸš€ å¿«é€Ÿç¯å¢ƒæ£€æŸ¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ” ç®€åŒ–ç‰ˆç¯å¢ƒæ£€æŸ¥\n",
        "# åŠŸèƒ½ï¼šå¿«é€Ÿæ£€æŸ¥ç³»ç»ŸåŸºæœ¬ä¿¡æ¯ï¼Œç¡®ä¿æ»¡è¶³è¿è¡Œè¦æ±‚\n",
        "\n",
        "import platform\n",
        "import sys\n",
        "\n",
        "print(\"ğŸ” ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# æ£€æŸ¥æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
        "print(f\"ğŸ“± æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}\")\n",
        "print(f\"ğŸ Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
        "\n",
        "# æ£€æŸ¥ PyTorch å’Œ CUDA æ”¯æŒ\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"ğŸ”¥ PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"ğŸ® GPU æ”¯æŒ: âœ… (CUDA {torch.version.cuda})\")\n",
        "        print(f\"ğŸ® GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"   - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    else:\n",
        "        print(\"ğŸ® GPU æ”¯æŒ: âŒ (å°†ä½¿ç”¨CPUï¼Œè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢)\")\n",
        "except ImportError:\n",
        "    print(\"âŒ PyTorch æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–åŒ…\")\n",
        "\n",
        "print(\"\\nâœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ ä¸€é”®å®‰è£…æ‰€æœ‰ä¾èµ–\n",
        "\n",
        "ä»¥ä¸‹å‘½ä»¤å°†å®‰è£…æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åŒ…ã€‚æˆ‘ä»¬æŒ‡å®šäº†å…·ä½“ç‰ˆæœ¬å·ä»¥ç¡®ä¿å…¼å®¹æ€§å’Œç»“æœçš„å¯é‡ç°æ€§ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“¦ æ ¸å¿ƒä¾èµ–ä¸€é”®å®‰è£…\n",
        "# è¯´æ˜ï¼šè¿™é‡Œå®‰è£…çš„æ˜¯ç»è¿‡æµ‹è¯•çš„ç¨³å®šç‰ˆæœ¬ç»„åˆï¼Œç¡®ä¿å…¼å®¹æ€§\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹å®‰è£… LoRA å¾®è°ƒæ‰€éœ€çš„æ ¸å¿ƒä¾èµ–åŒ…\")\n",
        "print(\"â³ é¢„è®¡å®‰è£…æ—¶é—´ï¼š2-5åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦ï¼‰\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# å®‰è£… PyTorchï¼ˆæ”¯æŒ CUDAï¼‰\n",
        "%pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# å®‰è£…å…¶ä»–æ ¸å¿ƒä¾èµ–\n",
        "%pip install numpy==1.24.3 pandas==2.0.3 matplotlib==3.7.2 seaborn==0.12.2 tiktoken==0.5.1 transformers==4.33.2 tqdm==4.66.1 requests==2.31.0\n",
        "\n",
        "print(\"\\nâœ… æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼\")\n",
        "print(\"ğŸ’¡ å»ºè®®é‡å¯ Jupyter Kernel ä»¥ç¡®ä¿æ–°å®‰è£…çš„åŒ…æ­£å¸¸å·¥ä½œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“– ç¬¬2ç« ï¼šLoRA ç†è®ºè¯¦è§£ - ä»é›¶ç†è§£æ ¸å¿ƒåŸç†\n",
        "\n",
        "## ğŸ¯ ä»€ä¹ˆæ˜¯ LoRAï¼Ÿ\n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œç”±å¾®è½¯ç ”ç©¶é™¢åœ¨ 2021 å¹´æå‡ºã€‚å®ƒé€šè¿‡**ä½ç§©çŸ©é˜µåˆ†è§£**çš„æ–¹æ³•ï¼Œåœ¨ä¿æŒåŸå§‹æ¨¡å‹å‚æ•°ä¸å˜çš„æƒ…å†µä¸‹ï¼Œä»…è®­ç»ƒå°‘é‡çš„å¢é‡å‚æ•°æ¥é€‚é…æ–°ä»»åŠ¡ã€‚\n",
        "\n",
        "### ğŸ” æ ¸å¿ƒæ€æƒ³\n",
        "\n",
        "ä¼ ç»Ÿå¾®è°ƒéœ€è¦æ›´æ–°æ¨¡å‹çš„**æ‰€æœ‰å‚æ•°**ï¼Œè€Œ LoRA çš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼š\n",
        "1. **å†»ç»“åŸå§‹å‚æ•°**ï¼šä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ W ä¸å˜\n",
        "2. **ä½ç§©åˆ†è§£**ï¼šå°†æƒé‡æ›´æ–° Î”W åˆ†è§£ä¸ºä¸¤ä¸ªæ›´å°çš„çŸ©é˜µ A å’Œ B\n",
        "3. **å¹¶è¡Œè®¡ç®—**ï¼šåœ¨æ¨ç†æ—¶å°†åŸå§‹è¾“å‡ºä¸ LoRA åˆ†æ”¯çš„è¾“å‡ºç›¸åŠ \n",
        "\n",
        "## ğŸ§® æ•°å­¦åŸç†è¯¦è§£\n",
        "\n",
        "### ğŸ“ åŸºç¡€å…¬å¼\n",
        "\n",
        "åœ¨ä¼ ç»Ÿçš„å…¨é‡å¾®è°ƒä¸­ï¼Œçº¿æ€§å±‚çš„å‰å‘ä¼ æ’­ä¸ºï¼š\n",
        "```\n",
        "h = x Â· W\n",
        "```\n",
        "\n",
        "å…¶ä¸­ï¼š\n",
        "- `x` âˆˆ â„^(d) æ˜¯è¾“å…¥å‘é‡\n",
        "- `W` âˆˆ â„^(dÃ—k) æ˜¯æƒé‡çŸ©é˜µ\n",
        "- `h` âˆˆ â„^(k) æ˜¯è¾“å‡ºå‘é‡\n",
        "\n",
        "å¾®è°ƒåçš„æƒé‡å˜ä¸ºï¼š\n",
        "```\n",
        "W_new = W + Î”W\n",
        "```\n",
        "\n",
        "### ğŸ”‘ LoRA çš„æ ¸å¿ƒåˆ›æ–°\n",
        "\n",
        "LoRA å°†æƒé‡æ›´æ–° Î”W åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ï¼š\n",
        "\n",
        "```\n",
        "Î”W = A Â· B\n",
        "```\n",
        "\n",
        "å…¶ä¸­ï¼š\n",
        "- `A` âˆˆ â„^(dÃ—r)ï¼ˆä¸‹æŠ•å½±çŸ©é˜µï¼‰\n",
        "- `B` âˆˆ â„^(rÃ—k)ï¼ˆä¸ŠæŠ•å½±çŸ©é˜µï¼‰  \n",
        "- `r` << min(d, k)ï¼ˆç§©ï¼Œè¿œå°äºåŸå§‹ç»´åº¦ï¼‰\n",
        "\n",
        "å› æ­¤ï¼ŒLoRA çš„å‰å‘ä¼ æ’­å˜ä¸ºï¼š\n",
        "```\n",
        "h = x Â· W + Î± Â· (x Â· A Â· B)\n",
        "```\n",
        "\n",
        "å…¶ä¸­ `Î±` æ˜¯ç¼©æ”¾å› å­ã€‚\n",
        "\n",
        "### ğŸ’¡ é€šä¿—ç†è§£\n",
        "\n",
        "æƒ³è±¡ä½ æœ‰ä¸€ä¸ªå·¨å¤§çš„**é­”æ–¹ï¼ˆåŸå§‹æ¨¡å‹ï¼‰**ï¼š\n",
        "\n",
        "1. **ä¼ ç»Ÿå¾®è°ƒ**ï¼šéœ€è¦é‡æ–°è°ƒæ•´é­”æ–¹çš„æ¯ä¸€ä¸ªå°å—ï¼ˆæ‰€æœ‰å‚æ•°ï¼‰\n",
        "2. **LoRA å¾®è°ƒ**ï¼šåªéœ€è¦åœ¨é­”æ–¹è¡¨é¢è´´ä¸Šç‰¹æ®Šçš„**è´´çº¸ï¼ˆLoRA å±‚ï¼‰**ï¼Œé€šè¿‡è°ƒæ•´è´´çº¸æ¥æ”¹å˜æ•´ä½“æ•ˆæœ\n",
        "\n",
        "è¿™æ ·åšçš„å¥½å¤„ï¼š\n",
        "- ğŸ¯ **æ•ˆç‡é«˜**ï¼šåªéœ€è®­ç»ƒè´´çº¸ï¼Œä¸åŠ¨åŸé­”æ–¹\n",
        "- ğŸ’¾ **å­˜å‚¨çœ**ï¼šåªéœ€ä¿å­˜è´´çº¸çš„ä¿¡æ¯\n",
        "- ğŸ”„ **å¯åˆ‡æ¢**ï¼šå¯ä»¥å¿«é€Ÿæ›´æ¢ä¸åŒçš„è´´çº¸é€‚é…ä¸åŒä»»åŠ¡\n",
        "\n",
        "## ğŸ“Š å‚æ•°é‡å¯¹æ¯”\n",
        "\n",
        "å‡è®¾ä¸€ä¸ªçº¿æ€§å±‚çš„ç»´åº¦ä¸º 4096Ã—4096ï¼š\n",
        "\n",
        "| æ–¹æ³• | å‚æ•°é‡ | ç›¸å¯¹æ¯”ä¾‹ |\n",
        "|------|--------|----------|\n",
        "| **å…¨é‡å¾®è°ƒ** | 16,777,216 | 100% |\n",
        "| **LoRA (r=16)** | 131,072 | 0.78% |\n",
        "| **LoRA (r=32)** | 262,144 | 1.56% |\n",
        "| **LoRA (r=64)** | 524,288 | 3.13% |\n",
        "\n",
        "å¯ä»¥çœ‹åˆ°ï¼Œå³ä½¿æ˜¯ r=64 çš„ LoRAï¼Œå‚æ•°é‡ä¹Ÿä¸åˆ°åŸå§‹çš„ 4%ï¼\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“Š ç¬¬3ç« ï¼šæ•°æ®é›†å‡†å¤‡ä¸é¢„å¤„ç†\n",
        "\n",
        "## ğŸ¯ æ•°æ®é›†é€‰æ‹©\n",
        "\n",
        "æˆ‘ä»¬ä½¿ç”¨ç»å…¸çš„ **SMS Spam Collection** æ•°æ®é›†æ¥æ¼”ç¤º LoRA å¾®è°ƒï¼š\n",
        "\n",
        "### ğŸ“‹ æ•°æ®é›†ä¿¡æ¯\n",
        "- **æ•°æ®æ¥æº**ï¼šUCI Machine Learning Repository\n",
        "- **æ•°æ®è§„æ¨¡**ï¼šçº¦ 5,572 æ¡çŸ­ä¿¡\n",
        "- **ä»»åŠ¡ç±»å‹**ï¼šäºŒåˆ†ç±»ï¼ˆæ­£å¸¸çŸ­ä¿¡ vs åƒåœ¾çŸ­ä¿¡ï¼‰\n",
        "- **æ ‡ç­¾åˆ†å¸ƒ**ï¼š\n",
        "  - `ham`ï¼ˆæ­£å¸¸çŸ­ä¿¡ï¼‰ï¼šçº¦ 87%\n",
        "  - `spam`ï¼ˆåƒåœ¾çŸ­ä¿¡ï¼‰ï¼šçº¦ 13%\n",
        "\n",
        "### ğŸ”„ æ•°æ®æµç¨‹å›¾\n",
        "\n",
        "```\n",
        "åŸå§‹æ•°æ® â†’ ç±»åˆ«å¹³è¡¡ â†’ æ•°æ®åˆ’åˆ† â†’ åˆ†è¯ç¼–ç  â†’ æ‰¹æ¬¡åŠ è½½\n",
        "   â†“           â†“          â†“          â†“          â†“\n",
        " 5572æ¡    å‡è¡¡é‡‡æ ·    7:2:1æ¯”ä¾‹   GPT-2ç¼–ç    DataLoader\n",
        "```\n",
        "\n",
        "## ğŸ› ï¸ æ•°æ®é¢„å¤„ç†æ ¸å¿ƒå‡½æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ æ•°æ®å¤„ç†å·¥å…·å‡½æ•°åº“\n",
        "# åŠŸèƒ½ï¼šæä¾›å®Œæ•´çš„æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å·¥å…·\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tiktoken\n",
        "\n",
        "def create_balanced_dataset(df):\n",
        "    \"\"\"\n",
        "    åˆ›å»ºç±»åˆ«å¹³è¡¡çš„æ•°æ®é›†\n",
        "    \n",
        "    ä½œç”¨ï¼šè§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œé¿å…æ¨¡å‹åå‘å¤šæ•°ç±»\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        df: åŒ…å« Label å’Œ Text åˆ—çš„åŸå§‹æ•°æ®æ¡†\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        balanced_df: ç±»åˆ«å¹³è¡¡åçš„æ•°æ®æ¡†\n",
        "        \n",
        "    ç¤ºä¾‹ï¼š\n",
        "        åŸå§‹æ•°æ®ï¼šham(4827æ¡) + spam(747æ¡) = 5574æ¡\n",
        "        å¹³è¡¡åï¼šham(747æ¡) + spam(747æ¡) = 1494æ¡\n",
        "    \"\"\"\n",
        "    # ç»Ÿè®¡åƒåœ¾çŸ­ä¿¡æ•°é‡ï¼ˆå°‘æ•°ç±»ï¼‰\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "    print(f\"ğŸ“Š åŸå§‹æ•°æ®åˆ†å¸ƒï¼š\")\n",
        "    print(f\"   - æ­£å¸¸çŸ­ä¿¡(ham): {df[df['Label'] == 'ham'].shape[0]} æ¡\")\n",
        "    print(f\"   - åƒåœ¾çŸ­ä¿¡(spam): {num_spam} æ¡\")\n",
        "    \n",
        "    # éšæœºé‡‡æ ·ç­‰é‡çš„æ­£å¸¸çŸ­ä¿¡ï¼ˆå›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°ï¼‰\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "    \n",
        "    # åˆå¹¶å¹³è¡¡æ•°æ®\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "    print(f\"ğŸ¯ å¹³è¡¡åæ•°æ®ï¼šæ¯ç±» {num_spam} æ¡ï¼Œæ€»è®¡ {len(balanced_df)} æ¡\")\n",
        "    \n",
        "    return balanced_df\n",
        "\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "    \"\"\"\n",
        "    éšæœºåˆ’åˆ†æ•°æ®é›†\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        df: å¾…åˆ’åˆ†çš„æ•°æ®æ¡†\n",
        "        train_frac: è®­ç»ƒé›†æ¯”ä¾‹ (å¦‚ 0.7)\n",
        "        validation_frac: éªŒè¯é›†æ¯”ä¾‹ (å¦‚ 0.2)\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        train_df, validation_df, test_df: ä¸‰ä¸ªæ•°æ®æ¡†\n",
        "        \n",
        "    ç¤ºä¾‹ï¼š\n",
        "        æ€»æ•°æ®1494æ¡ â†’ è®­ç»ƒ1045æ¡(70%) + éªŒè¯149æ¡(10%) + æµ‹è¯•300æ¡(20%)\n",
        "    \"\"\"\n",
        "    # éšæœºæ‰“ä¹±æ•°æ®ï¼ˆå›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°ï¼‰\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "    \n",
        "    # è®¡ç®—åˆ’åˆ†ç´¢å¼•\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "    \n",
        "    # æ‰§è¡Œåˆ’åˆ†\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "    \n",
        "    print(f\"ğŸ“Š æ•°æ®åˆ’åˆ†ç»“æœï¼š\")\n",
        "    print(f\"   - è®­ç»ƒé›†: {len(train_df)} æ¡ ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"   - éªŒè¯é›†: {len(validation_df)} æ¡ ({len(validation_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"   - æµ‹è¯•é›†: {len(test_df)} æ¡ ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    \"\"\"\n",
        "    SMS åƒåœ¾çŸ­ä¿¡æ•°æ®é›†ç±»\n",
        "    \n",
        "    åŠŸèƒ½ï¼š\n",
        "    1. ç»§æ‰¿ PyTorch Datasetï¼Œæ”¯æŒ DataLoader æ‰¹é‡åŠ è½½\n",
        "    2. è‡ªåŠ¨å¤„ç†æ–‡æœ¬åˆ†è¯ã€åºåˆ—å¡«å……å’Œæ ‡ç­¾è½¬æ¢\n",
        "    3. æ”¯æŒåŠ¨æ€åºåˆ—é•¿åº¦æˆ–å›ºå®šé•¿åº¦æˆªæ–­\n",
        "    \n",
        "    ä½¿ç”¨æµç¨‹ï¼š\n",
        "    text â†’ tokenize â†’ padding â†’ tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–æ•°æ®é›†\n",
        "        \n",
        "        å‚æ•°ï¼š\n",
        "            csv_file: CSV æ–‡ä»¶è·¯å¾„\n",
        "            tokenizer: åˆ†è¯å™¨å¯¹è±¡ï¼ˆtiktokenï¼‰\n",
        "            max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼ŒNone æ—¶ä½¿ç”¨è®­ç»ƒé›†æœ€é•¿é•¿åº¦\n",
        "            pad_token_id: å¡«å…… token IDï¼ˆGPT-2 é»˜è®¤ä¸º 50256ï¼‰\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        print(f\"ğŸ“‚ åŠ è½½æ•°æ®ï¼š{csv_file}ï¼Œå…± {len(self.data)} æ¡\")\n",
        "        \n",
        "        # é¢„åˆ†è¯ï¼šå°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸º token ID åºåˆ—\n",
        "        print(\"ğŸ”¤ æ‰§è¡Œåˆ†è¯...\")\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in tqdm(self.data[\"Text\"], desc=\"åˆ†è¯è¿›åº¦\")\n",
        "        ]\n",
        "        \n",
        "        # ç¡®å®šåºåˆ—é•¿åº¦\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "            print(f\"ğŸ“ è‡ªåŠ¨ç¡®å®šæœ€å¤§é•¿åº¦: {self.max_length}\")\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            print(f\"ğŸ“ ä½¿ç”¨æŒ‡å®šé•¿åº¦: {self.max_length}\")\n",
        "            # æˆªæ–­è¿‡é•¿çš„åºåˆ—\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "        \n",
        "        # å¡«å……åºåˆ—åˆ°ç»Ÿä¸€é•¿åº¦\n",
        "        print(\"ğŸ“ æ‰§è¡Œå¡«å……...\")\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "        print(\"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"è·å–å•ä¸ªæ ·æœ¬\"\"\"\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),  # è¾“å…¥åºåˆ—\n",
        "            torch.tensor(label, dtype=torch.long)     # æ ‡ç­¾\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"è¿”å›æ•°æ®é›†å¤§å°\"\"\"\n",
        "        return len(self.data)\n",
        "    \n",
        "    def _longest_encoded_length(self):\n",
        "        \"\"\"è®¡ç®—æœ€é•¿ç¼–ç åºåˆ—é•¿åº¦\"\"\"\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            if len(encoded_text) > max_length:\n",
        "                max_length = len(encoded_text)\n",
        "        return max_length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# âš¡ ç¬¬4ç« ï¼šLoRA å±‚å®ç° - æ ¸å¿ƒç®—æ³•\n",
        "\n",
        "## ğŸ§  LoRA å±‚çš„ Python å®ç°\n",
        "\n",
        "LoRA çš„æ ¸å¿ƒåœ¨äºå°†åŸå§‹çš„çº¿æ€§å±‚æ›¿æ¢ä¸ºåŒ…å«ä½ç§©åˆ†è§£çš„å¢å¼ºå±‚ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„å®ç°ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ LoRA å±‚å®Œæ•´å®ç°\n",
        "# åŠŸèƒ½ï¼šå®ç°å‚æ•°é«˜æ•ˆçš„ LoRA å¾®è°ƒå±‚\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    LoRA (Low-Rank Adaptation) å±‚å®ç°\n",
        "    \n",
        "    æ ¸å¿ƒå…¬å¼ï¼šh = xÂ·W + Î±Â·(xÂ·AÂ·B)\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        - W: å†»ç»“çš„åŸå§‹æƒé‡ [dÃ—k]\n",
        "        - A: ä¸‹æŠ•å½±çŸ©é˜µ [dÃ—r] (å¯è®­ç»ƒ)\n",
        "        - B: ä¸ŠæŠ•å½±çŸ©é˜µ [rÃ—k] (å¯è®­ç»ƒ)  \n",
        "        - Î±: ç¼©æ”¾å› å­\n",
        "        - r: ä½ç§©ç»´åº¦ (rank)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ– LoRA å±‚\n",
        "        \n",
        "        å‚æ•°:\n",
        "            in_dim: è¾“å…¥ç»´åº¦ (d)\n",
        "            out_dim: è¾“å‡ºç»´åº¦ (k)  \n",
        "            rank: LoRA ç§© (r)\n",
        "            alpha: ç¼©æ”¾å› å­\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # è®¾ç½® LoRA å‚æ•°\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # LoRA åˆ†æ”¯çŸ©é˜µ (å¯è®­ç»ƒå‚æ•°)\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)  # å°éšæœºåˆå§‹åŒ–\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))        # é›¶åˆå§‹åŒ–ç¡®ä¿åˆå§‹æ—¶ Î”W=0\n",
        "        \n",
        "        print(f\"âœ¨ åˆ›å»º LoRA å±‚: {in_dim}Ã—{out_dim} â†’ rank={rank}, Î±={alpha}\")\n",
        "        print(f\"   - AçŸ©é˜µ: {self.A.shape} ({self.A.numel():,} å‚æ•°)\")\n",
        "        print(f\"   - BçŸ©é˜µ: {self.B.shape} ({self.B.numel():,} å‚æ•°)\")\n",
        "        print(f\"   - æ€»å‚æ•°: {self.A.numel() + self.B.numel():,}\")\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        LoRA å‰å‘ä¼ æ’­\n",
        "        \n",
        "        è®¡ç®—: Î”WÂ·x = Î±Â·(AÂ·B)Â·x = Î±Â·xÂ·AÂ·B\n",
        "        \"\"\"\n",
        "        # xÂ·A: [batch, seq, in_dim] Ã— [in_dim, rank] â†’ [batch, seq, rank]\n",
        "        # (xÂ·A)Â·B: [batch, seq, rank] Ã— [rank, out_dim] â†’ [batch, seq, out_dim]\n",
        "        lora_output = torch.matmul(torch.matmul(x, self.A), self.B)\n",
        "        return self.alpha * lora_output\n",
        "\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    \"\"\"\n",
        "    é›†æˆ LoRA çš„çº¿æ€§å±‚\n",
        "    \n",
        "    åŠŸèƒ½ï¼šå°†åŸå§‹çº¿æ€§å±‚ä¸ LoRA åˆ†æ”¯å¹¶è¡Œè®¡ç®—å¹¶ç›¸åŠ \n",
        "    å‰å‘ä¼ æ’­ï¼šoutput = xÂ·W + LoRA(x)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, linear_layer, rank, alpha):\n",
        "        \"\"\"\n",
        "        å‚æ•°:\n",
        "            linear_layer: åŸå§‹çš„ nn.Linear å±‚\n",
        "            rank: LoRA ç§©\n",
        "            alpha: LoRA ç¼©æ”¾å› å­\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # ä¿å­˜åŸå§‹çº¿æ€§å±‚ (å†»ç»“å‚æ•°)\n",
        "        self.linear = linear_layer\n",
        "        for param in self.linear.parameters():\n",
        "            param.requires_grad = False  # å†»ç»“åŸå§‹å‚æ•°\n",
        "        \n",
        "        # æ·»åŠ  LoRA åˆ†æ”¯\n",
        "        self.lora = LoRALayer(\n",
        "            in_dim=linear_layer.in_features,\n",
        "            out_dim=linear_layer.out_features, \n",
        "            rank=rank,\n",
        "            alpha=alpha\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        å‰å‘ä¼ æ’­ï¼šåŸå§‹è¾“å‡º + LoRA å¢é‡\n",
        "        \"\"\"\n",
        "        # åŸå§‹çº¿æ€§å±‚è¾“å‡ºï¼ˆå†»ç»“æƒé‡ï¼‰\n",
        "        original_output = self.linear(x)\n",
        "        \n",
        "        # LoRA åˆ†æ”¯è¾“å‡ºï¼ˆå¯è®­ç»ƒå‚æ•°ï¼‰\n",
        "        lora_output = self.lora(x)\n",
        "        \n",
        "        # è¿”å›å åŠ ç»“æœ\n",
        "        return original_output + lora_output\n",
        "\n",
        "def replace_linear_with_lora(model, rank=16, alpha=32):\n",
        "    \"\"\"\n",
        "    é€’å½’æ›¿æ¢æ¨¡å‹ä¸­æ‰€æœ‰ Linear å±‚ä¸º LoRA å¢å¼ºç‰ˆæœ¬\n",
        "    \n",
        "    å‚æ•°:\n",
        "        model: å¾…æ›¿æ¢çš„æ¨¡å‹\n",
        "        rank: LoRA ç§© (é»˜è®¤16)\n",
        "        alpha: LoRA ç¼©æ”¾å› å­ (é»˜è®¤32ï¼Œé€šå¸¸è®¾ä¸º2Ã—rank)\n",
        "    \n",
        "    è¿”å›:\n",
        "        æ›¿æ¢åçš„æ¨¡å‹ï¼ŒåŸå§‹å‚æ•°è¢«å†»ç»“ï¼Œä»… LoRA å‚æ•°å¯è®­ç»ƒ\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ”„ å¼€å§‹ LoRA å±‚æ›¿æ¢ (rank={rank}, alpha={alpha})\")\n",
        "    \n",
        "    total_replaced = 0\n",
        "    original_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    # é€’å½’éå†æ‰€æœ‰å­æ¨¡å—\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # æ›¿æ¢ Linear å±‚\n",
        "            print(f\"  ğŸ“Œ æ›¿æ¢çº¿æ€§å±‚: {name} ({module.in_features}Ã—{module.out_features})\")\n",
        "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
        "            total_replaced += 1\n",
        "        else:\n",
        "            # é€’å½’å¤„ç†å­æ¨¡å—\n",
        "            replace_linear_with_lora(module, rank, alpha)\n",
        "    \n",
        "    # ç»Ÿè®¡å‚æ•°ä¿¡æ¯\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
        "    \n",
        "    print(f\"\\nğŸ“Š LoRA æ›¿æ¢å®Œæˆç»Ÿè®¡:\")\n",
        "    print(f\"   - æ›¿æ¢çº¿æ€§å±‚æ•°é‡: {total_replaced}\")\n",
        "    print(f\"   - åŸå§‹å‚æ•°æ€»é‡: {original_params:,}\")\n",
        "    print(f\"   - å†»ç»“å‚æ•°æ•°é‡: {frozen_params:,}\")\n",
        "    print(f\"   - å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}\")\n",
        "    print(f\"   - å‚æ•°æ•ˆç‡: {trainable_params/original_params*100:.2f}%\")\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¤– ç¬¬5ç« ï¼šç®€åŒ– GPT-2 æ¨¡å‹å®ç°\n",
        "\n",
        "## ğŸ“š è½»é‡çº§ GPT-2 ç”¨äºåˆ†ç±»ä»»åŠ¡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ—ï¸ ç®€åŒ–çš„ GPT-2 åˆ†ç±»æ¨¡å‹\n",
        "# åŠŸèƒ½ï¼šä¸“é—¨ä¸ºäºŒåˆ†ç±»ä»»åŠ¡è®¾è®¡çš„è½»é‡çº§ Transformer æ¨¡å‹\n",
        "\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        \n",
        "        # çº¿æ€§æŠ•å½±\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        # å› æœæ©ç  (ä¸‹ä¸‰è§’)\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n",
        "        scores.masked_fill_(mask == 0, float('-inf'))\n",
        "        \n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        \n",
        "        # é‡æ–°ç»„ç»‡å¹¶è¾“å‡ºæŠ•å½±\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "        return self.W_o(attn_output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"å‰é¦ˆç¥ç»ç½‘ç»œ\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer å—\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        x = x + self.feed_forward(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class SimpleGPTForClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    ç®€åŒ–çš„ GPT åˆ†ç±»æ¨¡å‹\n",
        "    \n",
        "    ç‰¹ç‚¹ï¼š\n",
        "    1. åŸºäº Transformer æ¶æ„\n",
        "    2. ä¸“é—¨ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡\n",
        "    3. ä½¿ç”¨æœ€åä¸€ä¸ª token çš„è¡¨ç¤ºè¿›è¡Œåˆ†ç±»\n",
        "    4. è½»é‡çº§è®¾è®¡ï¼Œä¾¿äº LoRA å¾®è°ƒæ¼”ç¤º\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=50257, max_seq_len=512, d_model=256, \n",
        "                 num_heads=8, num_layers=6, d_ff=1024, num_classes=2):\n",
        "        \"\"\"\n",
        "        å‚æ•°ï¼š\n",
        "            vocab_size: è¯æ±‡è¡¨å¤§å° (GPT-2 é»˜è®¤ 50257)\n",
        "            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦\n",
        "            d_model: æ¨¡å‹ç»´åº¦\n",
        "            num_heads: æ³¨æ„åŠ›å¤´æ•°\n",
        "            num_layers: Transformer å±‚æ•°\n",
        "            d_ff: å‰é¦ˆç½‘ç»œéšè—ç»´åº¦\n",
        "            num_classes: åˆ†ç±»ç±»åˆ«æ•° (äºŒåˆ†ç±»=2)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Token å’Œä½ç½®åµŒå…¥\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        \n",
        "        # Transformer å±‚å †æ ˆ\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff) \n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # æœ€ç»ˆå±‚å½’ä¸€åŒ–\n",
        "        self.ln_final = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # åˆ†ç±»å¤´ (ç”¨æœ€åä¸€ä¸ª token è¿›è¡Œåˆ†ç±»)\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "        \n",
        "        # ä¿å­˜é…ç½®\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        print(f\"ğŸ¤– åˆ›å»ºç®€åŒ– GPT åˆ†ç±»æ¨¡å‹:\")\n",
        "        print(f\"   - è¯æ±‡è¡¨å¤§å°: {vocab_size:,}\")\n",
        "        print(f\"   - æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_len}\")\n",
        "        print(f\"   - æ¨¡å‹ç»´åº¦: {d_model}\")\n",
        "        print(f\"   - æ³¨æ„åŠ›å¤´æ•°: {num_heads}\")\n",
        "        print(f\"   - Transformer å±‚æ•°: {num_layers}\")\n",
        "        print(f\"   - åˆ†ç±»ç±»åˆ«æ•°: {num_classes}\")\n",
        "        \n",
        "        # ç»Ÿè®¡å‚æ•°é‡\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"   - æ€»å‚æ•°é‡: {total_params:,}\")\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        å‰å‘ä¼ æ’­\n",
        "        \n",
        "        å‚æ•°ï¼š\n",
        "            input_ids: [batch_size, seq_len] è¾“å…¥ token ID\n",
        "            \n",
        "        è¿”å›ï¼š\n",
        "            logits: [batch_size, num_classes] åˆ†ç±» logits\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        # ä½ç½®ç´¢å¼•\n",
        "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        \n",
        "        # åµŒå…¥\n",
        "        token_emb = self.token_embedding(input_ids)        # [B, T, D]\n",
        "        pos_emb = self.position_embedding(position_ids)    # [1, T, D]\n",
        "        x = token_emb + pos_emb                           # [B, T, D]\n",
        "        \n",
        "        # Transformer å±‚\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x)\n",
        "        \n",
        "        # æœ€ç»ˆå±‚å½’ä¸€åŒ–\n",
        "        x = self.ln_final(x)  # [B, T, D]\n",
        "        \n",
        "        # ä½¿ç”¨æœ€åä¸€ä¸ª token è¿›è¡Œåˆ†ç±»\n",
        "        last_token_emb = x[:, -1, :]  # [B, D]\n",
        "        \n",
        "        # åˆ†ç±»å¤´\n",
        "        logits = self.classifier(last_token_emb)  # [B, num_classes]\n",
        "        \n",
        "        return logits\n",
        "\n",
        "def create_model_for_classification(vocab_size=50257, max_seq_len=256):\n",
        "    \"\"\"\n",
        "    åˆ›å»ºç”¨äºåˆ†ç±»çš„ç®€åŒ– GPT æ¨¡å‹\n",
        "    \n",
        "    å‚æ•°è°ƒä¼˜è¯´æ˜ï¼š\n",
        "    - d_model=256: é€‚ä¸­çš„æ¨¡å‹ç»´åº¦ï¼Œå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡\n",
        "    - num_heads=8: å¤šå¤´æ³¨æ„åŠ›ï¼Œæ¯å¤´ç»´åº¦ 256/8=32\n",
        "    - num_layers=4: è¾ƒå°‘çš„å±‚æ•°ï¼Œé™ä½å¤æ‚åº¦\n",
        "    - d_ff=512: å‰é¦ˆç½‘ç»œç»´åº¦ï¼Œé€šå¸¸ä¸º d_model çš„ 2-4 å€\n",
        "    \"\"\"\n",
        "    model = SimpleGPTForClassification(\n",
        "        vocab_size=vocab_size,\n",
        "        max_seq_len=max_seq_len,\n",
        "        d_model=256,\n",
        "        num_heads=8,\n",
        "        num_layers=4,  # å‡å°‘å±‚æ•°ä»¥åŠ å¿«è®­ç»ƒ\n",
        "        d_ff=512,\n",
        "        num_classes=2\n",
        "    )\n",
        "    \n",
        "    # å‚æ•°åˆå§‹åŒ– (Xavier/Glorot åˆå§‹åŒ–)\n",
        "    for param in model.parameters():\n",
        "        if param.dim() > 1:\n",
        "            nn.init.xavier_uniform_(param)\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ ç¬¬6ç« ï¼šè®­ç»ƒå’Œè¯„ä¼°å‡½æ•°\n",
        "\n",
        "## ğŸ“ˆ æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°å·¥å…·\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âš¡ è®­ç»ƒå’Œè¯„ä¼°æ ¸å¿ƒå‡½æ•°\n",
        "# åŠŸèƒ½ï¼šæä¾›å®Œæ•´çš„æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œå¯è§†åŒ–å·¥å…·\n",
        "\n",
        "def calc_accuracy(model, data_loader, device, max_batches=None):\n",
        "    \"\"\"\n",
        "    è®¡ç®—æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        model: å¾…è¯„ä¼°çš„æ¨¡å‹\n",
        "        data_loader: æ•°æ®åŠ è½½å™¨\n",
        "        device: è®¡ç®—è®¾å¤‡\n",
        "        max_batches: æœ€å¤§è¯„ä¼°æ‰¹æ¬¡æ•°ï¼ˆNone=å…¨éƒ¨ï¼‰\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        accuracy: å‡†ç¡®ç‡ (0-1 ä¹‹é—´)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (input_ids, labels) in enumerate(data_loader):\n",
        "            if max_batches and i >= max_batches:\n",
        "                break\n",
        "                \n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "            \n",
        "            # è·å–æ¨¡å‹é¢„æµ‹\n",
        "            logits = model(input_ids)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            \n",
        "            # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹æ•°\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    \n",
        "    model.train()\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "def calc_loss_batch(model, input_ids, labels, device):\n",
        "    \"\"\"è®¡ç®—å•ä¸ªæ‰¹æ¬¡çš„æŸå¤±\"\"\"\n",
        "    input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "    logits = model(input_ids)\n",
        "    return nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "def calc_loss_loader(model, data_loader, device, max_batches=None):\n",
        "    \"\"\"è®¡ç®—æ•°æ®åŠ è½½å™¨çš„å¹³å‡æŸå¤±\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (input_ids, labels) in enumerate(data_loader):\n",
        "            if max_batches and i >= max_batches:\n",
        "                break\n",
        "            \n",
        "            loss = calc_loss_batch(model, input_ids, labels, device)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "    \n",
        "    model.train()\n",
        "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "def train_lora_model(model, train_loader, val_loader, device, num_epochs=3, \n",
        "                    learning_rate=3e-4, eval_every=50):\n",
        "    \"\"\"\n",
        "    LoRA æ¨¡å‹è®­ç»ƒä¸»å‡½æ•°\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        model: LoRA å¢å¼ºçš„æ¨¡å‹\n",
        "        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
        "        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨\n",
        "        device: è®¡ç®—è®¾å¤‡\n",
        "        num_epochs: è®­ç»ƒè½®æ•°\n",
        "        learning_rate: å­¦ä¹ ç‡\n",
        "        eval_every: æ¯å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        è®­ç»ƒå†å²è®°å½• (æŸå¤±ã€å‡†ç¡®ç‡ç­‰)\n",
        "    \"\"\"\n",
        "    print(f\"ğŸš€ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ\")\n",
        "    print(f\"   - è®­ç»ƒè½®æ•°: {num_epochs}\")\n",
        "    print(f\"   - å­¦ä¹ ç‡: {learning_rate}\")\n",
        "    print(f\"   - è®¾å¤‡: {device}\")\n",
        "    \n",
        "    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡\n",
        "    model.to(device)\n",
        "    \n",
        "    # åˆ›å»ºä¼˜åŒ–å™¨ (ä»…ä¼˜åŒ–å¯è®­ç»ƒå‚æ•°)\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    print(f\"   - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}\")\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate)\n",
        "    \n",
        "    # è®­ç»ƒå†å²è®°å½•\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "    steps = []\n",
        "    \n",
        "    global_step = 0\n",
        "    \n",
        "    # è®­ç»ƒå¾ªç¯\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        print(f\"\\nğŸ“š Epoch {epoch + 1}/{num_epochs}\")\n",
        "        \n",
        "        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "        \n",
        "        for batch_idx, (input_ids, labels) in enumerate(pbar):\n",
        "            # å‰å‘ä¼ æ’­\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch(model, input_ids, labels, device)\n",
        "            \n",
        "            # åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "            \n",
        "            # æ›´æ–°è¿›åº¦æ¡\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "            \n",
        "            # å‘¨æœŸæ€§è¯„ä¼°\n",
        "            if global_step % eval_every == 0:\n",
        "                train_loss = calc_loss_loader(model, train_loader, device, max_batches=10)\n",
        "                val_loss = calc_loss_loader(model, val_loader, device)\n",
        "                \n",
        "                train_acc = calc_accuracy(model, train_loader, device, max_batches=10)\n",
        "                val_acc = calc_accuracy(model, val_loader, device)\n",
        "                \n",
        "                # è®°å½•å†å²\n",
        "                steps.append(global_step)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                train_accs.append(train_acc)\n",
        "                val_accs.append(val_acc)\n",
        "                \n",
        "                print(f\\\"\\\\n   Step {global_step}: Train Loss={train_loss:.3f}, Val Loss={val_loss:.3f}\\\")\\n                print(f\\\"   Train Acc={train_acc*100:.1f}%, Val Acc={val_acc*100:.1f}%\\\")\\n        \\n        # Epoch ç»“æŸè¯„ä¼°\\n        avg_epoch_loss = epoch_loss / len(train_loader)\\n        print(f\\\"ğŸ“Š Epoch {epoch+1} å®Œæˆ - å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}\\\")\\n    \\n    print(\\\"\\\\nğŸ‰ LoRA å¾®è°ƒè®­ç»ƒå®Œæˆï¼\\\")\\n    \\n    return {\\n        'steps': steps,\\n        'train_losses': train_losses,\\n        'val_losses': val_losses,\\n        'train_accs': train_accs,\\n        'val_accs': val_accs\\n    }\\n\\ndef plot_training_curves(history, save_path='training_curves.png'):\\n    \\\"\\\"\\\"\\n    ç»˜åˆ¶è®­ç»ƒæ›²çº¿\\n    \\n    å‚æ•°ï¼š\\n        history: è®­ç»ƒå†å²è®°å½•\\n        save_path: ä¿å­˜è·¯å¾„\\n    \\\"\\\"\\\"\\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\\n    \\n    steps = history['steps']\\n    \\n    # æŸå¤±æ›²çº¿\\n    ax1.plot(steps, history['train_losses'], 'b-', label='è®­ç»ƒæŸå¤±')\\n    ax1.plot(steps, history['val_losses'], 'r--', label='éªŒè¯æŸå¤±')\\n    ax1.set_xlabel('è®­ç»ƒæ­¥æ•°')\\n    ax1.set_ylabel('æŸå¤±')\\n    ax1.set_title('æŸå¤±æ›²çº¿')\\n    ax1.legend()\\n    ax1.grid(True)\\n    \\n    # å‡†ç¡®ç‡æ›²çº¿\\n    ax2.plot(steps, [acc*100 for acc in history['train_accs']], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')\\n    ax2.plot(steps, [acc*100 for acc in history['val_accs']], 'r--', label='éªŒè¯å‡†ç¡®ç‡')\\n    ax2.set_xlabel('è®­ç»ƒæ­¥æ•°')\\n    ax2.set_ylabel('å‡†ç¡®ç‡ (%)')\\n    ax2.set_title('å‡†ç¡®ç‡æ›²çº¿')\\n    ax2.legend()\\n    ax2.grid(True)\\n    \\n    # æŸå¤±å¯¹æ¯” (æœ€åå‡ ä¸ªç‚¹)\\n    if len(steps) > 5:\\n        recent_steps = steps[-5:]\\n        ax3.bar(range(len(recent_steps)), [history['train_losses'][i] for i in range(-5, 0)], \\n               alpha=0.7, label='è®­ç»ƒæŸå¤±')\\n        ax3.bar(range(len(recent_steps)), [history['val_losses'][i] for i in range(-5, 0)], \\n               alpha=0.7, label='éªŒè¯æŸå¤±')\\n        ax3.set_xlabel('æœ€è¿‘5ä¸ªè¯„ä¼°ç‚¹')\\n        ax3.set_ylabel('æŸå¤±')\\n        ax3.set_title('æœ€è¿‘æŸå¤±å¯¹æ¯”')\\n        ax3.legend()\\n    \\n    # å‡†ç¡®ç‡æå‡\\n    if len(history['val_accs']) > 1:\\n        acc_improvement = [(acc - history['val_accs'][0])*100 for acc in history['val_accs']]\\n        ax4.plot(steps, acc_improvement, 'g-', linewidth=2)\\n        ax4.axhline(y=0, color='k', linestyle='--', alpha=0.5)\\n        ax4.set_xlabel('è®­ç»ƒæ­¥æ•°')\\n        ax4.set_ylabel('å‡†ç¡®ç‡æå‡ (%)')\\n        ax4.set_title('éªŒè¯å‡†ç¡®ç‡æå‡')\\n        ax4.grid(True)\\n    \\n    plt.tight_layout()\\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n    plt.show()\\n    \\n    # æ‰“å°æœ€ç»ˆç»“æœ\\n    if history['val_accs']:\\n        print(f\\\"\\\\nğŸ“Š è®­ç»ƒç»“æœæ€»ç»“:\\\")\\n        print(f\\\"   - åˆå§‹éªŒè¯å‡†ç¡®ç‡: {history['val_accs'][0]*100:.2f}%\\\")\\n        print(f\\\"   - æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {history['val_accs'][-1]*100:.2f}%\\\")\\n        print(f\\\"   - å‡†ç¡®ç‡æå‡: {(history['val_accs'][-1] - history['val_accs'][0])*100:.2f} ç™¾åˆ†ç‚¹\\\")\\n        print(f\\\"   - æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_losses'][-1]:.4f}\\\")\\n        print(f\\\"   - æœ€ç»ˆéªŒè¯æŸå¤±: {history['val_losses'][-1]:.4f}\\\")\\n\\ndef classify_text(text, model, tokenizer, device, max_length=256):\\n    \\\"\\\"\\\"\\n    å¯¹å•æ¡æ–‡æœ¬è¿›è¡Œåˆ†ç±»é¢„æµ‹\\n    \\n    å‚æ•°ï¼š\\n        text: å¾…åˆ†ç±»çš„æ–‡æœ¬\\n        model: è®­ç»ƒå¥½çš„æ¨¡å‹\\n        tokenizer: åˆ†è¯å™¨\\n        device: è®¡ç®—è®¾å¤‡\\n        max_length: æœ€å¤§åºåˆ—é•¿åº¦\\n        \\n    è¿”å›ï¼š\\n        prediction: é¢„æµ‹ç»“æœ ('spam' æˆ– 'ham')\\n        confidence: ç½®ä¿¡åº¦\\n    \\\"\\\"\\\"\\n    model.eval()\\n    \\n    # æ–‡æœ¬é¢„å¤„ç†\\n    input_ids = tokenizer.encode(text)\\n    if len(input_ids) > max_length:\\n        input_ids = input_ids[:max_length]\\n    else:\\n        input_ids = input_ids + [50256] * (max_length - len(input_ids))  # å¡«å……\\n    \\n    # è½¬ä¸ºå¼ é‡\\n    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\\n    \\n    # æ¨¡å‹æ¨ç†\\n    with torch.no_grad():\\n        logits = model(input_tensor)\\n        probabilities = torch.softmax(logits, dim=-1)\\n        predicted_class = torch.argmax(logits, dim=-1).item()\\n        confidence = probabilities[0, predicted_class].item()\\n    \\n    # è¿”å›ç»“æœ\\n    prediction = \\\"spam\\\" if predicted_class == 1 else \\\"ham\\\"\\n    return prediction, confidence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¯ ç¬¬7ç« ï¼šå®Œæ•´ LoRA å¾®è°ƒå®éªŒæµç¨‹\n",
        "\n",
        "## ğŸš€ ç«¯åˆ°ç«¯å®éªŒï¼šä»æ•°æ®åˆ°æ¨¡å‹éƒ¨ç½²\n",
        "\n",
        "ç°åœ¨è®©æˆ‘ä»¬å°†æ‰€æœ‰ç»„ä»¶æ•´åˆèµ·æ¥ï¼Œæ‰§è¡Œå®Œæ•´çš„ LoRA å¾®è°ƒå®éªŒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ§ª å®Œæ•´ LoRA å¾®è°ƒå®éªŒ\n",
        "# åŠŸèƒ½ï¼šå±•ç¤ºä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹\n",
        "\n",
        "print(\"ğŸ”¬ å¼€å§‹ LoRA å¾®è°ƒå®Œæ•´å®éªŒ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# è®¾ç½®å®éªŒå‚æ•°\n",
        "torch.manual_seed(42)  # å›ºå®šéšæœºç§å­\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "\n",
        "# 1ï¸âƒ£ å‡†å¤‡ç¤ºä¾‹æ•°æ® (ç®€åŒ–ç‰ˆæœ¬ï¼Œæ— éœ€ä¸‹è½½)\n",
        "print(\"\\\\n1ï¸âƒ£ å‡†å¤‡ç¤ºä¾‹æ•°æ®...\")\n",
        "\n",
        "# åˆ›å»ºç¤ºä¾‹SMSæ•°æ®é›†\n",
        "sample_data = {\n",
        "    'Label': ['ham', 'spam', 'ham', 'spam', 'ham', 'spam'] * 20,\n",
        "    'Text': [\n",
        "        'Hey, how are you doing today?',\n",
        "        'WINNER! You have won $1000! Click here to claim!',\n",
        "        'Can you pick up milk on your way home?',\n",
        "        'FREE! Get rich quick scheme! No risk!',\n",
        "        'Meeting is at 3pm in conference room B',\n",
        "        'Congratulations! You are our lucky winner! Call now!'\n",
        "    ] * 20\n",
        "}\n",
        "\n",
        "# åˆ›å»ºDataFrameå¹¶ä¿å­˜\n",
        "df = pd.DataFrame(sample_data)\n",
        "df['Label'] = df['Label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# å¹³è¡¡æ•°æ®é›†\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "train_df, val_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "\n",
        "# ä¿å­˜åˆ°CSV\n",
        "train_df.to_csv('train_sample.csv', index=False)\n",
        "val_df.to_csv('val_sample.csv', index=False)\n",
        "test_df.to_csv('test_sample.csv', index=False)\n",
        "\n",
        "# 2ï¸âƒ£ åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "print(\"\\\\n2ï¸âƒ£ åˆ›å»ºæ•°æ®åŠ è½½å™¨...\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "train_dataset = SpamDataset('train_sample.csv', tokenizer, max_length=64)\n",
        "val_dataset = SpamDataset('val_sample.csv', tokenizer, max_length=train_dataset.max_length)\n",
        "test_dataset = SpamDataset('test_sample.csv', tokenizer, max_length=train_dataset.max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# 3ï¸âƒ£ åˆ›å»ºæ¨¡å‹\n",
        "print(\"\\\\n3ï¸âƒ£ åˆ›å»ºåŸºç¡€æ¨¡å‹...\")\n",
        "model = create_model_for_classification(max_seq_len=train_dataset.max_length)\n",
        "\n",
        "# 4ï¸âƒ£ åº”ç”¨ LoRA\n",
        "print(\"\\\\n4ï¸âƒ£ åº”ç”¨ LoRA å¾®è°ƒ...\")\n",
        "lora_model = replace_linear_with_lora(model, rank=8, alpha=16)\n",
        "\n",
        "# 5ï¸âƒ£ è®­ç»ƒæ¨¡å‹\n",
        "print(\"\\\\n5ï¸âƒ£ å¼€å§‹è®­ç»ƒ...\")\n",
        "\n",
        "# ç®€åŒ–è®­ç»ƒå‡½æ•°\n",
        "def quick_train(model, train_loader, val_loader, device, epochs=2):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-3)\n",
        "    \n",
        "    history = {'train_losses': [], 'val_losses': [], 'train_accs': [], 'val_accs': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # è®­ç»ƒ\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for input_ids, labels in train_loader:\n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids)\n",
        "            loss = nn.functional.cross_entropy(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # è¯„ä¼°\n",
        "        train_acc = calc_accuracy(model, train_loader, device)\n",
        "        val_acc = calc_accuracy(model, val_loader, device)\n",
        "        val_loss = calc_loss_loader(model, val_loader, device)\n",
        "        \n",
        "        history['train_losses'].append(train_loss/len(train_loader))\n",
        "        history['val_losses'].append(val_loss)\n",
        "        history['train_accs'].append(train_acc)\n",
        "        history['val_accs'].append(val_acc)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss/len(train_loader):.3f}, \"\n",
        "              f\"Train Acc={train_acc*100:.1f}%, Val Acc={val_acc*100:.1f}%\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# æ‰§è¡Œè®­ç»ƒ\n",
        "history = quick_train(lora_model, train_loader, val_loader, device, epochs=3)\n",
        "\n",
        "# 6ï¸âƒ£ æµ‹è¯•æ¨¡å‹\n",
        "print(\"\\\\n6ï¸âƒ£ æ¨¡å‹æµ‹è¯•...\")\n",
        "test_acc = calc_accuracy(lora_model, test_loader, device)\n",
        "print(f\"ğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc*100:.2f}%\")\n",
        "\n",
        "# 7ï¸âƒ£ å®é™…åº”ç”¨æ¼”ç¤º\n",
        "print(\"\\\\n7ï¸âƒ£ å®é™…åº”ç”¨æ¼”ç¤º...\")\n",
        "test_texts = [\n",
        "    \"Hey, want to grab lunch together?\",\n",
        "    \"URGENT! You've won $5000! Click now to claim your prize!\",\n",
        "    \"The meeting has been moved to 2pm\",\n",
        "    \"Free iPhone! Limited time offer! Call immediately!\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    pred, conf = classify_text(text, lora_model, tokenizer, device, max_length=64)\n",
        "    print(f\"æ–‡æœ¬: '{text[:40]}...'\")\n",
        "    print(f\"é¢„æµ‹: {pred} (ç½®ä¿¡åº¦: {conf:.3f})\\\\n\")\n",
        "\n",
        "print(\"ğŸ‰ LoRA å¾®è°ƒå®éªŒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“ æ€»ç»“ä¸å…³é”®è¦ç‚¹\n",
        "\n",
        "## âœ¨ æœ¬æ•™ç¨‹çš„æ ¸å¿ƒæ”¶è·\n",
        "\n",
        "é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å·²ç»æŒæ¡äº† LoRAï¼ˆLow-Rank Adaptationï¼‰å¾®è°ƒçš„å®Œæ•´ç†è®ºå’Œå®è·µï¼š\n",
        "\n",
        "### ğŸ§  ç†è®ºç†è§£\n",
        "1. **LoRA åŸç†**ï¼šé€šè¿‡ä½ç§©çŸ©é˜µåˆ†è§£ `Î”W â‰ˆ AÂ·B` å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ\n",
        "2. **æ•°å­¦æœ¬è´¨**ï¼š`h = xÂ·W + Î±Â·(xÂ·AÂ·B)` - åŸå§‹è¾“å‡ºå åŠ  LoRA å¢é‡\n",
        "3. **å‚æ•°ä¼˜åŠ¿**ï¼šä»…è®­ç»ƒ <5% å‚æ•°ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬\n",
        "4. **åˆå§‹åŒ–ç­–ç•¥**ï¼šBçŸ©é˜µé›¶åˆå§‹åŒ–ç¡®ä¿è®­ç»ƒå¼€å§‹æ—¶æ¨¡å‹è¡Œä¸ºä¸å˜\n",
        "\n",
        "### ğŸ’» å®è·µæŠ€èƒ½\n",
        "1. **æ•°æ®é¢„å¤„ç†**ï¼šç±»åˆ«å¹³è¡¡ã€åºåˆ—å¡«å……ã€æ‰¹é‡åŠ è½½\n",
        "2. **æ¨¡å‹æ”¹é€ **ï¼šé€’å½’æ›¿æ¢çº¿æ€§å±‚ä¸º LoRA å¢å¼ºç‰ˆæœ¬\n",
        "3. **è®­ç»ƒæµç¨‹**ï¼šå†»ç»“åŸå§‹å‚æ•°ï¼Œä»…ä¼˜åŒ– LoRA åˆ†æ”¯\n",
        "4. **æ€§èƒ½è¯„ä¼°**ï¼šå‡†ç¡®ç‡è®¡ç®—ã€æŸå¤±å¯è§†åŒ–ã€å®é™…åº”ç”¨æµ‹è¯•\n",
        "\n",
        "### ğŸ”‘ å…³é”®è¶…å‚æ•°\n",
        "- **rank (r)**ï¼šæ§åˆ¶ LoRA è¡¨è¾¾èƒ½åŠ›ï¼Œæ¨è 8-64\n",
        "- **alpha (Î±)**ï¼šç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ä¸º 2Ã—rank\n",
        "- **å­¦ä¹ ç‡**ï¼šLoRA å¾®è°ƒæ¨è 1e-4 åˆ° 1e-3\n",
        "- **è®­ç»ƒè½®æ•°**ï¼šé€šå¸¸ 3-5 è½®å³å¯æ”¶æ•›\n",
        "\n",
        "### ğŸ“Š é¢„æœŸæ•ˆæœ\n",
        "- **å‚æ•°æ•ˆç‡**ï¼šç›¸æ¯”å…¨é‡å¾®è°ƒå‡å°‘ 95%+ å¯è®­ç»ƒå‚æ•°\n",
        "- **è®­ç»ƒé€Ÿåº¦**ï¼šæ˜¾å­˜å ç”¨é™ä½ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­\n",
        "- **éƒ¨ç½²ä¾¿åˆ©**ï¼šLoRA æ–‡ä»¶ä»…æ•°MBï¼Œæ”¯æŒå¤šä»»åŠ¡åˆ‡æ¢\n",
        "- **æ€§èƒ½ä¿æŒ**ï¼šåœ¨åƒåœ¾çŸ­ä¿¡åˆ†ç±»ä»»åŠ¡ä¸Šå¯è¾¾ 95%+ å‡†ç¡®ç‡\n",
        "\n",
        "## ğŸš€ è¿›é˜¶æ–¹å‘\n",
        "\n",
        "### ğŸ”¬ æ·±å…¥ç ”ç©¶\n",
        "1. **QLoRA**ï¼šç»“åˆé‡åŒ–çš„æ›´é«˜æ•ˆ LoRA å˜ä½“\n",
        "2. **AdaLoRA**ï¼šè‡ªé€‚åº”è°ƒæ•´ä¸åŒå±‚çš„ rank å¤§å°\n",
        "3. **LoRA+**ï¼šæ”¹è¿›çš„ LoRA ç®—æ³•ï¼Œæ›´å¥½çš„æ”¶æ•›æ€§\n",
        "4. **å¤šä»»åŠ¡ LoRA**ï¼šä¸€ä¸ªåŸºåº§æ¨¡å‹é€‚é…å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡\n",
        "\n",
        "### ğŸ› ï¸ å®é™…åº”ç”¨\n",
        "1. **é¢†åŸŸé€‚åº”**ï¼šå°†é€šç”¨æ¨¡å‹é€‚é…åˆ°ç‰¹å®šé¢†åŸŸï¼ˆæ³•å¾‹ã€åŒ»ç–—ç­‰ï¼‰\n",
        "2. **å¤šè¯­è¨€æ”¯æŒ**ï¼šä¸ºä¸åŒè¯­è¨€åˆ›å»º LoRA é€‚é…å™¨\n",
        "3. **ä¸ªæ€§åŒ–æœåŠ¡**ï¼šä¸ºä¸åŒç”¨æˆ·ç¾¤ä½“è®­ç»ƒä¸“å± LoRA\n",
        "4. **æ¨¡å‹å‹ç¼©**ï¼šç»“åˆå‰ªæã€é‡åŒ–ç­‰æŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–\n",
        "\n",
        "### ğŸ“š å­¦ä¹ èµ„æº\n",
        "1. **è®ºæ–‡é˜…è¯»**ï¼šLoRAåŸè®ºæ–‡åŠç›¸å…³å·¥ä½œ\n",
        "2. **å¼€æºé¡¹ç›®**ï¼šPEFTã€LLaMA-Factoryã€Alpaca-LoRA\n",
        "3. **å®è·µé¡¹ç›®**ï¼šå°è¯•åœ¨æ›´å¤§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šåº”ç”¨\n",
        "4. **ç¤¾åŒºäº¤æµ**ï¼šå‚ä¸å¼€æºç¤¾åŒºï¼Œåˆ†äº«ç»éªŒå¿ƒå¾—\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ æœ€åçš„å»ºè®®\n",
        "\n",
        "1. **å¾ªåºæ¸è¿›**ï¼šå…ˆåœ¨å°æ¨¡å‹å°æ•°æ®ä¸ŠéªŒè¯ï¼Œå†æ‰©å±•åˆ°å¤§è§„æ¨¡\n",
        "2. **å‚æ•°è°ƒä¼˜**ï¼šæ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´ rank å’Œ alpha\n",
        "3. **è¯„ä¼°å…¨é¢**ï¼šä¸ä»…çœ‹å‡†ç¡®ç‡ï¼Œè¿˜è¦å…³æ³¨æ³›åŒ–èƒ½åŠ›\n",
        "4. **ç‰ˆæœ¬æ§åˆ¶**ï¼šè®°å½•è¶…å‚æ•°å’Œç»“æœï¼Œæ–¹ä¾¿å¯¹æ¯”ä¼˜åŒ–\n",
        "\n",
        "ğŸ‰ **æ­å–œä½ å®Œæˆäº† LoRA å¾®è°ƒçš„å®Œæ•´å­¦ä¹ ä¹‹æ—…ï¼ç°åœ¨ä½ å·²ç»å…·å¤‡äº†å°†ç†è®ºåº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­çš„èƒ½åŠ›ã€‚**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
